{"pages":[{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"},{"title":"About Author","text":"Self-introductionThe only thing I know is that I know nothing. I decided to start writing blogs to record interesting papers and fascinating ideas in the fields of Financial Engineering, Machine learning, Optimization, and Algorithm. After several months’ tough job seeking process, my conclusion is that thinking and researching is reductive but finding a job and networking is practical. If you find any mistakes and typos, please feel free to contact me. Thank you for reading my blogs. Peimou Email: ps3136@columbia.edu","link":"/About%20author/index.html"}],"posts":[{"title":"Hello World","text":"Some helpful links to help you blogging. How to use Hexo Hexo. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting. http://theme-next.iissnan.com/theme-settings.html (language: Chinese) https://www.jianshu.com/p/efbeddc5eb19 (language: Chinese) Math support in Hexo: Mathjax vs KatexAlthough Katexs support less mathematics symbols compared to Mathjax, Katex only takes hundreds microsecond to render math formula, which is hundred times faster than Mathjax. I appreciate Zijing Hu’s help when I struggled with the settings of Katex and hexo-renderer-kramed. You can visit Zijing’s github page for details. You can read the Katex documents to learn more about the syntax and details.","link":"/2020/01/13/Hello%20World/"},{"title":"Kalman Filter-Notes-Part1-Discrete Kalman Filter","text":"R.E Kalman published a paper about a recursive solution to a discrete-data linear filtering problem. The Kalman filter provided us a computational method to estimate unobservable variables through minimization the posterior error. Moreover, Kalman filter enabled us to obtain the consistent parameter estimates by maximizing the likelihood function of error innovations. 1. The basis of discrete Kalman filterGenerally, Kalman filter solved the problem of estimating the unobservable state $x \\in \\mathcal{R}^n$ of a discrete time control process. The dynamic of the unobservable state’s ($n \\times 1 $ vector) can be written as: $x_k = Ax_{k - 1} + Bu_{k - 1}＋ｗ_{k - 1}$ The measurement variable $z_k$ follows a linear transformation of unobservable states with an linearly additive noise term $v_k$: $z_k = Hx_k＋v_k$ Note that we assume the probability measure of $w$ and $v$ follow Gaussian distribution with zero mean and covariance matrix Q , and R, which are both independent of each other and white. I will use the same symbols as Greg Welch and Gary Bishop’s: An introduction to Kalman filter to show you the computational origin of Kalman filter. Notations: $\\hat{x_k^{-}} \\in \\mathcal{R}^n$ is the estimation of unobservable states $x$ at time $k$ under the filtration $\\mathcal{F}_{t-1}$. $\\hat{x_k}$ is the posteriori state estimate under the information at time $k$ (with the help of $z_k$). Priori estimate error $e^{-}_{k}$: $x_k - \\hat{x_k^{-}}$. Posteriori estimate error $e_{k}$: $x_k - \\hat{x_k}$. Priori estimate error covariance matrix $P^{-}_{k} = \\mathbb{E}[e^{-}_ke^{-T}_k]$ Posteriori estimate error covariance matrix $P_{k} = \\mathbb{E}[e_ke^{T}_k]$ We assume the relationship between the posteriori estimate and the priori estimate follows: $\\hat{x_k} = \\hat{x^{-}_k} + K_k(z_k - H\\hat{x^{-}_k})$ Remark: If you know something about Gaussian distribution, you may find this is the BUE given observed states if we assume the noises is Gaussian distributed (it is a result of joint Gaussian distribution). I firmly believe this assumption is inspired from a probability origin. $z_k - H\\hat{x^{-}_k}$ are related to many frightening names like measure residual or innovation. As we need to minimize the posteriori error of each entry of the state vector, we should minimize the trace of $P_k$. Note that in the classical Kalman filter, the transition matrix of the system is deterministic. We only need to consider some incontrollable factors like measurement error and transition error. $\\hat{x_k} = \\hat{x^{-}_k} + K(z_k - Hx^{-}_k) \\leftrightarrow \\hat{x_k} = \\hat{x^{-}_k} + K(Hx_k + v_t - Hx^{-}_k)$ $\\leftrightarrow x_k - \\hat{x_k} = x_k - \\hat{x^{-}_k} - K(Hx_k + v_t - Hx^{-}_k)$ $\\leftrightarrow P_k = (I - K_kH_k)P_k^-(I - K_kH_k)^T+K_kR_kK_k^T$ (*) Let’s review some tricks in matrix derivation. You can see more details about layout with wiki. $\\frac{d(tr(AB))}{dA} = B^T$ $\\frac{d(tr(ACA^T))}{dA} = 2AC$ (It is trivial if you know $tr(AB) = \\sum_i\\sum_j a_{ij}b_{ji}$) We can get the optimal $K_k$ (the Kalman Gains): $K_k = P^{-}_kH^T(HP^{-}_kH^T+R)^{-1}$ With the Kalman gain and (*), we can get the posteriori estimates of state’s posteriori covariance matrix $P_k$. $P_k = (I - K_kH_k)P_k^-$ Note that if $\\lim \\limits{R \\xrightarrow{} 0}$ (there is no error in measurement), then the Kalman gain get the maximum value $H^{-1}$. If $\\lim \\limits{P_k^- \\xrightarrow{} 0}$, then the Kalman gain get the minimum value 0. You can see the adaptiveness of the methods here. If we are more confidence with the transition equation of unobservable state, we should allocate relatively more weight on the priori estimates; If we are more confident with the measurement of the observable equation, then we should make the posteriori estimate more close to the observable states. With the best estimated posteriori estimates of unobservable state, we can update the new priori estimate of unobservable variables. $P_k^- = AP_kA^T + Q$ $\\hat{x_k^-} = A\\hat{x_{k-1}} + Bu_{k-1}$ Remark: In the system of Kalman filter, we know exactly the transition of unobservable states. In this situation, the Kalman filter found a solution to estimate the unobservable states $x_k$ based on information at time k. If we know nothing about the transition of unobservable states (unknown A for example), we can maximize the likelihood function based on the measurement function. 2. Discussions about the initial valuesUnfortunately, we need to input four tricky initial values to the Kalman filter system: $x_0^-$, $P_0^-$, R, and Q. The more unfortunate fact is that KF is sensitive to the initial values. Based on this fact, we have to adjust hyperparameters for our model. The best statement of adjustment of initial values I have ever met is that the innovation ($z_k - H\\hat{x_k^-}$) should be a white noise with zero mean if KF works well (See Optimal State Estimation Kalman-H-and Nonlinear Approaches). We have to test different hyperparameters to help the model make sense. Thank you for reading my blog! KF is a big topic and I will dig deeper in the next parts of this Note.","link":"/2020/01/16/Kalman%20Filter-Notes-Part1/"},{"title":"Kalman Filter-Projects-Part1-N factors Gaussian model","text":"Before we dig deeper in the Kalman filter, I would like to share one of the applications of Kalman filter with you. N-factors Gaussian model is a direct application of what we have discussed in Kalman filter Part1. This blog includes a brief introduction about this method, and some tricks in this algorithm‘s implement. 1. The Financial basis of N-factor Gaussian modelIf you have heard Barra model for stocks or other factors analysis models, you may understand this n-factors Gaussian model easily. Factor analysis is a cross-sectional methodology, which totally depends on the information at current time slot. To be more specific, it can detect the sources of correlations if we assume the correlations between different observable states are caused by communalities. Of course, we can use factors analysis framework to predict with some extra assumptions (for example, the factors’ returns keep constant between time slots). At time $t$, we can know the return of $n$ assets. We assume that the correlations between assets are caused by $m$ factors (risk factors). Matrix $X_{(m \\times n)}$ denotes the exposures of assets on risk factors, while vector $f$ denotes the ‘return’ of risk factors. If we write this model in a linear regression scheme, then $r_t = X_t^Tf + \\epsilon$ Intuitively, we decompose the observed returns according to the risk factors, and the coefficients of factors describe the average returns of each factors. In this framework, we treat the returns of factors as constants. What if we want to treat the coefficients like a distribution or something which contains the uncertainties? Let’s review the framework of risk neutral theorem in financial engineering. We assume the market (efficient and complete) has $n$ risk factors which determine the return of observable assets. Let $\\Theta(t)$ be a n-dimensional adapted process according to $W(t)$, which is a m-dimensional Brownian motions. In this framework, the returns of the risk factors are described as a m-dimensional Brownian motion. $d\\Theta(t) = \\mu(\\Theta)dt + \\sigma(\\Theta)dW(t)$ $sigma(\\Theta)$ works the same as the risk factors exposures in this framework. From this perspective, multi-factors model in stocks is a just special case. 2. The settings of N-factors Gaussian modelWe assume the spot prices of assets ($\\mathbf{S}_t$ is a $N \\times 1$ vector)in a market can be described with $n$ risk factors as: $log\\mathbf{S_t} = \\mathbf{L}_t’ \\mathbf{x_t} + \\mu t $ $d\\mathbf{x}_t = -\\mathbf{K}\\mathbf{x}_tdt + \\mathbf{\\Sigma} d\\mathbf{W}_t$ where $\\mathbf{K} = \\begin{Bmatrix} 0 & 0 & … & 0 \\\\ 0 & k_2 & … & 0 \\\\ .. & .. & .. & .. \\\\ 0 & 0 &…&k_n \\end{Bmatrix}$, and $\\mathbf{\\Sigma} = \\begin{Bmatrix} \\sigma_1 & 0 & … & 0 \\\\ 0 & \\sigma_2 & … & 0 \\\\ .. & .. & .. & .. \\\\ 0 & 0 &…&\\sigma_n \\end{Bmatrix}$ $\\mathbf{L}_t$ is a $n \\times N$ matrix. This settings of multi-factors analysis include the independence of communalities. However, of course, we can handle the correlations between the risk factors. For example, we assume the risk factors are linear combinations of m independent factors (m < n), then the observed returns of the markets is a affined transformation of m-factor Gaussian model we showed above (See Affined N-factors Gaussian Model). 3. Parameters estimationParameters estimation is a direct application of Kalman filter (See here to know more about Kalman filter). As we have discussed in the Part I of KF Notes, we can maximize the likelihood function based on innovation ($z_t - \\hat{z_t}$) to estimate the transition matrix of unobservable states. $\\mathbf{x}_t = A \\mathbf{x}(t-1)+ \\mathbf{c}_t + \\mathbf{\\epsilon}_t$, where $\\mathbf{\\epsilon}_t \\backsim N(0, Q_t)$ $\\mathbf{z}_t = H \\mathbf{x}_t + \\mathbf{d}_t + \\mathbf{v}_t$, where $\\mathbf{v}_t \\backsim N(0, R_t)$ $\\hat{x_t^-} = A\\hat{x_{t-1}} + \\mathbf{c}_t$ $\\hat{P_t^-} = A \\hat{P_{t-1}}A^T + Q_t$ $\\hat{z_t^-} = H \\hat{\\mathbf{x}_t^-} + \\mathbf{d}_t$ Let $\\hat{F_t^-}$ be the priori covariance matrix of observable states. $\\hat{F_t^-} = H\\hat{P_t^-}H^T + R_t$ We need to parameterize the covariance matrix $R_t$. Some papers assume that the error of measurement equation is a homoscedastic diagonal matrix. However, we can try some more complex assumption, which is another typical bias and variance trade off. We need to maximize the log-likelihood function of $z_t$: $\\mathcal{L}(\\Theta) = -\\frac{1}{2} \\sum_t log|F_t^-| - \\frac{1}{2} \\sum_t [z_t - \\hat{z_t^-(\\Theta)}]^T (F_t^-)^{-1}[z_t - \\hat{z_t^-(\\Theta)}]$ If $H_t$ also needs to be parameterized, we can use EM algorithm to estimate the parameters. Remark: We used the priori estimation of the covariance matrix of $\\hat{z_t^-}$ in the likelihood function. Why not the posterior covariance matrix? The answer is quite trivial if you notice that the posterior estimation includes a weighted average of priori estimation and innovation (measure residual), which means we have to know the innovation before the posteriori estimation. In the likelihood function methodology, we use Fisher information matrix to estimate the variance of parameters.","link":"/2020/01/18/Kalman%20Filter-Projects-Part1/"},{"title":"Stochastic Approximation Methods-Notes-Part 1-An Overview and Robbins Monro Algorithm","text":"Generally, stochastic approximation methods are a family of iterative methods. The goal of these algorithms is to recover some properties of a function depending on random variables. The application of stochastic approximation ranges from deep learning (e.g., SGD ) to online learning methods. 1. An introduction to Stochastic Approximation MethodsIn this section, I will show you some examples of stochastic approximation methods. 1.1 Example 1: ExpectationFor a given probability space $(\\Omega, \\mathcal{F}, P)$, we want to solve the expectation of $E^P(f(\\omega))$, where $f$ is a measurable function, and $\\omega \\in \\Omega$. A reasonable solution is to estimate the expectation via arithmetic average. The following equations provide us an iterative method to this problem. Let $\\mathbb{E}_n$ be the estimation of $\\mathbb{E}^P(f(\\omega))$ given $n$ observation. $n \\times E_n = \\sum_{i = 1}^n f(\\omega_i)$ $ (n+1) \\times E_{n+1} = \\sum_{i = 1}^{n+1} f(\\omega_i) = n \\times E_n + f(\\omega_{n+1}) $ $ \\implies (n + 1) \\times E{n+1} = (n+1) \\times E_n + f(\\omega_{n+1}) - E_n $ $ \\implies E_{n+1} = E_n + \\frac{1}{n+1} [f(\\omega_{n+1}) - E_n] $ 1.2 Example 2: SGDIn the framework of stochastic gradient descent (SGD), or on-line gradient descent, the true gradient $\\nabla Q(\\omega)$ is approximated by a gradient at a single sample. Let $\\omega_n$ be the estimation of true gradient in the $nth$ iteration, then $\\omega_{n+1} = \\omega_n - \\lambda \\nabla Q(\\omega)$ where $\\lambda$ is the learning rate. There are several popular extensions of the SGD method, including Momentum, Nesterov Momentum, and Adam. 1.3 Example 3: Q-Learning in reinforcement learningIn the field of reinforcement learning, function $Q: S \\times A \\xrightarrow{} \\mathcal{R}$ combines the reward with states and actions. At each time $t$, the agent select an action $a_t$, get an reward $r_t$ from the environment, and enter into the next state $s_{t+1}$. The discussions of convergence of RL cannot avoid the topics on the Q function or the value function. $$Q^{new}(s_t, a_t) = (1 - \\alpha)Q^{old}(s_t, a_t) + \\alpha (r_t + \\gamma \\max \\limits_a Q^{old}(s_{t+1}, a))$$ where $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor. 2. Robbins-Monro AlgorithmIn 1951, Robbins and Monro developed a methodology for solving a root finding problem given a function depends (or partially depends) on a random variable. Root finding problem is the generalization of several important topics, including optimizations and extrema finding. 2.1 Root finding problem without uncertaintyLet $M(x)$ be a given function and $\\alpha$ a given constant such that the equation $M(x) = a$ has a unique root $x = \\theta$. For any iterative method for this root finding problem, we find one or more initial values $\\ x_{initial}$, and then successively obtain new values as certain functions of the previous obtained $x$. The most famous algorithm is the Newton–Raphson method. 2.2 A stochastic generalization of the root finding problemLet’s consider how to solve this problem when $M(x)$ is unknown for some reason. Let’s assume the randomness of the function is captured by the probability space $(\\Omega, \\mathcal{F}, P)$, and there is a random variable $Y$ on $(\\Omega, \\mathcal{F}, P)$ which takes values in a state space $(S, \\mathcal{L})$. In this blog, we assume the state space is a d-dimensional Euclidean space equipped with the $\\sigma$-field of Borel sets. In this algorithm, $M(x)$ is a deterministic function which defined as follows: $M(x) = \\int_{y \\in S }ydH(y|x)$ where $H(y|x) = \\mathbb{P}[Y(\\omega) \\le y|x]$ ($\\omega \\in \\Omega$) is the distribution function of $Y$ given an unobservable deterministic variable $x$. In other word, $M(x)$ is the expectation of $Y$ for the given $x$. Although we know nothing about the nature of function $M(x)$ and the distribution function $H(y|x)$, but we assume (which mean this assumption will work as a known condition in the proof) that the equation $M(x) = a$ has a unique solution $x = \\theta$. Bobbins-Monro algorithm introduced a method to estimate $\\theta$ by making successive observations on $Y$ at given $x_1, x_2, …$, which are determined sequentially in accordance with some definite experimental procedure. The idea of this algorithm is to construct a transition probability between two states. Moreover, with this sequence, the difference between $\\lim \\limits_{k \\xrightarrow{} \\infty}x_k$ and $\\theta$ can be controlled. We can conclude this idea as finding a sequence $\\{x_k\\}$, such that $\\lim \\limits_{k \\xrightarrow{} \\infty}E(x_k - \\theta)^2 = 0$ This idea also implies the convergence in probability (or in $\\mathcal{L}^2$) of $x_n$ to $\\theta$. 2.3 The proof of convergence theoremThe computational origin of this algorithm is the first example (expectation estimation) in the Introduction section. If we review the expectation estimation problem in the framework of Robbins-Monro, then in this example, $x$ denotes the expectation and function $M(x) = \\mathbb{E}^P[Y - x]$, and we want to solve the equation $M(x) = 0$. I only present brief proof to show the idea. You may read this paper for more details. We want to prove that: Condition 1: For every $x$, a distribution function in $y$, and there exist a positive constant C such that $\\mathbb{P}[|Y(\\omega)| \\le C|x] = \\int_{-c}^cdH(y|x) = 1$ Condition 2: There exist finite constants $\\alpha, \\theta$ such that $M(x) \\le \\alpha$ for $x \\lt \\theta$, and $M(x) \\ge \\alpha$ for $x \\gt \\theta$ Condition 3: Let $\\{a_n\\}$ be a fixed sequence of positive constants such that $0","link":"/2020/01/13/Stochastic%20Approximation%20Methods-Notes-1/"},{"title":"Generalized methods of Moments and Newy-West Adjustment-Notes-Part 1-GMM","text":"Newy-West adjustment provides a consistent estimation given autocorrelation in samples. The origin of Newy-West adjustment is closely related to Generalized Method of Moments. This blog includes the motivations, the definition and some properties of Generalized methods of Moments. 1. Motivations of GMMLet $\\Omega$ denote the set of sample points in the underlying probability space in some estimation problem. Let E denote the expectations operator. For a stochastic process ${x_n: n \\ge 1}$ defined in this probability space, we can get a finite segment of one realization of this process, i.e., ${x_n(\\omega_0): 1 \\le n \\le N}$. This sequence can be treated as the observable data sequence. Let moments function $f(\\cdot, \\theta)$ : $R^p \\xrightarrow{} R^q$ be a continuous and continuously differentiable measurable function of $\\theta$. To be more specific, the parameter $\\theta$ is a $(q \\times 1)$ vector, and we know $q$ moment based on the underlying distribution (or probability measure). Set the population moment conditions are that $E[f(x_i, \\theta_0)] = 0$ (Note that $\\theta_0$ is the solution of equations based on population moments, and it is a deterministic variable given information of population). The associated sample moments are given by: $f_n(\\theta) = \\frac{1}{n} \\sum_{i = 1}^nf(x_i, \\theta)$ We want to estimate $\\theta_0$ based on the solution of the equations $f_n(\\theta) = 0$ (to be more specific the bias to the moments is zero). Note that we have $q - p$ additional moments (if $q < p$), and the remedy for this situation is call GMM, which was introduced by Hansen[1982]. Intuitively, if we cannot find a good enough $\\theta$ to make sure the equation system $f_n(\\theta) = 0$, then we should at least try to keep the sample moments are very close to zero. Then the strategy is to measure the distance between $f_n(\\theta)$ and $0$. 2. The definition of GMMDef: Suppose function $f$ satisfied all the properties in section 1. We have an observed sample ${x_i: i = 1, 2, 3, \\dots}$, and we want to estimate parameter vector $\\theta$ ($p \\times 1$) with true value $\\theta_0$. Let $E[f(x_i, \\theta_0)]$ denote a set of $q$ population moments and $f_n(\\theta)$ denote the associated sample counterparts. Define the criterion function $Q_n(\\theta)$ as: $Q_n(\\theta) = f_n(\\theta)^TW_nf_n(\\theta)$ where $W_n$ is the weighting matrix, which converges to a positive definite matrix $W$ as n grows large. Then the GMM estimator of $\\theta_0$ is given by $\\hat{\\theta} = \\argmin Q_n(\\theta)$ Note the $Q_n(\\theta)$ measure the distance between $\\theta$ and $\\theta_0$, since $f_n(\\theta_0)$ should converge to zero with probability. In conclusion, we try to measure the distance between $\\theta_0$ and $\\theta$ in a metric space. If $p = q$, then the GMM degenerates to MM. 3. The property of GMM3.1 AssumptionsGiven some assumptions, we can conclude that the GMM estimator is consistent and asymptotically normally distributed. Assumption 1: we have more moments equations than parameters; the rank of Jacobian matrix of the moment equations evaluated at $\\theta_0$ is at least p; $\\theta_0$ is the unique solution of the moments equations system. Assumption 2: The weak large number law holds, which means for any $\\epsilon > 0$, we have $\\lim \\limits_{n \\xrightarrow{} \\infty }\\mathbb{P}[|f_n(\\theta_0) - f(\\theta_0)| > \\epsilon] = 0$. Assumption 3: The sample moments should ensure central limit theorem holds, with a finite asymptotic covariance matrix $\\frac{1}{n}F$. 3.2 The distribution of GMM estimatorThe variance of GMM estimator is consistent and asymptotically normally distributed with asymptotic covariance matrix $V_{GMM}$ $V_{GMM} = \\frac{1}{n} [G(\\theta_0)^{T}WG(\\theta_0)]^{-1}G(\\theta_0)^{T}WFWG(\\theta_0) [G(\\theta_0)^{T}WG(\\theta_0)]^{-1}$ where $G(\\theta_0)$ is the Jacobian matrix of the population moment functions evaluated at the true parameter value $\\theta_0$. I give up to articulate the proof of this result due to the complexity (due to my weak asymptotic statistic foundation QAQ) (See here for more details). Note that the variance of GMM estimator depends on the choice of $W_n$. If we review the criterion function of GMM, we can deduce that this distance function is the sample moment’s error sum of squares. Note that, given the knowledge of conditions of a matrix ($\\frac{\\lambda_{max}}{\\lambda_{min}}$) and Gaussian-Markov condition, it is make sense to normalize the errors in the moments by their variance. (Recall WLS and Hat Matrix ) 3.3 The optimal weighting matrixThe optimal choice of the weighting matrix $W_n = F^{-1}$, the GMM estimator is a asymptotically efficient with covariance matrix: $V_{GMM}^* = \\frac{1}{n} [G(\\theta_0)^TF^{-1}G(\\theta_0)]^{-1}$ However, if we want to know matrix $F$ (the asymptotic covariance matrix of sample moments), we have to estimate $\\theta$ first (Recall EM algorithm). Then we solve this circularity by adopting a multi-step method: Step 1: Choose a sub-optimal weighting matrix ($I$ for example), this will give us a consistent estimation of $\\theta$. Then we can estimate the matrix $F$. Step 2: Use the new $F$, estimate $\\theta_0$ Reference Zsochar, P. Short Introduction to the Generalized Method of Moments. HUNGARIAN STATISTICAL REVIEW. Hansen, L. (1982). Large Sample Properties of Generalized Method of Moments Estimators. Econometrica, 50(4), 1029. doi: 10.2307/1912775","link":"/2020/01/31/GMM-NW-Notes-1/"},{"title":"Stochastic Approximation Methods-Notes-Part2-Flat Minima and Gradient Descent","text":"The parameterized function with similar training error widely diverge in the generalization performance. However, the flat minima may imply a low-complexity neural network structure. Some SGD methods have shown can converge to a flatter minima, which potentially make the solution of nonconvex optimization more robust. The first part of this note is a review of Flat minima( Hochreiter and Schmidhuber, 1997). The second part contains an introduction to Gradient Descent algorithms’ properties and visualization. 1. DefinitionsA flat minimum is a large connected region in weight space where the error remains approximately constant. It can be rigorously defined with the tolerance error and training error. 1.1 The General TaskWe want to approximate the an unknown function f: $\\mathbb{R}^n \\rightarrow \\mathbb{R}$(Note that in the original paper, it is $\\mathbb{R}^n \\rightarrow \\mathbb{R}^k$, and there is no difference if we consider multi-dimensional target space). More specifically, function f mapping a finite set of possible input $X \\subset \\mathbb{R}^n$ to a finite possible output $Y \\subset \\mathbb{R}^k$. This function is assumed perfectly describe the real data generation process. The training information is given by a finite set $D_0 \\subset D$. We call $D_0$ the training data. Let tuple $(x_p, y_p)$ denotes the pth element in the data set. Let $net(w)$ denotes a realized neural network structure parameterized by $w$. Note that $w_ij$ represent for the weight on the connection from unit $i$to unit $j$. 1.2 Training Error (Empirical risk)The squared error of a realized structure over a given training set is defined as: $E(net(w), D_0) = \\sum_{p}||y_p-f(w,x_p)||^2$ where $||\\cdot||$ is the Euclidean norm. 1.3 Tolerance ErrorNote that we want to define a region in weight space where each parameter in this region has small empirical risk and similar output. The so called structure with ‘small’ risk denotes those of lower empirical risk than a given constant $\\mathbf{E}_{tol}$. 1.4 Boxes and Flat minimaWe want to study the property of a large region of connected acceptable minima, where each weight w in a this set lead to an identical structure (The authors used almost identical here, but I cannot get the point since no definition about any measure or metric space over net function were mentioned in this article). Since accurately approximation to this region is an absolutely difficult task, we can consider a box (or Hypercuboid) in the parameter space. Assume the parameter space is an L-dimension Euclidian space, we can define the concept of box: For each acceptable minimum $w$, its box $M_w$ is an L-dimension hypercuboid with center w. Each edge of the box is taken to be parallel to one weight axis. Let $\\Delta w(x)$ be the maximal values such that for all L-dimensional vectors k whose components ${k_ij}_{i=1,\\dots,n}$ are restricted by $|k_ij| \\le \\Delta w_ij(x)$. We can find this box with the constraint: $\\mathbf{E}(net(w), net(w+k), X) = \\sum_{x \\in X} ||o(w,x) - o(w, x+k)||^2 < \\epsilon$ ,where the $\\epsilon$ represent for the tolerable output change. Note that we assume the target function f is continuous here. Box’s volume is defined as $V(\\Delta w(X)) = 2^L \\Pi_{i,j}\\Delta w_{i,j}(x)$ 2. Information entropy of the minimaLet $B$ denote the bits required to describe the weights, whereas the number to describe the $y_p$,given $w$, can be bounded by fixing $\\mathbb{E_{tol}}$. In other words, we can control the smoothness of the function by choosing tolerance error. $B$ can be written as: $B(w, X_0) = -log(\\frac{1}{2^L}V(\\Delta w)) = \\sum_{i,j} -log(\\Delta w_{i,j})$ Of course, we hope the entropy as low as possible. Therefore, the authors recommend a penalty term into the objective function: $E(w, D_0) = E(net(w), D_0) + \\lambda B(w, X_0)$ , where $B(w, X_0) = \\sum_{x_p \\in X_0} B(w, x_p)$ 3. An Introduction to Gradient DescentBefore the introduction to GD methods, let’s define the optimization problem first. We assume the prediction function $f$ has a fixed form and is parameterized by a real vector $w \\in \\mathbb{R}^d$. Formally, for some given $f: \\mathbb{R}^{d_x} \\times \\mathbb{R}^d \\rightarrow{} \\mathbb{R}^{d_y}$, the family of prediction function is: $\\mathcal{H} = {f(\\cdot,w):w \\in \\mathbb{R}^d}$ Ideally, we want to minimize the expected risk. We assume there exists a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$, where $\\Omega=\\mathcal{X}\\times \\mathcal{Y}$, $\\mathcal{F}$ is the filtration of $(x,y)$, and $\\mathbb{P}$ is the joint probability measure of $(x, y) \\in \\Omega$. The expected risk is defined as: $R(w) = \\int_{\\mathbb{R}^{d_x}\\times\\mathbb{R}^{d_y}}l(f(x;w),y)d\\mathbb{P}(x,y) = \\mathbb{E}(l(f(x;w),y))$ ,where function $l(\\cdot, \\cdot)$ is the loss function, which measure the difference between prediction and labels. The following statements are all based on the optimization problem we defined above. (Reference: An overview of gradient descent optimization algorithms, Sebastian Ruder) It is worth explaining that the convergence speed depends both on the data set and hyperparameters of optimizers. Generally, if the optimizer converge at a slow speed or diverge sometimes, we can adjust the initial step scale. 3.1 Full Gradient DescentThe Gradient Descent is an first order iterative method for finding the local minimum of a differentiable function, which also provides the core optimization methodology in machine learning. Given a function $f(x)$, the basic GD method can be written as: $x^{t+1} = x^{t} -\\eta \\nabla f(x^{t})$ , where $\\eta$ is represented for learning rate. In full GD method, we have to run through all the data samples in our training set to accomplish one update for a parameter. Therefore, if the size of data set is very large, the update process will take a lot of time. More specifically, the full GD method can be written as: $w^{t+1} = w^{t} - \\eta \\nabla_w \\sum_{i = 0}^{n}{l(f(x_i;w_t), y_i)}$ 3.2 Stochastic Gradient DescentThe origin of Stochastic GD method is stochastic approximation. Stochastic GD method is a specific case of Robbins-Monro algorithm. The stochastic GD can be written as: $w^{t+1} = w^{t} - \\eta \\nabla_w l(f(x_i; w_t),y_i)$ In other words, we use only one sample (or a subset of samples) to update the parameter. Compared to full GD method, the SGD method often converges much faster. Note that ${w_t}_{t\\ge0}$ is a stochastic process (a family of random variables defined on the same probability space) which is defined on the filtration generated by $x_i$. Note that we actually want to reduce the expected risk function, which implies that the expectation of gradient should be the best choice. From this perspective, the variance of estimation based on a single data point is much higher than the average over whole samples. Therefore, SGD may lead to a more oscillated convergence compared to full GD (but still good enough). 3.3 SGDMEach update step in SGDM is a combination of the steepest descent direction and the most recent iterate displacement. The iteration process can be written as: $v_k = \\gamma v_{k-1} + \\eta \\nabla_w l(f(x_i; w_k),y_i)$$w_{k+1} = w_k - v_k$ The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. This method is most effective when the parameters work differently in the system. For example, the data generated linearly with $w_1 = 100$, and $w_2 = 0.1$. 3.4 AdaGradThe AdaGrad method adjusts the learning rate with respect to the the history gradients. The learning rate gradually decreases with the decrements of the risk function. We have articulated that the unsuitable learning rate may lead to oscillation when we approach the local optima. This method update the learning rate in adaptive way. More specifically, $ g_{k} = \\nabla_w{l(f(x_k;w_k), y_k)}$$ w_{k+1} = w_{k} - \\frac{\\eta}{\\sqrt{G_k+ \\epsilon}}\\odot g_{k} $ , where $G_{k}$ is the sum of the square of the gradients up to the $k$-th update, and $\\epsilon$ is a smoothing term that avoids division by zero. Adagrad’s main weakness is its accumulation of the squared gradients in the denominator, and this in turn causes the learning rate to shrink, which stop us from learning any information from the data. Moreover, although it is still reasonable in convex optimization problem, it is a bad property when we get stuck on the saddle point. However, AdaGrad performs well in sparse data set. 3.5 RMSPropAdaGrad accumulates all past gradients and the learning rate keeps decreasing, which may make the learning much slower over time. RMSProp does not shrink the learning rate as rapidly due to the use of the decaying average. The moving/decaying average of the $G_k$ in the $k$-th update is defined as:$g_k = \\nabla_w{l(f(x_k;w_k), y_k)}$$G_k = \\gamma G_{k-1} + (1-\\gamma) g_k^2$ The gradient update under the following method: $w_{k+1} = w_{k} - \\frac{\\eta}{\\sqrt{G_k + \\epsilon}}\\odot g_k$ According to Geoffrey Hinton’s lecture Notes, the $\\gamma$ here was 0.9. 3.6 AdaDeltaAdaDelta made some extra improvements compared to AdaGrad and RMSProp (actually, they are developed independently). It reduces AdaGrad’s rapidly decreasing learning rate by using the same exponentially decaying average for past squared gradients as RMSProp. In addition, AdaDelta is aware that, in RMSProp, the base unit of the gradient is $\\frac{1}{x’s; unit}$. That is if we assume WLOG that the value of risk function have no base unit, then consider $\\frac{\\partial f}{\\partial w}dw$ should have no base unit, which means the base unit of the gradient is $\\frac{1}{x’s; unit}$. We should notice that the RMSProp method changes the base unit of $w$. Therefore, the authors suggested to revised this problem with: $W_k = \\gamma W_{k-1} + (1-\\gamma)w_k^2$$w_{k+1} = w_{k} - \\frac{\\sqrt{W_k + \\epsilon}}{\\sqrt{G_k + \\epsilon}}\\odot g_k$ Learning rate is removed. Thus, we do not need to set a fixed number for learning rate. 3.7 ADAMAdaptive Moment Estimation (Adam) computes adaptive learning rates considering the first and second order information:$M_t = \\beta_1 M_{t-1} + (1-\\beta_1) g_{t}$$V_t = \\beta_2 V_{t-1} + (1-\\beta_2) g_{t}^2$ Since $M_t$ and $V_t$ are initialized with 0, then the calculation of the first moment and second moment are biased towards zero, which is can be problematic within the first few time steps and when $\\beta$’s are close to $1$. To deal with that, we can use the bias-corrected estimations of $V_t$ and $M_t$, which can be written as:$\\hat{M_t} = \\frac{M_t}{1-\\beta_1^t}$$\\hat{V_t} = \\frac{V_t}{1-\\beta_2^t}$,where $\\beta_1^t$ and $\\beta_2^t$ is the t power of the $\\beta_1$ and $\\beta_2$ Then update process can be written as: $w_{k+1} = w_{k} - \\eta\\frac{M_k}{\\sqrt{V_k} + \\epsilon}$ The authors proposed default values of 0.9 for $\\beta_1$, 0.999 for $\\beta_2$, and$1e-8$ for $\\epsilon$. Adam prefers flat minima in the error surface, which can avoid over-fitting in some extent. 3.8 Disadvantages of SGD and full GDNote that GD and SGD method are sensitive to learning rate, and it is not easy to choose a optimal learning rate. Moreover, the schedule of learning rate (or the evolution of learning rate) is a even more difficult task. The pre-defined learning rate schedule may not adapt to the data set’s characteristics. Last but not least, the loss function or the feasible region of the optimization problem may not be convex, which means our algorithm has to avoid numerous saddle and local minima. Reference","link":"/2020/03/22/Stochastic%20Approximation%20Methods-Notes-2/"},{"title":"Mathematcial Programming-Notes-Part 1-Strong Duality in Linear Programming","text":"Programming is the basis for a wide range of fields. This blog summerized the sufficient conidtions for strong duality. The strong duality of linear programming is a summery of Mathemetical Programming lecture notes (David P. Williamson). 1. Separating Hyperplane Theorem1.1 Weierstrass TheoremLet $C \\subset \\mathbb{R}^n$ be a closed, non-empty and bounded set. Let $f$: $C \\rightarrow \\mathbb{R}$ be continous on $C$. Then $f$ attains a maximum (and a minimum) on some point of C. 1.2 Separating Hyperplane TheoremLet $C \\subset \\mathbb{R}^n$ be closed, non-empty and convex set. Let $y \\notin C$, then there exists a hyperplane $a \\ne 0$, $a \\in \\mathbb{R}^n, b \\in \\mathbb{R}$, such that $a^Ty > b$ and $a^Tx","link":"/2020/03/25/Mathematical%20Programming-Notes-1/"}],"tags":[{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Katex","slug":"Katex","link":"/tags/Katex/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Kalman Filter","slug":"Kalman-Filter","link":"/tags/Kalman-Filter/"},{"name":"Optimal Control","slug":"Optimal-Control","link":"/tags/Optimal-Control/"},{"name":"N-factors Gaussian Model in Finance","slug":"N-factors-Gaussian-Model-in-Finance","link":"/tags/N-factors-Gaussian-Model-in-Finance/"},{"name":"Statistics","slug":"Statistics","link":"/tags/Statistics/"},{"name":"GMM","slug":"GMM","link":"/tags/GMM/"},{"name":"Mathematical Programming","slug":"Mathematical-Programming","link":"/tags/Mathematical-Programming/"}],"categories":[{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"},{"name":"Notes","slug":"Notes","link":"/categories/Notes/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"}]}