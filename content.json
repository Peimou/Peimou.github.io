{"pages":[{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"About Author","text":"About me见字如我。","link":"/about/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"CUDA-Notes-Part1-Set up CUDA computation for Tensorflow and Tensorflow2","text":"I have to solve the GPU computing toolkits problem several times since different project frequently changes between Tensorflow and Tensorflow2. This tutorial concludes the details in setting up the GPU environment for Tensorflow. I won’t introduce how to set up python environment and anything about hardware in this blog. This tutorial is for Windows OS (The logic also works in other OSs, but the links will not be available anymore). 1. PrerequisitesIn this blog, I will discuss about how to setup GPU environment for Tensorflow and Tensorflow2. Let’s create two virtual environment for DL research first. In this blog, I will use conda as the environment management tool. 1.1 Create virtual environmentIn Anaconda Prompt, create two new virtual environment for python. For example, 1create create -n ENV_NAME python=3.7 For Tensorflow, I created a new virtual env called DLrshenv_tf1; for Tensorflow2, I created a new virtual env called DLrshenv. 1.2 Software requirement Visual Studio 2015. Note that CUDA only support VS2015. Any other version will lead to a failed installation for CUDA Toolkits. May need a subscription (it’s free). CUDA Toolkit 10.1 for Tensorflow2 and CUDA Toolkit 10.0 for Tensorflow. The difference is ‘cudart64_101.dll’ and ‘cudart64_100.dll’. In CUDA Toolkit 10.1, the ‘cudart64_100.dll’ was updated to ‘cudart64_101.dll’. cuDNN for CUDA 10.1 and cuDNN for CUDA 10.0 NVIDIA driver. (As a GPU user, I think you are very familiar with it.) 1.3 How to test the prerequisites?Open cmd, and try 1nvcc -V Note that if you installed two CUDA version like 10.1 and 10.0, the return will be the last one your installed, which should be consistent with the system variable CUDA_PATH. 2. Python env setupInstall Tensorflow and Tensorflow2 in two virtual envs. For Tensorflow2: 123activate DLrshenvpip install tensorflowpip install tensorflow-gpu For Tensorflow: 123activate DLrshenv_tf1pip install tensorflow==1.15pip install tensorflow-gpu==1.15 3. cuDNNCopy all the file under CUDA into the path you install CUDA. Generally, it should under 1C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0 1C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1 4. Environment variablesAdd bins and libnvvp in two paths to the User’s variables. Under Path, there should be four new variables include: 1234567C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvpC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\binC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\libnvvpC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin","link":"/2020/10/14/CUDA-notes-1/"},{"title":"A review of Merton Problem-Part 1","text":"Keywords: Portfolio selection, utility function, Inada condition, square-integrable, Ito-process. 1 The wealth processConsider a market with $l + 1$ assets, and an m-dimensional Brownian motion $W$ defined on the filtered probability space $\\Big(\\Omega, \\mathcal{F}, \\mathbf{P}; (\\mathcal{F}_t)_{t > 0}\\Big)$. One of the asset is risk-free, which means the return of this asset is $\\mathcal{F}_{t-1}$ measurable. The dynamic of this asset can be written as: $dS^0(t) = r(t)S^0(t)dt$ , where $r(t)$ can be random but always $\\mathcal{F}_{t-1}$ measurable. The dynamic of other assets can be written as: $dS(t) = \\mu(t) S(t) dt + S(t) \\sigma(t) dW_t$ , where $\\mu(t)$ is a diagonal matrix, and $\\sigma\\sigma^T$ is the covariance matrix. At time t, the number of the $i$th asset is $N^{i}(t)$; the dollar amount is $\\pi^{i}(t)$. Let’s assume the agent’s trading is self-financing. Therefore, the dynamic of the wealth process can be written as: $dx(t) = \\sum_{i=0}\\limits^l N^{i}(t)d S^{i}(t) = N \\mu S_tdt + N \\sigma S_t d W_t = \\mu S_t N dt + \\sigma S_t N dW_t$ $dx(t) = \\mu \\pi(t) dt + \\sigma \\pi(t) dW(t) = r(t) \\sum(\\pi(t)) dt + (\\mu - r(t)) \\pi(t) dt + \\sigma \\pi(t) dW(t)$ Note that by the definition of self-financing, $\\sum(\\pi(t)) = x(t)$. Therefore, the wealth process can be written as: $dx_t = [r(t)x(t) + B^{\\prime}(t)\\pi (t)]dt + \\sigma (t) \\pi (t) dW_t$ ,where $B(t) = (\\mu^{1}(t) - r(t), \\dots, \\mu^{l}(t) - r(t))$ (the excess of the return) $\\sigma(t) = (\\sigma^{ij}(t))_{l \\times m}$. Note that we need to add extra constraints (square integrable) to the portfolio process $\\pi_{t}$, i.e. $\\mathbb{E}(\\int_{0}^T \\pi(t)^T\\pi(t)dt) < +\\infty$. This constraint has economic sense since we can only use limited leverage in the investment. The portfolio follows the above constraints is called admissible portfolio. 2 Consumption DecisionAn admissible consumption plan should also be non-negative and square-integrable in $[0, T]$. Note that although we have $l + 1$ assets in the market, due to the self-financing and consumption plan, the freedom degree in the asset allocation is $l$. Therefore, let’s just choose the risky asset. Another interesting setting in Merton problem is quite implicit. The system is generated by the filtration generated by the Brownian motion, which means all the uncertainties comes from the randomness of the Brownian motion. However, the drifts and diffusions are $\\mathcal{F}_t-$adapted. This property indicate the implementation details, we can use the current information to determine the drift and diffusion. 3 Utility functionA concave utility function indicate the risk reversion condition. 1) U is defined on $\\mathbb{R}^{+}$ and $U(0) = 0$. 2) $\\dot{U}(0) = +\\infty$ 3) $\\dot{U}(+\\infty) = 0$ Utility function works as the pivot role in the portfolio selection. Utility function works as a paneity of the variance (or risk), which make the investment problem to a second order issue.","link":"/2020/11/04/A-review-of-Merton-Problem/"},{"title":"Admissibility and consistency of informed search","text":"Keywords: BFS, DFS, UCS, A* search, tree search, graph search, heuristic function Search and dynamic programming are two fundamental methodologies with a strong connection. (Recall Feynman-Kac and Fokker-Planck equation) 1. Definition1.1 State space and search problemA formal formulation of a search problem requires four elements: a) A state space: the set of all possible states b) A successor function: $\\text{state}_t \\times \\text{action} \\xrightarrow{} \\text{state}_{t + 1} \\times \\text{cost}$ c) Start state d) Goal state A path from start state to goal state is called a plan, while the order in which states are considered is determined by a strategy. 1.2 World state and search stateThese are two important concepts. The world state concludes information like the number of rows and columns in a maze, while the search state conclude the information which is essential in the planning and successor function, i.e the information that may change given an action. Remark: 强化学习的弹球游戏的讨论中实际上涉及到了这个问题，我们通常上所描述的状态是指search state，而world state在强化学习中是需要我们建模的部分，或者说决定了value function 或者 Q 函数的性质。在SC问题中，world state又体现为HJB equation的边界条件。 1.3 The element of search (finding a plan)1) The fringe of partial plans derived from the tree. (fringe: 穗，即稻、麦等谷物成熟后聚生在茎干顶端的花或果实。这理解为一次扫描的结果，或子搜索问题，或类似子节点的概念，从初始点开始的搜索问题可以转变为与所有与初始点连接的点为初始点的搜索问题) 2) Expansion of the fringe. 3) Strategy of expansion 1.4 The evaluation of the search strategyCompleteness: The strategy will find a solution if there does exist one. Optimality: Is the strategy guaranteed to find the lowest cost path to a goal state. (Path is the action set from the start state to goal state) Remark: Optimality means if we can find the path with lowest cost, rather than the number of expansion or time we spend on the search process. 2. A review of uninformed searchBranching factor $b$: the increase in the number of nodes on the fringe each time a fringe node is dequeued and replaced with its children is $O(b)$ 2.1 DFSDue to the properties of graph, the DFS is not complete due to the properties of the fringe representation . (因为我们总是弹出最后一个进来的元素，所以栈中无法保存所有已经遍历过的元素，如果存在环，即节点连入节点本身，我们在栈中留不下这个节点, 我们甚至没法发现这个成环的问题，因为环中的前一个节点总是被弹出). DFS is not optimal as well since we cannot evaluate the cost. We just expand given the ‘leftmost’ strategy. The time complexity is $O(b^m)$ in the worst scenario if the length of tree has m layers. (一下有b个节点，b个节点每个节点下又有b个，共m层，全需要遍历). The space complexity is $O(mb)$ (according to fringe representation, we at most keep one branch of each layer) 2.2 BFSWe want to traverse the graph or tree layer to layer. Therefore, we need to output the oldest enqueued object to represent the fringe. BFS is complete due to the fringe representation (BFS 是能够发现环的，因为每次进队列的时候，本次fringe的上一个元素还留着). Let $s$ denote the shallowest solution. Apparently, the time complexity is $O(b^s)$ ($1 + b + b^2 + \\dots + b^s$). The space complexity is $O(b^s)$. The BFS is optimal if the cost of all the actions is a constant. For example, the cost of a path is defined as the length of the actions set. 2.3 UCS (Uniform Cost Search) The strategy of expansion is to choose the lowest cost fringe node. Therefore, we always use a heap-based priority queue to represent the fringe, where the weight is the path cost from the start node to the current state, or the backward cost. The properties of this fringe representation is that the nodes with lower backward cost will never be visited later than the ones with higher backward cost if the cost of each edge is nonnegative (反证易得，这个二级结论后面还要用). USC is complete and optimal is the cost of each edge is nonnegative (Recall Dijkstra algo). Let C denote the optimal cost while $\\epsilon$ denote the minimal cost. We must explore every nodes at depths from 1 to $\\frac{C}{\\epsilon}$, i.e. $O(b^{\\frac{C}{\\epsilon}})$. The same as the space complexity. BFS is a special version of USC. 3. Informed searchAlthough we can find the optimal path with USC, it can be fairly slow since we need to expands in every direction. We can fasten the searching process if we can know the effect of our action. We want to go through the nodes with probably lower total cost first. The searching direction is the focus of informed search. 3.1 Heuristic function$state \\times problem \\xrightarrow{} cost$. This function only depends on the world state, i.e. the problem here. 3.2 A* search Identical to USC (Priority queue with estimated cost as the weights). Given a good enough heuristic function, A* search is both complete and optimal. Let $n$ denote the current state, $g(n)$ denote the backward cost, i.e. from the start state to the current state, $h(n)$ denote the value of the heuristic function, $f(n)$ denote the estimated total cost, i.e. $g(n) + h(n)$. 3.2.1 A* tree searchThe condition required for optimality when using A* tree search (the construction of a tree requires the children have at most one parent node, i.e. no cycle) is known as admissibility. Define $h^*(n)$ is the true optimal forward cost to reach a goal state form a given node n. The admissibility constraint of a heuristic function is defined as: $\\forall n, 0 \\le h(n) \\le h^*(n)$. Theorem If the admissibility constraint holds, the A* tree search will be complete and optimal. Proof: Assume two reachable goal states A, B are located in the search tree for a given search problem. Some ancestor n (on the optimal path)of A must be on the fringe. We claim n will be selected for expansion before B (不妨理解为A,B是能通线终点的不同两个节点， 从A，B后所有通先最终节点的路径相同，因此A,B区分了两个path). Because of the fact that 1) $g(A) < g(B)$ (The definition of optimal) 2) h(A) = h(B) = 0 (By the definition of goal state) 3) $f(n) \\le f(A)$. Due to the admissibility of heuristic function, $f(n) = g(n) + h(n) \\le g(n) + h^*(n) \\le g(A) = f(A)$ (Note that by Bellman principle the lowest backward cost of the goal state indicate that the for any n on the optimal path $g(n) + h^*(n) = g(A)$) Therefore, $f(n) \\le f(A) \\le f(B)$ By the construction of the fringe representation of the UCS, n will be visited before B. 3.2.2 A* graph searchThe restriction of heuristic function to make the A* graph search optimal is called consistency. For any A, C, $h(A) - h(C) \\le cost(A, C)$. For any successor $n^\\prime$ of current state n, $f(n^\\prime) = g(n^\\prime) + h(n^\\prime) = g(n) + \\text{cost}(n, n^\\prime) + h(n^\\prime) > g(n) + f(n) = f(n)$. Therefore, the value of estimated cost on any path will be nondecreasing. 3.3 The dominance of heuristic functionFor any state n, if $h_a(n) \\ge h_b(n)$, then $h_a$ will be a better heuristic function compared to $h_b$, since the estimation of the total cost will be closer to the true cost.","link":"/2021/03/13/Admissibility-and-Consistency%20-of-Informed-search/"},{"title":"A review of autocovariance function for stationary process","text":"Keywords: Non-Negative Definiteness, Autocovariance Function, Sample Autocovariance Function, Kolmogorov’s theorem The autocovariance comes from the transition probability between states in $\\mathcal{F}_t$, i.e. the probability measure of the probability space. Therefore, just as the martingale, Markov property and independency, the autocovariance is the consequence of specific probability measure. (Kolmogorov theorem某种程度上解决了随机过程逻辑构型上的重要环节，我们能够因此判断论域是否为空，是防止我们掉入逻辑陷阱的重要保险绳) 1. Stationary Process1.1 The existence of a stochastic processThe distribution of a Stochastic Process Let $\\mathcal{T}$ be the $\\sigma-$algebra defined on discrete time $\\{t = (t_1, t_2, \\dots, t_n) \\in T^n, t_1 < t_2 < \\dots < t_n, n = 1, 2, \\dots\\}$. Then the (finite-dimensional) distribution functions of $\\{X_t, t \\in T\\}$ are the functions $\\{F_t(\\cdot), t \\in \\mathcal{F}\\}$ defined for $t = (t_1, t_2, \\dots, t_n)^\\prime$ by $F_t(\\textbf{x}) = \\textbf{Pr}(X_{t1} \\leq x_1, \\dots, X_{tn} \\leq x_n )$ The distribution function of a stochastic process greatly depends on the time set. We only consider the discrete time problem. Kolmogorov’s Theorem The probability distribution functions $\\{F_t(\\cdot), t \\in \\mathcal{F}\\}$ are the distribution functions of some stochastic process if and only if for any $n \\in \\{1, 2, \\dots\\}$, $t \\in \\mathcal{F}$ and $1 \\leq i \\leq n$, $\\lim\\limits_{x_i \\xrightarrow{} \\infty} F_t(x) = F_{t(i)}(x(i))$ where $t(i)$ and $x(i)$ are the $(n - 1)-$component vectors obtained by deleting the $i$th components of $t$ and $x$ respectively. If $\\phi_t(\\cdot)$ is the characteristic function corresponding to $F_t(\\cdot)$, the Kolmogorov’s theorem can be written as: $\\lim\\limits_{u_i \\xrightarrow{} 0} \\phi_t(u) = \\phi_{t(i)}(u(i))$ Remark: Note that it is always a problem if the stochastic process we defined exists. Some most common stochastic processes, like Brownian motion and Poisson process, have strong constraints and properties. It is a must to check the definition since most logic errors are originated from the empty set. (marginal distribution 存在) 1.2 Stationarity The time series $\\{X_t, t \\in \\mathbb{Z}\\}$ is said to be stationary if $(i) E(|X_t|^2) < \\infty$ $(ii) E X_t = m$ $(iii) \\gamma_X(r, s) = \\gamma_X(r + t, s + t) = \\gamma_X(|r- s|, 0) = cov(X_r, X_s)$ for all $r, s, t \\in \\mathbb{Z}$ Remark: Stationarity only considers the limited properties of the stochastic process (the first moment and the second moment), while the strict stationarity considers all the properties of the distribution of stochastic process. When we are conducting statistical inference, the LLN, CLT, and Levy stable distribution are only based on the second-order properties. Both trend and seasonality are non-stationary features. 2. The autocovariance function of a stationary process2.1 Elementary Properties$\\gamma(0) \\ge 0$ (显然) $\\gamma(h) = \\gamma(-h)$ (阿贝尔群， 易证) $|\\gamma(h)| \\le \\gamma(0)$ (Cauchy-Schwarz inequality) Proof: $|cov(X_{t+h}, X_t)| \\le (Var(X_t))^{\\frac{1}{2}}(Var(X_{t+h}))^{\\frac{1}{2}} = \\gamma(0)$ 2.2 Non-Negative DefinitenessA real-values function on the integers, $\\kappa: \\mathbb{Z} \\xrightarrow{} \\mathbb{R}$, is said to be non-negative definite if and only if $\\sum_{i, j= 1}^n a_i k(t_i - t_j)a_j \\ge 0$ for all positive integers n and for all vectors $\\textbf{a}=(a_1, \\dots, a_n) \\in \\mathbb{R}^n$ and $\\textbf{t} = (t_1, \\dots, t_n) \\in \\mathbb{Z}^n$ . 2.3 Characterization of Autocovariance functionsA real-valued function defined on the integers is the autocovariance function of a stationary time series if and only if it is even and non-negative definite. Remark: 这条定理说明了非负定函数和平稳随机过程的关系。从下面的证明中我们亦能发现，所有定义在离散时间上的非负定函数一定能找到对应的定义在离散时间上的随机过程（不止一个，但必然存在），平稳的随机过程的自相关函数也必定是非负定的。 Proof: $(i)$ For any $a \\in \\mathbb{R}^n$, $t \\in \\mathbb{Z}^n$, and $Z_t = (X_{t1} - \\mathbb{E}(X_{t1}), X_{t2} - \\mathbb{E}(X_{t2}), \\dots, X_{tn} - \\mathbb{E}(X_{tn}))$. $0 \\le Var(a^\\prime Z_t) = a^\\prime \\mathbb{E}Z_t Z_t^\\prime a = a^{\\prime} \\Gamma_n a$ $(ii)$ Let $K = [\\kappa(t_i - t_j)]^n_{i, j = 1}$. Since $\\kappa$ is non-negative definite, $k$ is also non-negative. There exists a characteristic function $\\phi_t(u) = \\exp(-u^\\prime K u /2)$. It is clear that $\\phi_{t(i)}(u(i)) = \\lim\\limits_{u_i \\xrightarrow{} 0} \\phi_t(u)$ (线性代数问题，等于把$K$中第i行，第i列全都换成了0， 不影响其他entries). According to Kolmogorov’s theorem, there exists a stochastic process whose covariance matrix is $K$. Remark: For any non-negative definite function, we can find a joint Gaussian distribution. (这其实是一个构造性证明，从感觉上看，不同分布肯定有相同的ACF，因此只要找到一个就得证。) 2.4 Sample Autocovariance function of an observed seriesThe sample ACF of $\\{x_1, x_2, \\dots, x_n\\}$ is defined by: $\\hat{\\gamma}(h) := n^{-1}\\sum_{j = 1}^{n - h}(x_{h + j} - \\bar{x})(x_{j} - \\bar{x})$. Note that the coefficient must be $n^{-1}$ to make sure the non-negative definite of ACF. Remark: 虽然我们有很多可能的估计量，但是这里的ACF是定义的，也就是说一旦我们提到样本ACF，定义就是这样，而不用考虑是不是UMVE或者UMVUE, admissible 以及 minimax这样的话题。","link":"/2021/01/21/A-review-of-autocovariance-function-for-stationary-process/"},{"title":"Foundation of Machine Learning-Part1-PAC","text":"keywords: Machine learning, PAC, PAC learnable 1. Definitions: the formulation of the learning problemsInput space and the set of labels : Let $\\mathcal{X}$ denote the set of all possible examples or instances (sometimes also called input space). The set of all possible labels or target values is denoted by $\\mathcal{Y}$. By default, the set of all possible values will be ${0, 1}$, i.e. the so-called binary classification. The definition of concept: Let $c: \\mathcal{X} \\xrightarrow{} \\mathcal{Y}$ denote a mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. For example, we can formulate the concept of a triangle with a plane, where the points on the plane are denoted as ${0, 1}$. The concept of triangle can be formulated as the point in a specific shape is label of 1. Then the concept of a triangle can be well defined. A set of concepts will be denoted as $C$. The formulation of learning problem: We assume the samples are IID according to some fixed but unknown distribution D. The learner considers a fixed set of possible concepts $H$, called a hypothesis set, which may not be the same as $C$ (but with the same input space and output space). The learner receives a sample $S = (x_1, x_2, \\dots, x_m)$ drawn iid with distribution of D, as well as the labels $(c(x_1), \\dots, c(x_m))$. The learner’s task is to use the labeled sample S to select a hypothesis $h_S \\in H$ that has a small generalization error with respect to the concept c. An algorithm is defined as a mapping from $x$ to the selected hypothesis. The learning task is to formulate the algorithm that can find the mapping from input space to output space(state space) that has the minimized generalization error based on distribution D. The definition of generalization error and empirical error : Given a hypothesis $h \\in H$, a target concept $c \\in C$, and an underlying distribution D, the generalization error or risk of h is defined as: $\\mathbb{E}_{x \\sim D}[1_{h_s(x) \\neq c(x)}] = Pr_{x \\sim D}(h_s(x) \\neq c(x))$ Note the generalization error cannot be directly evaluated since we do not know both the of the distribution D and the concept c. Therefore, we need to introduce the empirical error based on the observed samples. Empirical error: Given a hypothesis $h \\in \\mathcal{H}$, a target concept $c \\in C$, and a sample $S = (x_1, x_2, \\dots, x_m)$, the empirical error can be defined as $\\hat{R}(h) = \\frac{1}{m}\\sum_{i=1}^m1_{h(x_i)\\neq c(x_i)}$ Note that we do not know $c$. However, we own the knowledge of the label set. Comments: The expectation of the empirical error given a set of iid samples is exactly the same as the generalization error. (以及这块和statistical decision没啥不一样的，熟悉下notation就行了) The definition of PCA learning: A concept class $C$ is said to be PAC-learnable if there exits an algorithm $\\mathcal{A}$ and a polynomial function $poly(\\cdot, \\cdot, \\cdot, \\cdot)$ such that for any $\\epsilon > 0$ and $\\delta > 0$, for all distribution D on $\\mathcal{X}$ for any target concept $c \\in C$, the following holds for any sample size $m > poly(\\frac{1}{\\epsilon}, \\frac{1}{\\delta}, n, size(c))$: $Pr_{S \\sim D^m}[R(h_S) \\leq \\epsilon] \\ge 1 - \\delta$ , where n is the upper bound on the cost of computational representation of any element $x \\in \\mathcal{X}$ and $size(c)$ the maximal cost of the computational representation of $c \\in C$. (即需要多少成本才能够表征出input space和output space, 联系Monte Carlo在高维度的收敛问题，以及高维球的体积都集中在壳处可以推知，表征维度对于可学习型是存在影响的，我们必须要求一个能被多项式压住的东西。需要无穷样本的问题就是不可学习的问题。) 2. Guarantees for finite hypothesis sets: the consistent case and inconsistent case2.1 Consistent caseLet $\\mathcal{H}$ be a finite set of functions mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. Let $\\mathcal{A}$ be an algorithm that for any target concept $c \\in \\mathcal{H}$ and $i.i.d$ sample S returns a consistent hypothesis $h_S$: $\\hat{R}_s(h_s) = 0$. Then, for any $\\epsilon, \\delta > 0$, the inequality $\\mathbb{P}_{S \\sim D^m}[R(h_s) \\le \\epsilon] \\ge 1 - \\delta$ holds if $m \\ge \\frac{1}{\\epsilon} (\\log|\\mathcal{H}| + \\log \\frac{1}{\\delta})$ The above statement is equivalent to the statement of the generalization bound: for any $\\epsilon, \\delta > 0$, with probability at least $1 - \\delta$, $R(h_s) \\le \\frac{1}{m} \\big(\\log |\\mathcal{H}| + \\log \\frac{1}{\\delta} \\big)$","link":"/2021/05/24/Foundation-of-Machine-Learning-Part1-PAC/"},{"title":"C++并发cheating Paper","text":"keywords: RAII, daemon threads, race condition 注意：这篇博客是对《C++并发编程实战》的整理，在原文的基础上重新梳理逻辑关系和要点分析，无任何原创内容。 1. 线程管理1.1 线程的初始化每一个线程都需要有一个初始函数（一个函数指针），std::thread可以和任何可调用类型(callable，通俗的说就是后面可以接括号的)一起使用，即可以把一个实例传递给std::thread的构造函数。注意实例被复制到新线程的栈里，并从这个新的储存器中调用；另外，如果传递一个临时的且未命名的函数对象（函数对象是重载了函数调用符的对象，优点是可以有自己的状态，并且有自己的类型，可以用于初始化C++的stl模板），那么编译器会将这个线程的初始化编译为返回一个std::thread 对象的函数的声明，此时的初始化方法是使用std::thread my_thread{callable_object()}来避免这种情况；另一种避免临时的，未命名的函数对象的编译问题的方法是使用lambda表达式。 1.2 线程等待和分离在线程初始化结束后，我们需要显示地确定主线程是等待新线程(join)还是让这个线程继续运行(detaching)。只有在线程joinable属性为True的时候，我们才能进行join或者detach。 1.2.1 RAII技巧为了防止任务在主线程发生异常的情况下没有及时join新线程，导致主线程没有等待新线程结束而提前终止的情况，我们应使用RAII（资源获取即初始化）的方法。这种技巧主要依靠构造函数的逆序销毁特性，在新线程初始化后，在主线程中新建一个thread_guard类，并在这个thread_guard()析构的时候判断新线程是否是joinable的, True则调用join。注意这个判断joinable是很重要的一步，防止在异常发生时新线程已经被等待，此时如果继续调用join则会报错。另外构造RAII时的细节是一定要把thread_guard的复制构造函数和赋值操作符给去掉（在c++11中使用 “= delete“更加优雅可读些），防止这个thread_guard在新线程的生命周期之外继续存续。在互斥锁中，为了保证unlocking被顺利调用，也会用到类似的技巧。 1.2.1 守护线程及其应用场景如果我们将线程分离(detaching)，那么将没有办法继续与这个线程通信或者继续调用这个线程的join。此时这个线程的控制权会交給C++运行时库，以保证线程相关的资源在线程推出后被正常回收。被分离的线程也别叫做守护线程(daemon thread, 参考守护进程的概念)。这种线程一般在整个应用程序的周期都存续，并在后台运行。常见的应用场景包括监控文件系统，清除对象缓存中未使用的对象或者优化数据结构等。 1.3 给线程传递参数如果将主线程中的数据移交给新线程，那么这个数据就会被copy。即使我们在新线程调用的函数中将这个数据声明为引用，我们也仅仅是在新线程内部调用这个数据的时候使用引用机制。总之，在新线程中使用主线程的数据肯定是要深拷贝的，不要抱有不切实际的幻想。 1.3.1 将指针直接传入新线程中的后果：悬浮指针不要使用指针传递参数到新线程里，否则主线程结束后，新线程中就无法找到这个指针指向的对象，产生悬浮指针。一个可能的解决方法是强制转换为原子类型（如果可以的话，例如char数组转化为string）。 1.3.2 在新线程直接传输数据（数据对象而非引用或者指针）的后果和解决方法结果是：在线程中对数据进行的更改将在新线程结束后一起被销毁，即没有任何在新线程中的操作结果被保留。解决方法是使用std:ref。这其实是一个reference wrapper，可以通过一个右值初始化，其.get()方法返回的就是正常的引用。使用ref的另一个好处是，在新线程析构时，其栈中的内存也会被释放，因此被复制的数据是不会被保留的，如果多线程计算最终是为了返回数据，记得加ref。 1.3.3 move 语义在多线程上的优化如果给函数或者新线程传递一个右值，那么这个临时变量的所有权会自动转移到新函数或者新线程中，原scope中的改变量会被释放。如果我们确定某个很大动态数据结构不会再在原空间内使用，那么我们可以强行把左值move到右值，从而节约内存开销。 2. 线程间共享数据和竞争条件竞争条件是指运行结果依赖于线程相对操作顺序的一些事物。一般race condition指恶性的情况，即可能发生未定义的事宜。避免竞争条件在并发中的影响的方式主要有优化数据结构中的不变量（无锁编程， lock free programming）以及使用互斥锁（mutex，mutual exclusion）保护数据。 2.1 互斥元保护共享数据互斥锁是跟随数据资源的。在共享数据之前，锁定于该数据相关的互斥元，访问完成后，解锁互斥元。C++的线程库保证了一个线程在访问一个互斥锁的时候另外一个线程必须等待，直到该数据的互斥元被解锁。在设计互斥元的时候应该避免通过指针过引用传出被互斥元保护的数据。这些数据一旦被传出，那么无论是在主线程还是在其他调用该数据的线程中都可能与其他线程中的调用形成竞争条件。一旦脱离mutex_guard的作用域，数据就不再被mutex保护。所以在多线程的共享数据的时候我们有时不得不返回一个动态结构的副本，以内存开销为代价，所以在设计接口的时候记得避免。 在特定函数中的互斥锁不能保证一个函数调用序列的安全，在设计接口中应该考虑避免这样的情况发生。 2.2 同步结构防死锁如果一个函数中需要调用两个被互斥锁保护的数据块，可能出现的情况就是一个线程锁住了其中一个数据块，而另一个线程锁住了另一个数据块，此时两个线程就会互相等待对方释放互斥元，形成死锁。一个理所当然的处理方法就是在一个函数调用两个被互斥元保护的数据前，把这两个互斥元提前锁定（如果这两个互斥元不能被同时锁定，threading库就会等待直到能够同时锁住为止）。 一些防止死锁的原则和技巧： 这种描述本身其实也体现了防止死锁最基础的原则：在一个函数中只有一个锁（或者把多个互斥元同时锁定，从行为上看只等于有一个锁）；上一个原则衍生出来的另一个准则就是不要在函数内调用其他用户代码，因为这些函数中就可能有锁，从而违背第一原则；如果必需使用多个锁，那么必须用相同的顺序调用(反证法，自证不难)。 在使用lock的RAII时，应该注意unique_lock的性能稍弱于lock_guard, 但是unique_lock有一个非常强力的特性，那就是可以安全地在scope之外转移锁, 例如，某个带锁定的互斥元取决于某个程序的运行状态，而我们又懒得使用conditional variable时。 2.3 优化技巧和锁粒度首先不要给IO bounded的任务上互斥锁，如果不加会出错，就老老实实改API。另外，养成在代码不需要调用共享数据块的时候unlock(注意为了不造成可能的死锁，不要在这种时候放两个lock)的好习惯。","link":"/2021/04/09/C-%E5%B9%B6%E5%8F%91cheating-Paper/"},{"title":"Eigenfunction and Mercer's Theorem","text":"Keywords: linear operator, compact linear operator, Mercer’s theorem, eigenfunction, foundation of kernel trick 1. EigenfunctionsAn eigenfunctions of a linear operator $D$ defined on some function space is any non-zero function $f$ in that space that when acted upon by D, is equivalent to multiplied by some scaling factor called an eigenvalue, i.e. $Df = \\lambda f$ Note that the boundary conditions may greatly limit the value of $\\lambda$. The all possible values of $D$ is sometimes called its spectrum. Moreover, given a inner-product function space, whose inner product is defined as: $< f, g > = \\int_{\\Omega} f^{*}(t)g(t)dt$ ($*$ means complex conjugate) Suppose the function space has an orthonormal basis given by the set of functions $\\{u_1(t), u_2(t), u_3(t), \\dots\\}$ $< u_i, u_j > = \\delta_{i, j}$ (Kronecker delta), such that for any functions can be written as a a linear combination of the basis functions: $f(t) = \\sum_{j = 1}^n b_j u_j(t)$ (The infinite dimension version should be written as integral, e.g. Fourier series) Define a matrix representation of the linear operator D with elements $A_{i, j} = < u_i, Du_j > = \\int_{\\Omega} u^{*}_i(t) D u_{j}(t) dt$ We can write the function $Df(t)$ either on $\\{u_i\\}$ or $\\{Du_i\\}$, i.e. $Df(t) = \\sum_{j = 1}^n c_j u_j(t) = \\sum_{j = 1}^n b_j Du_j(t)$ Taking the inner product of each side of the above equation with an arbitrary basis function: $< Df, Df >_i = \\sum_{j = 1}^n c_{j} \\int_{\\Omega} u^{*}_i(t)u^{*}_j(t)dt = \\sum_{j = 1}^n c_{j} \\delta_{i, j} = c_i = \\sum_{j = 1}^n b_{j} \\int_{\\Omega} u^{*}_i(t)Du^{*}_j(t)dt = \\sum_{j = 1}^n b_j A_{i, j}$ 2. Mercer’s TheoremSuppose K is a continuous symmetric non-negative definite kernel. Then there is an orthonormal basis $\\{e_i\\}$ of $L^2[a, b]$ consisting of eigenfunctions of $T_k$ such that the corresponding sequence of eigenvalues $\\{\\lambda_j\\}$ is nonnegative. The eigenfunctions corresponding to non-zero eigenvalues are continuous on $[a, b]$ and K has the representation $K(t, s) = \\sum_{j = 1}^{\\infty} \\lambda_j e_j(s) e_j(t)$ where the convergence is absolute and uniform. Note that $[T_{K}\\phi](x) = \\int_{a}^b K(x, s)\\phi(s) ds$ $T_{K}$ is defined as a linear operator. $[T_{K}(\\phi_1 + \\phi_2)](x) = \\int_{a}^b K(x, s)(\\phi_1(s) + \\phi_2(s))ds = \\int_{a}^b K(x, s) \\phi_1(s) ds + \\int_{a}^b K(x, s) \\phi_2(s) ds$ In another word, the kernel function should be able to written as: $K_{i, j} = \\phi(x_i)^T \\phi(x_j)$ $\\phi$ is also called basis function, the state space of the mapping function can be a $\\mathbb{R}^n$ space. 2.1 Mercer KernelsLet $\\{x_1, x_2, \\dots, x_n\\}$ be a finite set of n samples from $\\mathcal{X}$. The Gram matrix of X is defined as $\\mathbf{K}(X; \\kappa) \\in \\mathbf{R}^{n \\times n}$ or $\\mathbf{K}$ for short, such that $(\\mathbf{K})_{ij} = \\kappa(x_i, x_j)$. If $\\forall X \\subset \\mathcal{X}$, the matrix $\\mathbf{K}$ is positive definite, $\\kappa$ is called a Mercer Kernel, or a positive definite kernel. (Note that Mercer kernel is symmetric by definition) According to Mercer’s theorem, the kernel should be able to be written as $\\kappa(x, x^\\prime) = \\phi(x)^T \\phi(x^\\prime)$. We can also easily get the basis function through the construction of the kernel function.","link":"/2021/02/19/Eigenfunction-and-Mercer-s-Theorem/"},{"title":"Game Theory-Notes1-Strategic form Games with Complete Information","text":"Keywords: Strategic form games 1. PreliminariesA strategic form game is composed of the set of players $N$, a set of actions $A_i$, and a payoff function $u_i: A \\xrightarrow{} \\mathbb{R}$. Note that the payoff functions may differ among the players, but the domain of the function is the outcome space (the cartesian product of the actions set). The outcome of a game is the actions profile of each player. 1.1 The definition of the rational playersWe just consider scenario where all the players are rational. The basic definition of rational is that the player has well-defined objectives over the set of the possible outcome, i.e. the payoff function should be a well-defined function whose domain is the outcome space. Note that the assumption about rational players may not be a realistic one. An active study of lacking the rational players is called bounded rationality. For a player in the strategic form game, if $u_i(a_1, \\dots, a_n) > u_i(b_1, \\dots, b_n)$, then we understand that player i like outcome $(a_1, \\dots, a_n)$ strictly better than $(b_1, \\dots, b_n)$ 1.2 The formulation of the strategic formWe can formally define a game in strategic form as the tuple: $(N, \\{A_i\\}_{i \\in N}, \\{u_i\\}_{i \\in N})$. More specifically, we sometimes consider the so-called symmetric game which follows two conditions: a) $A_i = A_k$, $\\forall i, k \\in N$ b) $u_i(a_i, a_k, a_{-i, -k}) = u_k(a_k, a_i, a_{-i, -k})$. Note that the second condition implies that the exchange of the actions will lead to the exchange of payoff. The second condition further indicate that the same actions of every two players will lead to the same payoff. 2. Solution concepts of strategic form gameAction $a_i \\in A_i$ weakly dominate action $b_i \\in A_i$ for player i if $u_i(a_i, a_{-i}) \\ge u_i(b_i, a_{-i})$ for all $a_{-i} \\in A_{-i}$, and strictly dominates action $b_i \\in A$ for player i if $u_i(a_i, a_{-i}) > u_i(b_i, a_{-i})$. We say an action strictly dominate if for any other action $a$, $u_i(a_i, a_{-i}) > u_i(a, a_{-i})$. A rational player will not choose the dominated (被占优) action. We can further search for strictly dominated actions in this smaller game, that is, eliminate next the strictly dominated actions of another player requiring dominance only against actions not yet eliminated. The elimination process is called iterated elimination of strictly dominated actions (IESD). Proposition: If both players have strictly dominant actions, then IESD actions leads to the unique dominant strategy equilibrium. Definition: A strategic form game $G$ is dominance solvable if IESD actions leads to a unique outcome.","link":"/2021/03/01/Game-Theory-Notes1-Strategic-form-Games-with-Complete-Information/"},{"title":"GPU-programming-note1","text":"","link":"/2021/07/26/GPU-programming-note1/"},{"title":"Generalized methods of Moments and Newy-West Adjustment-Notes-Part 1-GMM","text":"Newy-West adjustment provides a consistent estimation given autocorrelation in samples. The origin of Newy-West adjustment is closely related to Generalized Method of Moments. This blog includes the motivations, the definition and some properties of Generalized methods of Moments. 1. Motivations of GMMLet $\\Omega$ denote the set of sample points in the underlying probability space in some estimation problem. Let E denote the expectations operator. For a stochastic process ${x_n: n \\ge 1}$ defined in this probability space, we can get a finite segment of one realization of this process, i.e., ${x_n(\\omega_0): 1 \\le n \\le N}$. This sequence can be treated as the observable data sequence. Let moments function $f(\\cdot, \\theta)$ : $R^p \\xrightarrow{} R^q$ be a continuous and continuously differentiable measurable function of $\\theta$ ($\\in$ $C^1$). Intuitively, this function measures the difference between some representation of population and some interested values. To be more specific, the parameter $\\theta$ is a $(p \\times 1)$ vector, and we know $q$ moment based on the underlying distribution (or probability measure). Based on the knowledge of simple algebra, we know the system with $p$ parameters admit solution if we have $p$ constraints. However, we are facing up the situation where we have more constraints then the parameters. Set the population moment conditions such that $E[f(x_i, \\theta_0)] = 0$ (Note that $\\theta_0$ is the solution of equations based on population moments, and it is a deterministic variable given information of population). The associated sample moments are given by: $f_n(\\theta) = \\frac{1}{n} \\sum_{i = 1}^nf(x_i, \\theta)$ We want to estimate $\\theta_0$ based on the solution of the equations $f_n(\\theta) = 0$ (to be more specific the bias to the moments is zero). Note that we have $q - p$ additional moments (if $q > p$), and the remedy for this situation is call GMM, which was introduced by Hansen[1982]. Intuitively, if we cannot find a good enough $\\theta$ to make sure the equation system $f_n(\\theta) = 0$, then we should at least try to keep the sample moments are very close to zero. Then the strategy is to measure the distance between $f_n(\\theta)$ and $0$. 2. The definition of GMMDef: Suppose function $f$ satisfied all the properties in section 1. We have an observed sample ${x_i: i = 1, 2, 3, \\dots}$, and we want to estimate parameter vector $\\theta$ ($p \\times 1$) with true value $\\theta_0$. Let $E[f(x_i, \\theta_0)]$ denote a set of $q$ population moments and $f_n(\\theta)$ (since the observations are given, we treated $\\mathbb{x}$ as constant in this system) denote the associated sample counterparts. Define the criterion function $Q_n(\\theta)$ as: $Q_n(\\theta) = f_n(\\theta)^TW_nf_n(\\theta)$ where $W_n$ is the weighting matrix, which converges to a positive definite matrix $W$ as n grows large. Then the GMM estimator of $\\theta_0$ is given by $\\hat{\\theta} = \\argmin Q_n(\\theta)$ Note the $Q_n(\\theta)$ measure the distance between $\\theta$ and $\\theta_0$, since $f_n(\\theta_0)$ should converge to zero with probability. In conclusion, we try to measure the distance between $\\theta_0$ and $\\theta$ in a metric space. If $p = q$, then the GMM degenerates to MM. 3. The property of GMM3.1 AssumptionsGiven some assumptions, we can conclude that the GMM estimator is consistent and asymptotically normally distributed. Assumption 1: we have more moments equations than parameters; the rank of Jacobian matrix of the moment equations evaluated at $\\theta_0$ is at least p; $\\theta_0$ is the unique solution of the moments equations system. Assumption 2: The weak large number law holds, which means for any $\\epsilon > 0$, we have $\\lim \\limits_{n \\xrightarrow{} \\infty }\\mathbb{P}[|f_n(\\theta_0) - f(\\theta_0)| > \\epsilon] = 0$. Assumption 3: The sample moments should ensure central limit theorem holds, with a finite asymptotic covariance matrix $\\frac{1}{n}F$. 3.2 The distribution of GMM estimatorThe variance of GMM estimator is consistent and asymptotically normally distributed with asymptotic covariance matrix $V_{GMM}$ $V_{GMM} = \\frac{1}{n} [G(\\theta_0)^{T}WG(\\theta_0)]^{-1}G(\\theta_0)^{T}WFWG(\\theta_0) [G(\\theta_0)^{T}WG(\\theta_0)]^{-1}$ where $G(\\theta_0)$ is the Jacobian matrix of the population moment functions evaluated at the true parameter value $\\theta_0$. I give up to articulate the proof of this result due to the complexity (but I hope we can get some intuition from delta method) (See here for more details). Note that the variance of GMM estimator depends on the choice of $W_n$. If we review the criterion function of GMM, we can deduce that this distance function is the sample moment’s error sum of squares. Note that, given the knowledge of conditions of a matrix ($\\frac{\\lambda_{max}}{\\lambda_{min}}$) and Gaussian-Markov condition, it is make sense to normalize the errors in the moments by their variance. (Recall WLS and Hat Matrix ) 3.3 The optimal weighting matrixThe optimal choice of the weighting matrix $W_n = F^{-1}$, the GMM estimator is a asymptotically efficient with covariance matrix: $V_{GMM}^* = \\frac{1}{n} [G(\\theta_0)^TF^{-1}G(\\theta_0)]^{-1}$ However, if we want to know matrix $F$ (the asymptotic covariance matrix of sample moments), we have to estimate $\\theta$ first (Recall EM algorithm). Then we solve this circularity by adopting a multi-step method: Step 1: Choose a sub-optimal weighting matrix ($I$ for example), this will give us a consistent estimation of $\\theta$. Then we can estimate the matrix $F$. Step 2: Use the new $F$, estimate $\\theta_0$ Reference Zsochar, P. Short Introduction to the Generalized Method of Moments. HUNGARIAN STATISTICAL REVIEW. Hansen, L. (1982). Large Sample Properties of Generalized Method of Moments Estimators. Econometrica, 50(4), 1029. doi: 10.2307/1912775","link":"/2020/02/01/GMM-NW-Notes-1/"},{"title":"HDF5-Notes","text":"Keywords: HDF5 information set, HDF5 dataset, HDF5 group, HDF5 links, HDF5 datatypes, HDF5 attributes, hyperlabs, MPI-based application, independent mode & collaborative mode Preface: 学好抽象代数，走遍天下都不怕。 Note: For non-commercial use only. 1. Data modelHDF5 data model defines HDF5 data information set, i.e. a container for annotated associations of array variables, groups, and types. A HDF5 information set is represented by an HDF5 file with a designated root. The elements in a HDF5 dataset include HDF5 datasets, HDF5 groups, and HDF5 data objects. 1.1 HDF5 dataset-Layout strategies and their advantagesHDF5 datasets are array variables whose data elements are logically laid out or shaped as a multidimensional array. The two key properties of the HDF5 dataset includes the rank of the dataset (the number of dimensions), as well as the maximum extant in the respective dimensions (dataspace). With the help of dataspace, the HDF5 dataset can be viewed as as relational variables with the HDF5 dataspace as a system-defined candidate key space. Remark: the key here is how to understand the structure of multidimensions. Recall the existence of a mapping such that a dataset is linear separable. Depending on the layout strategy, the maximum extent may be unlimited. Currently, there are three different layout strategy in the HDF5. a) Contiguous, array elements are laid out as a collection of a linear sequence. The properties of this layout strategy is that we can easily conduct slicing operation on the dataset. However, there is a limit among the maximum extent. (Nearly constant time to get access to any element of the data.) b) Chunked, Array elements are laid out as a collection of fixed-size regular sub-arrays, or chunks. With the chunked layout strategy, we can extent the dataset to the unlimited size in some or all of the dimensions. c)_Compacted, the data set smaller than 64KB can be stored in the meta data or can be retrieved from the header (a data type in HDF5). There is no free lunch in the decision of the layout strategy. HDF5 also provides the users with filter pipelines for the customized and standard raw data. The filter pipelines includes compression and error checking. The chunked layout strategy allows the definition of new filter pipeline. Note that HDF5 do not support filter pipeline for contiguous layout strategy. Although we can compress the data chunk to gain additional performance improvement, we should be very careful to the determination of the size of the chunk, since it is a trade off between the IO and added pipelines and functionalities. 1.2 HDF5 GroupsAn HDF5 group is an explicit representation of an association between zero or more HDF5 information items (includes the HDF5 groups themselves). The association or relationship represented by an HDF5 group is made explicit through a collection of named links to the participating HDF5 information items. Remark: 在数学里，群是一类特定的代数结构。当然，代数结构本身其实是定义了至少一种运算的非空集合，归根到底还是具备某种性质的集合。运算的本质其实是一种映射，所以群实际上是具备了某些特定类型映射的非空集合。当然，数学上某些具备特殊性质的映射又会产生别的性质，归根到底是在研究映射的性质。这里link的概念实际是从HDF5 information item $\\xrightarrow{}$ HDF5 information item的映射。 Link traversal from the HDF5 root group generates the HDF5 information set graph, a rooted, directed, connected graph (是可以有环的). In one H5 file, there is only one root group. Link traversal from the HDF5 root group generates the HDF5 information set graph, a tooted, directed, multiple edges. Users and applications can refer to the specific information set via the HDF5 pathname. 1.3 HDF5 linksCurrently, HDF5 only support unidirectional, single source/destination links only. Further more, the source must be a HDF5 group. The different kinds of HDF5 links can be distinguished by the destination and whether they affect the commitment state of an HDF5 information item. An HDF5 dataset, group, or datatype object is committed to an HDF5 infoset by linking it to at least one HDF5 group using an HDF5 hard link. (the hard link is determined by the identifiers of the source and the link. Note that there is only one source and one destination in a linked pair). HDF5 soft and external links are so-called symbolic links because they refer to their destination via an HDF5 pathname or a file name/HDF5 pathname combination. The soft links and external links will not change the commitment of the destination. Note that the destination of the soft and external links may not even exist during the creation of the symbolic links. The destination may cease or even change during the life time of a symbolic links. 1.4 HDF5 DatatypesThe non-scalar HDF5 array variable type has two key ingredients: an HDF5 dataspace which describes its shape, and an HDF5 datatype which describe the type of its data element. Ten families or classes of HDF5 datatypes are currently supported: integer, floating-point, string, bitfield, opaque, compound, reference, enum, variable-length sequence and array. Note that the compound type is a collection of different datatypes(of course given specific length, 不然对齐毛线). The reference family includes HDF5 datatype references and HDF5 region references. While the HDF5 data groups can be treated as the explicit representation of a set of HDF5 information items, the reference can be treated as an implicit representation of such association. It is not an association in the HDF5 data model, but it does when it comes to users and applications. 1.5 HDF5 AttributesHDF5 is used to annotate HDF5 dataset, groups and datatype objects. The name of an HDF5 attribute must be unique in the scope of the HDF5 info item they attached to (kind of key-value pairs). 2. Software architecture HDF5 hyperslabs are the higher-dimensional analogue of a one-dimensional [start, stride, count, block] pattern. (高维slice) 3. Performance3.1 Parallel I/OThe HDF5 library can be used for accessing HDF5 files in parallel in MPI-based parallel applications. Parallel access can be independent or collective. The independent mode, the individual MPI process perform IO operation will not be influenced by other MPI process in the same application accessing the same HDF5 file (独立模式下，MPI进程不会被相同应用下的访问相同HDF5文件的进程影响). In collective mode, parallel access is a highly coordinated, cooperative effort involving all MPI processes in an application. 考虑到独立模式下需要考虑各种竞争条件和因此带来的进程交流和同步成本，无非必要，不要上独立模式。 3.2 IndexingAs the introduction in HDF5 dataset, the dataspace attribute can be treated as the system defined key for a relational variable. Simple queries targeting attributes in dataspace-ware attribute (integer lattice coordinates) can take advantage of the quasi-random access or HDF5 region references, and then use traditional indexing techniques to work on the complementary of dataspace-aware attributes. An optimizer (programmer or system component) decides if there is an index (one or more) such that an efficient data access path can be devised.","link":"/2021/04/26/HDF5/"},{"title":"Estimation in Time Series","text":"Keywords: Asymptotic distribution of estimators 1. The estimation of the mean value1.1 ConsistencyIf $\\{X_t\\}$ is stationary with mean and $\\mu$ and autocovariance function $\\gamma(\\cdot)$, then as $n \\xrightarrow{} \\infty$, $\\text{Var}(\\bar{X}_n) = \\mathbb{E}(\\bar{X}_n - \\mu)^2 \\xrightarrow{} 0$ if $\\gamma(\\cdot) \\xrightarrow{} 0$, and $n\\mathbb{E}(\\bar{X}_n - \\mu)^2 \\xrightarrow{} \\sum_{n = -\\infty}^{\\infty} \\gamma(h)$ if $\\sum_{n = -\\infty}^{\\infty} |\\gamma(h)| \\le \\infty$ Proof: $n\\text{Var}(\\bar{X}_t) = \\frac{1}{n} \\sum_{i,j = 1}^{\\infty} \\text{Cov}(X_i, X_j) = \\frac{1}{n} \\sum_{i,j = 1}^{\\infty} \\gamma(h) = \\sum_{|h| < n}(1 - \\frac{|h|}{n})\\gamma(h) \\le \\sum_{|h| < n}|\\gamma(h)|$ Note that $\\lim_{n \\xrightarrow{} \\infty}n^{-1}\\sum_{|h| < n}|\\gamma(h)| = 2 \\lim_{n \\xrightarrow{} \\infty} |\\gamma(n)| \\xrightarrow{} 0$, hence $\\text{Var}(\\bar{X}_n) \\xrightarrow{} 0$. If $\\sum_{|h| < n}|\\gamma(h)| < \\infty$ then the dominate convergence theorem indicates that $n\\mathbb{E}(\\bar{X}_n - \\mu)^2 \\xrightarrow{} \\sum_{n = -\\infty}^{\\infty} \\gamma(h)$ 1.2 CLT for strictly stationary processLemma： Let $X_n$, $n = 1, 2,\\dots$ and $Y_{nj}$, $j = 1, 2, \\dots; n= 1, 2, \\dots$ be random k-vectors such that (i) $Y_{nj} \\xrightarrow{d} Y_j$ as $n \\xrightarrow{} \\infty$ (ii) $Y_{j} \\xrightarrow{d} Y$ as $j \\xrightarrow{} \\infty$ (iii) $\\lim_{j \\xrightarrow{} \\infty}\\text{limsup}_{n\\xrightarrow{}\\infty}P(|X_n - Y_{nj}| > \\epsilon) = 0$ for every $\\epsilon > 0$ Then $X_n \\xrightarrow{d} Y$ Definition m-dependence: A strictly stationary sequence of random variables $\\{X_t\\}$ is said to be m-dependent if for each $t$, the two sets of random variables $\\{X_j, j \\le t\\}$ and $\\{X_j, j \\ge t + m + 1\\}$ are independent. CLT for strictly stationary m-dependent sequences: If $\\{X_t\\}$ is a strictly stationary m-dependent sequence of random variables with mean zero and autocovariance function $\\gamma(\\cdot)$, and if $v_m = \\gamma(0) + 2\\sum_{j=1}^m \\gamma(j) \\neq = 0$ then (i) $\\lim_{n \\xrightarrow{} \\infty} n \\text{Var}(\\bar{X}_n) = v_m$ (ii) $\\bar{X}_n$ is $AN(0, v_m/n)$ Proof： (i) 易证。 (ii) 先通过m-dependent的独立性构造一个辅助随机变量序列，在证明$\\{X_n\\}$依分布收敛到这个辅助随机变量序列，这个方法值得借鉴。 For each integer k such that $k > 2m$, let $Y_{nk} = n^{-1/2} [(X_1 + X_2 + \\dots + X_{k - m}) + (X_{k +1} + X_{k+2} + \\dots + X_{2k - m}) + \\dots + (X_{(r-1)k+1} + \\dots + X_{rk-m})]$, where $r = [n/k]$. Note that $n^{1/2}Y_{nk}$ is a sum of r iid random variables each having mean zero and variance, $R_{k-m} = \\text{Var}(X_1 + \\dots + X_{k-m}) = \\sum_{|j|< k-m}(k-m-|j|)\\gamma(j)$. Apply CLT, we have $Y_{nk} \\xrightarrow{} Y_{k}$, where $Y_k \\sim N(0, k^{-1}R_{k-m})$. Moreover, note that $Y_{k} \\xrightarrow{} Y$, where $Y \\sim N(0, v_m)$. Then we need to prove that $\\bar{X}_n$ converge to $Y_{nk}$ almost surely. $\\lim_{k \\xrightarrow{} \\infty} \\text{limsup}_{n \\xrightarrow{} \\infty} P(|n^{1/2} \\bar{X}_n - Y_{nk}| > \\epsilon) = 0$ for every $\\epsilon > 0$ (easy to prove). 1.3 Asymptotic normalityIf $\\{X_t\\}$ is the stationary process, $X_t = \\mu + \\sum_{j = -\\infty}^{\\infty} \\varphi_{j}Z_{t-j}$, where $\\{Z_t\\} \\sim IID(0, \\sigma^2)$, where $\\sum_{j = -\\infty}^{\\infty}|\\varphi_j| < \\infty$ and $\\sum_{-\\infty}^{\\infty} \\varphi_{j} \\neq 0$, then $\\bar{X}_n \\sim AN(\\mu, n^{-1}v)$, where $v = \\sum_{h=-\\infty}^{\\infty}\\gamma(h) = \\sigma^2(\\sum_{j = -\\infty}^{\\infty} \\varphi_j)^2$ (AN means asymptotic normal) Proof: Define $X_{tm} = \\mu + \\sum_{j = -m}^m \\varphi_{j}Z_{t-j}$ and $Y_{nm} = \\bar{X}_{nm} = \\frac{\\sum_{i=1}^n X_{tm}}{n}$, $\\text{Var}(n^{1/2}(\\bar{X}_n - Y_{nm})) = n\\text{Var}(n^{-1} \\sum_{t=1}^n \\sum_{|j|>m} \\phi_{j}Z_{t-j}) \\xrightarrow{}(\\sum_{|j|>m}\\varphi_j)^2\\sigma^2$ 2. The estimation of autocovariance function2.1 Autocovariance function$\\gamma(h) = n^{-1} \\sum_{t = 1}^{n - h} (X_t - \\bar{X}_n)(X_{t + h} - \\bar{X}_n), 0 \\le h \\le n - 1$. The reason why we do not use the unbiased estimator (you know what) is the we want to get an non-negative covariance matrix. For the estimated covariance matrix, we can write the $\\hat{\\Tau}_n$($\\{\\hat{\\gamma}(i-j)\\}$) as $\\frac{1}{n}TT^{\\prime}$. (Note that $TT^{\\prime} = \\{\\sum_{k=1}T_{ik} T_{kj}\\}$, recall the Einstein summation).$T = $ is a $(n \\times 2n)$ matrix. Recall that the sufficient and necessary condition of the non-negative matrix is that the there exist such a decomposition. Note that for any real vector $a$, we have $a^{\\prime}\\hat{\\Tau}_n a = n^{-1}(a^{\\prime}T)(a^{\\prime}T)^\\prime$ 2.2 Autocorrelation function and asymptotic distribution$\\hat{\\rho}(h) = \\hat{\\gamma}(h) / \\hat{\\gamma}(0)$ Theorem 2.2.1: If $\\{X_t\\}$ is the stationary process, $X_t - \\mu = \\sum_{j = -\\infty}^{\\infty} \\varphi_j Z_{t - j}$, $Z_t \\sim \\text{IID}(0, \\sigma^2)$, where $\\sum_{j=-\\infty}^{\\infty}|\\varphi_{j}|","link":"/2021/02/02/Estimation-in-Time-Series/"},{"title":"Hello World","text":"Some helpful links to help you blogging. How to use Hexo Hexo. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting. http://theme-next.iissnan.com/theme-settings.html (language: Chinese) https://www.jianshu.com/p/efbeddc5eb19 (language: Chinese) Math support in Hexo: Mathjax vs KatexAlthough Katexs support less mathematics symbols compared to Mathjax, Katex only takes hundreds microsecond to render math formula, which is hundred times faster than Mathjax. I appreciate Zijing Hu’s help when I struggled with the settings of Katex and hexo-renderer-kramed. You can visit Zijing’s github page for details. You can read the Katex documents to learn more about the syntax and details.","link":"/2020/01/02/Hello%20World/"},{"title":"Inner product space and Hilbert space","text":"Keywords: Inner product space and Hilbert space 细节梳理 1. Vector spaceA vector space (also called a linear space) is a set of objects called vectors, which may be added together and multiplied(“scaled”) by numbers, called scalars. Remark: 所谓空间，就是具备某些性质的元素的集合。大多数空间的定义是先射箭，再画靶的典型。 2. Inner product spaceA complex(real) vector space $\\mathcal{H}$ is said to be an inner-product space if for each pair of elements $x$ and $y$ in $\\mathcal{H}$, there is a complex number $(x, y)$, called the inner product of $x$ and $y$, such that: $(a) (x, y) = \\bar{(y, x)}$ $(b) (x+y, z) = (x, z) + (y, z)$ $(c) (\\alpha x, y) = \\alpha (x, y)$ $\\alpha \\in \\mathcal{C}$ $(d) (x, x) \\ge 0, \\forall x \\in \\mathcal{H}$ $(e) (x, x) = 0$ if and only if $x=0$ The norm of $x$ is defined as $|x| = \\sqrt{(x, x)}$ 2.1 Cauchy-Schwarz inequalityIf $\\mathcal{H}$ is an inner-product space, then $|(x, y)| \\le |x||y|$ and the equality holds iff $x=y(x, y)/(y, y)$ Proof: Let $a = |y|^2$, $b = |(x, y)|$ and $c=|x|^2$. The polar representation of $$ is then $(x, y) = b e^{i\\theta}$ for some $\\theta \\in (-\\pi, \\pi])$ Note that $\\forall r \\in \\mathbb{R}$, $(x - re^{i\\theta}y, x - re^{i\\theta}y) = (x, x) - re^{i\\theta}(y, x) - re^{-i\\theta}(x, y) + r^2(y, y)$ (Note that $(x, by) = \\bar{(by, x)} = \\bar{b} \\bar{(y, x)} = \\bar{b} (x, y)$) Note that $\\bar{z_1 z_2} = \\bar{z_1}\\bar{z_2}$, $(y, x) = e^{-i \\theta}$ $(x - re^{i\\theta}y, x - re^{-i\\theta}y) = c - 2rb + r^2a \\ge 0$ (by definition of inner product) The inequality holds for any $a, b, c, r$. Therefore $\\min(c - 2rb + r^2a) = c - \\frac{b^2}{a} \\ge 0$, i.e. $b^2 \\le ac$ The minimum value is achieved when $r = \\frac{b}{a}$, i.e. $x = y e^{i\\theta}b/a = y(x, y)/(y, y)$ 2.2 Convergence and continuityDefinition: Convergence in Norm: A sequence $\\{x_n, n = 1, 2, \\dots\\}$ of elements of an inner-product space $\\mathcal{H}$ is said to converge in norm to $x \\in \\mathcal{H}$ if $|x_n - x|\\xrightarrow{}0$ as $n \\xrightarrow{} 0$ Proposition: Continuity of the Inner Product: If ${x_n}$ and ${y_n}$ are sequence of elements of the inner-product space $\\mathcal{H}$ such that $|x_n - x|\\xrightarrow{}0$ and $|y_n - y|\\xrightarrow{}0$: (a) $|x_n| \\xrightarrow{} x$ (易证) (b) $(x_n, y_n) \\xrightarrow{} (x, y)$ 3. Hilbert spaceAn inner product space with completeness is called a Hilbert space. 3.1 Cauchy SequenceA sequence $\\{x_n, n=1, 2, \\dots\\}$ of elements of an inner-product space is said to be a Cauchy sequence if $|x_n - x_m| \\xrightarrow{} 0$ as $m, n \\xrightarrow{} \\infty$ i.e. if for every $\\epsilon > 0$ there exists a positive integer $N(\\epsilon)$ such that $|x_n - x_m| < \\epsilon$ for all $m, n > N(\\epsilon)$ 3.2 Definition of Hilbert spaceA Hilbert space $\\mathcal{H}$ is an inner-product space which is complete, i.e. an inner-product space in which every Cauchy sequence ${x_n}$ converges in norm to some element $x \\in \\mathcal{H}$. Remark: 收敛以及连续实际上定义了一种逼近过程，即不断靠近某个值，但是不一定能够等于这个值（我们把等于元素a定义为找不到更加接近a的元素，再收敛中的序列中，我们总能根据$\\epsilon-\\delta$找到更接近的，因此这是一种动态的感觉，某种程度上类似于$a.s.$和依概率收敛的感觉）。完全的意义在于，这个空间里面不存在一个“空”。我们可以轻松地用Euclidean space中挖去一些点来构造一个不完备的内积空间。Hilbert某种程度上避免了不知逼近到何处的尴尬。 Norm convergence and the Cauchy criterion: If $\\{x_n\\}$ is a sequence of elements belonging to a Hilbert space $\\mathcal{H}$, then $\\{x_n\\}$ converges in norm iff $|x_n - x_m| \\xrightarrow{} 0$ as $m, n \\xrightarrow{} 0$. Note that Cauchy series to norm convergence is the definition of Hilbert space. We need to prove norm converge has a corresponding Cauchy series. It is obvious if we consider the following fact: if $|x_n - x|\\xrightarrow{}0$ $|x_n - x_m| \\le |x_n - x| + |x_m - x|$ 4 A review of probability space-from the perspective of Inner-product spaceConsider a probability space $(\\Omega, \\mathcal{F}, P)$ and the collection $C$ of all random variables X defined on $\\Omega$ and satisfying the condition, $\\mathcal{E}(X^2) = \\int_{\\Omega} X(\\omega)^2P(d \\omega) < +\\infty$ Let’s define in the $L2$ space that $(X, Y) = E(XY)$ (注意这是随机变量，即从概率空间映射到向量空间的一个函数，的距离。) Note that if $E(X^2) = 0$, then $X(\\cdot) \\xrightarrow{a.s.} 0$, i.e. $\\textbf{Pr}(X = 0) = 1$ In the L2 space, norm convergence of a sequence $\\{x_n\\}$ can be written as:$|X_n - X|^2 = \\mathbb{E}|X_n - X|^2 \\xrightarrow{} 0$. It is also called mean-square convergence and is also written as $X_n \\xrightarrow{m.s.} X$","link":"/2021/01/22/Inner-product-space-and-Hilbert-space/"},{"title":"Kalman Filter-Notes-Part1-Discrete Kalman Filter","text":"R.E Kalman published a paper about a recursive solution to a discrete-data linear filtering problem. The Kalman filter provided us a computational method to estimate unobservable variables through minimization the posterior error. Moreover, Kalman filter enabled us to obtain the consistent parameter estimates by maximizing the likelihood function of error innovations. 1. The basis of discrete Kalman filterGenerally, Kalman filter solved the problem of estimating the unobservable state $x \\in \\mathcal{R}^n$ of a discrete time control process. The dynamic of the unobservable state’s ($n \\times 1 $ vector) can be written as: $x_k = Ax_{k - 1} + Bu_{k - 1}＋ｗ_{k - 1}$ The measurement variable $z_k$ follows a linear transformation of unobservable states with an linearly additive noise term $v_k$: $z_k = Hx_k＋v_k$ Note that we assume the probability measure of $w$ and $v$ follow Gaussian distribution with zero mean and covariance matrix Q , and R, which are both independent of each other and white. I will use the same symbols as Greg Welch and Gary Bishop’s: An introduction to Kalman filter to show you the computational origin of Kalman filter. Notations: $\\hat{x_k^{-}} \\in \\mathcal{R}^n$ is the estimation of unobservable states $x$ at time $k$ under the filtration $\\mathcal{F}_{t-1}$. $\\hat{x_k}$ is the posteriori state estimate under the information at time $k$ (with the help of $z_k$). Priori estimate error $e^{-}_{k}$: $x_k - \\hat{x_k^{-}}$. Posteriori estimate error $e_{k}$: $x_k - \\hat{x_k}$. Priori estimate error covariance matrix $P^{-}_{k} = \\mathbb{E}[e^{-}_ke^{-T}_k]$ Posteriori estimate error covariance matrix $P_{k} = \\mathbb{E}[e_ke^{T}_k]$ We assume the relationship between the posteriori estimate and the priori estimate follows: $\\hat{x_k} = \\hat{x^{-}_k} + K_k(z_k - H\\hat{x^{-}_k})$ Remark: If you know something about Gaussian distribution, you may find this is the BUE given observed states if we assume the noises is Gaussian distributed (it is a result of joint Gaussian distribution). I firmly believe this assumption is inspired from a probability origin. $z_k - H\\hat{x^{-}_k}$ are related to many frightening names like measure residual or innovation. As we need to minimize the posteriori error of each entry of the state vector, we should minimize the trace of $P_k$. Note that in the classical Kalman filter, the transition matrix of the system is deterministic. We only need to consider some incontrollable factors like measurement error and transition error. $\\hat{x_k} = \\hat{x^{-}_k} + K(z_k - Hx^{-}_k) \\leftrightarrow \\hat{x_k} = \\hat{x^{-}_k} + K(Hx_k + v_t - Hx^{-}_k)$ $\\leftrightarrow x_k - \\hat{x_k} = x_k - \\hat{x^{-}_k} - K(Hx_k + v_t - Hx^{-}_k)$ $\\leftrightarrow P_k = (I - K_kH_k)P_k^-(I - K_kH_k)^T+K_kR_kK_k^T$ (*) Let’s review some tricks in matrix derivation. You can see more details about layout with wiki. $\\frac{d(tr(AB))}{dA} = B^T$ $\\frac{d(tr(ACA^T))}{dA} = 2AC$ (It is trivial if you know $tr(AB) = \\sum_i\\sum_j a_{ij}b_{ji}$) We can get the optimal $K_k$ (the Kalman Gains): $K_k = P^{-}_kH^T(HP^{-}_kH^T+R)^{-1}$ With the Kalman gain and (*), we can get the posteriori estimates of state’s posteriori covariance matrix $P_k$. $P_k = (I - K_kH_k)P_k^-$ Note that if $\\lim \\limits{R \\xrightarrow{} 0}$ (there is no error in measurement), then the Kalman gain get the maximum value $H^{-1}$. If $\\lim \\limits{P_k^- \\xrightarrow{} 0}$, then the Kalman gain get the minimum value 0. You can see the adaptiveness of the methods here. If we are more confidence with the transition equation of unobservable state, we should allocate relatively more weight on the priori estimates; If we are more confident with the measurement of the observable equation, then we should make the posteriori estimate more close to the observable states. With the best estimated posteriori estimates of unobservable state, we can update the new priori estimate of unobservable variables. $P_k^- = AP_kA^T + Q$ $\\hat{x_k^-} = A\\hat{x_{k-1}} + Bu_{k-1}$ Remark: In the system of Kalman filter, we know exactly the transition of unobservable states. In this situation, the Kalman filter found a solution to estimate the unobservable states $x_k$ based on information at time k. If we know nothing about the transition of unobservable states (unknown A for example), we can maximize the likelihood function based on the measurement function. 2. Discussions about the initial valuesUnfortunately, we need to input four tricky initial values to the Kalman filter system: $x_0^-$, $P_0^-$, R, and Q. The more unfortunate fact is that KF is sensitive to the initial values. Based on this fact, we have to adjust hyperparameters for our model. The best statement of adjustment of initial values I have ever met is that the innovation ($z_k - H\\hat{x_k^-}$) should be a white noise with zero mean if KF works well (See Optimal State Estimation Kalman-H-and Nonlinear Approaches). We have to test different hyperparameters to help the model make sense. Thank you for reading my blog! KF is a big topic and I will dig deeper in the next parts of this Note.","link":"/2020/01/17/Kalman%20Filter-Notes-Part1/"},{"title":"Levy Stable Distributions","text":"Key words: convergence in distribution, Cauchy-Lorentz law, Domain of attraction, stable distribution, symmetric alpha-stable distribution ($\\mathcal{S}\\alpha \\mathcal{S}$), Levy distribution 1. The exclusive properties of stable distributions1.1 The definition of stable distributions-Invariance under additionA random variables X is said to have a stable distribution $P(x) = Prob\\{X \\le x\\}$ if for any $n \\ge 2$, there is a positive number $c_n$ and a real number $d_n$ such that $X_1 + X_2 + \\dots + X_n \\overset{d}{=} c_n X + d_n$ Note that the sum of i.i.d random variables becomes a random variable with a distribution of different form. However, for independent random variables with a common stable distribution, the sum obeys to a distribution of the same type, which differs from the original one only for a scaling and possibly for a shift. When $d_n = 0$, the distribution is called strictly stable. It is known that the norming constants $c_n$ are of the form $c_n = n^{\\frac{1}{\\alpha}}$ with $00$. With the same logic we can define the slowly varying at infinity. Note that 1.1 can derive 1.2, while 1.2 can derive 1.1 as well. 1.3 Canonical forms for the characteristic functionAnother definition specifies the canonical form that the characteristic function of a stable distribution of index $\\alpha$ must have. This condition implies the properties of cf, which will be useful in the CLT. The canonical forms reads $\\hat{p}_{\\alpha}(\\kappa; \\theta) \\overset{\\Delta}{=} exp\\{-|\\kappa|^\\alpha e^{i(\\text{sign} \\kappa)\\theta \\pi/2}\\}$ $\\theta$ is called asymmetric parameter. The domain is restricted to the following region (depending on $\\alpha$). For $ 0","link":"/2021/01/03/Levy-Stable-Distributions/"},{"title":"Mathematical Programming-Notes-Part 1-Strong Duality in Linear Programming","text":"Programming is the basis for a wide range of fields. This blog summarized the sufficient conditions for strong duality. Moreover, it is a summery of Mathematical Programming lecture notes (David P. Williamson). 1. Separating Hyperplane Theorem1.1 Weierstrass TheoremLet $C \\subset \\mathbb{R}^n$ be a closed, non-empty and bounded set. Let $f$: $C \\rightarrow \\mathbb{R}$ be continuous on $C$. Then $f$ attains a maximum (and a minimum) on some point of C. 1.2 Separating Hyperplane TheoremLet $C \\subset \\mathbb{R}^n$ be closed, non-empty and convex set. Let $y \\notin C$, then there exists a hyperplane $a \\ne 0$, $a \\in \\mathbb{R}^n, b \\in \\mathbb{R}$, such that $a^Ty > b$ and $a^Tx","link":"/2020/03/26/Mathematical%20Programming-Notes-1/"},{"title":"Kalman Filter-Projects-Part1-N factors Gaussian model","text":"Before we dig deeper in the Kalman filter, I would like to share one of the applications of Kalman filter with you. N-factors Gaussian model is a direct application of what we have discussed in Kalman filter Part1. This blog includes a brief introduction about this method, and some tricks in this algorithm‘s implement. 1. The Financial basis of N-factor Gaussian modelIf you have heard Barra model for stocks or other factors analysis models, you may understand this n-factors Gaussian model easily. Factor analysis is a cross-sectional methodology, which totally depends on the information at current time slot. To be more specific, it can detect the sources of correlations if we assume the correlations between different observable states are caused by communalities. Of course, we can use factors analysis framework to predict with some extra assumptions (for example, the factors’ returns keep constant between time slots). At time $t$, we can know the return of $n$ assets. We assume that the correlations between assets are caused by $m$ factors (risk factors). Matrix $X_{(m \\times n)}$ denotes the exposures of assets on risk factors, while vector $f$ denotes the ‘return’ of risk factors. If we write this model in a linear regression scheme, then $r_t = X_t^Tf + \\epsilon$ Intuitively, we decompose the observed returns according to the risk factors, and the coefficients of factors describe the average returns of each factors. In this framework, we treat the returns of factors as constants. What if we want to treat the coefficients like a distribution or something which contains the uncertainties? Let’s review the framework of risk neutral theorem in financial engineering. We assume the market (efficient and complete) has $n$ risk factors which determine the return of observable assets. Let $\\Theta(t)$ be a n-dimensional adapted process according to $W(t)$, which is a m-dimensional Brownian motions. In this framework, the returns of the risk factors are described as a m-dimensional Brownian motion. $d\\Theta(t) = \\mu(\\Theta)dt + \\sigma(\\Theta)dW(t)$ $sigma(\\Theta)$ works the same as the risk factors exposures in this framework. From this perspective, multi-factors model in stocks is a just special case. 2. The settings of N-factors Gaussian modelWe assume the spot prices of assets ($\\mathbf{S}_t$ is a $N \\times 1$ vector)in a market can be described with $n$ risk factors as: $log\\mathbf{S_t} = \\mathbf{L}_t’ \\mathbf{x_t} + \\mu t $ $d\\mathbf{x}_t = -\\mathbf{K}\\mathbf{x}_tdt + \\mathbf{\\Sigma} d\\mathbf{W}_t$ where $\\mathbf{K} = \\begin{Bmatrix} 0 & 0 & … & 0 \\\\ 0 & k_2 & … & 0 \\\\ .. & .. & .. & .. \\\\ 0 & 0 &…&k_n \\end{Bmatrix}$, and $\\mathbf{\\Sigma} = \\begin{Bmatrix} \\sigma_1 & 0 & … & 0 \\\\ 0 & \\sigma_2 & … & 0 \\\\ .. & .. & .. & .. \\\\ 0 & 0 &…&\\sigma_n \\end{Bmatrix}$ $\\mathbf{L}_t$ is a $n \\times N$ matrix. This settings of multi-factors analysis include the independence of communalities. However, of course, we can handle the correlations between the risk factors. For example, we assume the risk factors are linear combinations of m independent factors (m < n), then the observed returns of the markets is a affined transformation of m-factor Gaussian model we showed above (See Affined N-factors Gaussian Model). 3. Parameters estimationParameters estimation is a direct application of Kalman filter (See here to know more about Kalman filter). As we have discussed in the Part I of KF Notes, we can maximize the likelihood function based on innovation ($z_t - \\hat{z_t}$) to estimate the transition matrix of unobservable states. $\\mathbf{x}_t = A \\mathbf{x}(t-1)+ \\mathbf{c}_t + \\mathbf{\\epsilon}_t$, where $\\mathbf{\\epsilon}_t \\backsim N(0, Q_t)$ $\\mathbf{z}_t = H \\mathbf{x}_t + \\mathbf{d}_t + \\mathbf{v}_t$, where $\\mathbf{v}_t \\backsim N(0, R_t)$ $\\hat{x_t^-} = A\\hat{x_{t-1}} + \\mathbf{c}_t$ $\\hat{P_t^-} = A \\hat{P_{t-1}}A^T + Q_t$ $\\hat{z_t^-} = H \\hat{\\mathbf{x}_t^-} + \\mathbf{d}_t$ Let $\\hat{F_t^-}$ be the priori covariance matrix of observable states. $\\hat{F_t^-} = H\\hat{P_t^-}H^T + R_t$ We need to parameterize the covariance matrix $R_t$. Some papers assume that the error of measurement equation is a homoscedastic diagonal matrix. However, we can try some more complex assumption, which is another typical bias and variance trade off. We need to maximize the log-likelihood function of $z_t$: $\\mathcal{L}(\\Theta) = -\\frac{1}{2} \\sum_t log|F_t^-| - \\frac{1}{2} \\sum_t [z_t - \\hat{z_t^-(\\Theta)}]^T (F_t^-)^{-1}[z_t - \\hat{z_t^-(\\Theta)}]$ If $H_t$ also needs to be parameterized, we can use EM algorithm to estimate the parameters. Remark: We used the priori estimation of the covariance matrix of $\\hat{z_t^-}$ in the likelihood function. Why not the posterior covariance matrix? The answer is quite trivial if you notice that the posterior estimation includes a weighted average of priori estimation and innovation (measure residual), which means we have to know the innovation before the posteriori estimation. In the likelihood function methodology, we use Fisher information matrix to estimate the variance of parameters.","link":"/2020/01/19/Kalman%20Filter-Projects-Part1/"},{"title":"McKean SDE and Particle method","text":"Keywords: McKean Stochastic Differential Equations, simulation 1. DefinitionsA McKean equation for an n-dimensional process X is an SDE in which the drift and volatility depend not only on the current value $X_t$ of the process, but also on the probability distribution $\\mathbb{P}_t$ of $X_t$, i.e. $dX_t = b(t, X_t, \\mathbb{P}_t) dt + \\sigma(t, X_t, \\mathbb{P}_t) dW_t$, $X_0 \\in \\mathbb{R}^n$ where $W_t$ is a d-dimensional Brownian motion. One of the most celebrated form of McKean SDE is the McKean-Vlasov SDE, where for $1 \\le i \\le n$ and $1 \\le j \\le d$, $b^{i}(t, x, \\mathbb{P}_t) = \\int_{\\Omega} b^i(t, x, \\omega) d\\mathbb{P}_t(\\omega)$ (scalar) $\\sigma^{i}_j(t, x, \\mathbb{P}_t) = \\int_{\\Omega} \\sigma^i_j(t, x, \\omega) d\\mathbb{P}_t(\\omega)$ (scalar) 1.1 The uniqueness and existence of the SDEIf the drift and volatility coefficients are Lipschitz-continuous functions of x and $\\mathbb{P}_t$ wrt Wasserstein distance, the solution of the SDE has a strong unique solution. Theorem Let $b : \\mathbb{R}_{+} \\times \\mathbb{R}^n \\times \\mathcal{P}_2(\\mathbb{R}^n) \\xrightarrow{} \\mathbb{R}^n$ and $\\sigma: \\mathbb{R}_{+} \\times \\mathbb{R}^n \\times \\mathcal{P}_2(\\mathbb{R}^n) \\xrightarrow{} \\mathbb{R}^{n \\times d}$ be Lipschitz-continuous functions and satisfy a linear growth condition $|b(t, X, \\mathbb{P}) - b(t, Y, \\mathbb{Q})| + |\\sigma(t, X, \\mathbb{P}) - \\sigma(t, Y, \\mathbb{Q})| \\le C(|X - Y| + d(\\mathbb{P}, \\mathbb{Q}))$ and $|b(t, X, \\mathbb{P})| + |\\sigma(t, X, \\mathbb{P})| \\le C(1 + |X|)$ (C is a positive constant) $\\mathcal{P}_2(\\mathbb{R}^n)$ denotes the probability measures with finite second order moment (notice the difference between the $L^2$-space). The Wasserstein distance is defined as: $d(\\mu, v) = \\inf_{\\tau \\in \\mathcal{P}(\\mathbb{R}^n \\times \\mathbb{R}^n)} \\Big(\\int_{\\mathbb{R}^n \\times \\mathbb{R}^n} |x - y| \\tau(dx, dy)\\Big)^{\\frac{1}{2}}$ Note that the marginal distribution of $\\mathcal{P}(\\mathbb{R}^n \\times \\mathbb{R}^n)$ should be $\\mu$ and $v$. The optimizer here is the possible coupling method between two random variables. Then the nonlinear SDE $dX_t = b(t, X_t, \\mathbb{P}_t) dt + \\sigma(t, X_t, \\mathbb{P}_t)dW_t$, where $\\mathbb{P}_{t}$ denotes the probability distribution of $X_t$, admits a unique solution such that $\\mathbb{E}(\\sup_{0 \\le t \\le T}|X_t|^2) < \\infty$ Proof CRC-Nonlinear Option Pricing 251 The probability density function $p(t, y) dy = \\mathbb{P}_t(dy)$ of $X_t$ is a solution to the Fokker-Planck PDE: $-\\partial_t p(t, x) - \\sum_{i = 1}^n \\partial_i (b^{i}(t, x, \\mathbb{P}_t)p(t, x)) + \\frac{1}{2} \\sum_{i, j = 1}^n \\partial_{i, j} \\Big(\\sum_{k = 1}^d \\sigma^{i}_{k}(t, x, \\mathbb{P}_t) \\sigma_k^j(t, x, \\mathbb{P}_t)p(t, x)\\Big) = 0$ with initial condition $\\lim\\limits_{t \\xrightarrow{} 0} p(t, x) = \\delta(x - X_0)$ (Dirac function) (Note that it is a non-linear PDE since the drift and diffusion depends on the $p(x, t)$) 2. Particle method (McKean-Vlasov)We can approximate the $\\mathbb{P}_t^N$ with the empirical distribution $\\frac{1}{N} \\sum_{i = 1}^N \\delta_{X_t^{i, N}}$, where the $(X^{i, N})_{1 \\le i \\le N}$ are solutions to the $(\\mathbb{R}^n)^N$-dimensional classical (linear) SDE, i.e. $dX_t^{i, N} = \\Big(\\int b(t, X_t^{i, N}, y) d\\mathbb{P}_t^N(y)\\Big)dt + \\Big(\\int \\sigma(t, X_t^{i, N}, y) d\\mathbb{P}_t^N(y)\\Big)dW^i_t$, which is equivalent to $dX_t^{i, N} = \\sum_{j = 1}^N b(t, X_t^{i, N}, X_t^{j, N}) dt + \\sum_{j = 1}^N \\sigma^i(t, X_t^{i, N}, X_t^{j, N}) dW^i_t$ (*) (Law $\\Big(X_0^{i, N}\\Big)$) The particle method differs from classical Monte Carlo methods as it involves a system of N interacting particles. 2.1 Propagation of chaos and convergence of the particle methodLet’s consider scalar case first. Let $\\mu_t^{N}$ the density of $(X_t^{1, N}, \\dots, X_t^{N, N})$. We have the marginal laws that: $\\mu_t^{k}(x_1, \\dots, x_k) = \\int \\mu_t^{(N)}(x_1, \\dots, x_N)d x_{k + 1} \\dots d x_{N}$ The Fokker-Planck PDE of (*) can be written as: $\\partial_t \\mu_t^{N}(x_1, \\dots, x_N) = -\\frac{1}{N} \\sum_{i, j = 1}^N \\partial_{x_i}{b(x_i, x_j) \\mu_t^{(N)}} + \\frac{1}{2N^2}\\sum_{i, p, q = 1}^N \\partial^2_{x_i} {\\sigma(x_i, x_p)\\sigma(x_i, x_q)\\mu_t^{(N)}}$ ($L^2$ adjoint see )","link":"/2021/02/13/Mckean-SDE/"},{"title":"Numpy-Notes-Part1-How to Generate Multivariates Gaussian Distribution in A Decent Way?","text":"Numpy is the most powerful scientific computation tools in Python. I decide to start a new set of blogs to elaborate the amazing tricks in its source codes. This blog introduced how Numpy generates multivariates Guassian distribution. You can read the source code via Link. 1. Definitions and Concepts1.1 Positve-semidefiniteDefinition 1: Hermitian MatrixWe call a square matrix $A$ Hermitian if it is self-adjoint. i.e. $A = \\bar{A^T}$ Definition 2: Positive-SemidefiniteA positive semidefinite matrix is a Hermitian matrix if all of whose eigenvalues are nonnegative. 1.2 SVD decompositionSVD decomposition generalizes the eigendecomposition of a square matrix to any $m\\times n$ matrix. Matrix $M$ can be written as $U\\Sigma V^{*}$, where $U$ is a $m \\times m$ unitary matrix, $V$ is $n \\times n$ unitary matrix, and $\\Sigma$ is a rectangular diagonal matrix with non-negative real numbers. Note that if $M$ is real, then matrix $U$ and $V$ must be orthonormal matrices. 2. How to generate r.v. following multivariate Gaussian distribution?The most common idea is to use the Cholesky decomposition. However, the Cholesky–Banachiewicz and Cholesky–Crout algorithms require a positive definite matrix (Click here for details), which implies that the matrix must have full rank. In most situiation, we have to handle positive semidefinite matrix. For example, generate a covariance matrix with kernel trick. In this situation, Numpy provides us with a brilliant idea via SVD decomposition. Note that if we can find a Matrix A, such that $A^TA = M$ Then we can conclude that $M$ is positive-semidefinite. Moreover, recall that for real matrix M: $M = U\\Sigma V^T = U \\Sigma^{\\frac{1}{2}} \\Sigma^{\\frac{1}{2}} V^T = U \\Sigma^{\\frac{1}{2}} (V\\Sigma^{\\frac{1}{2}})^T$ If $ U \\Sigma^{\\frac{1}{2}} = V\\Sigma^{\\frac{1}{2}}$, then we find an available A for matrix M. $ U \\Sigma^{\\frac{1}{2}} = V\\Sigma^{\\frac{1}{2}} \\leftrightarrow{} U\\Sigma U^T = V\\Sigma V^T=M$ $(*)$ If $(*)$ holds, then we can make sure $M$ is a positive-semidefinite matrix. Therefore, we can easily generate random vector following multivariates Gaussian distribution via: $rv = AR + \\mu$ where $R$ is random vector following standard Gaussian distribution with $\\Sigma = I$. (Just use $n$ independent r.v. following standard Gaussian distribution)","link":"/2020/03/30/Numpy-Notes-1/"},{"title":"MapReduce","text":"keywords: Functional programming, programming model Map: specify a map function to generate intermediate key-value pairs based on primal key-value pairs. Reduce function: merge all intermediate values associated with the same intermediate key. Functional style programming allows users to utilize the resources of a large distributed system in natural. (the key-value data structure allow parallelism in the perspective of data splitting.) 1. Execution overviewThe map invocation are distributed across multiple machines by partitioning the input data into a set of M splits. The input splits can be processed in parallel by different machines. Reduce invocations are distributed by partitioning the intermediate key space into R pieces using a partitioning function. (Recall 一致hash) Step1: The user defined MapReduce library first split the input file into M pieces. The size of each part of the map section depends on the hardware. The whole application will then starts up many copies of the program on a cluster of machines. Step2: One of the copies is called master, which assign work to “workers”. Recall that we have M map tasks and R reduce tasks. Each worker has three status, idle, in-progress, or completed. The master picks the idle workers and assign each one a map task or a reduce task. Step3: A worker who is assigned a map task reads the contents of the corresponding input split. It parses key/value pairs out of th input data and passes each pair to the user-defined map function. The intermediate key/value pairs produced by the Map function are buffered in memory. Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. (Recall the differences between spark and Hadoop) The location of these R partitions on local disk are passed back to the master, who is responsible for forwarding these locations to the reduce workers. Step4: When the reduce worker is notified by the master about these locations, it uses remote procedure calls to read the buffered data from the local disks of the map workers. (感觉调度问题会是个麻烦) When a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences are grouped together. (可能会用到虚拟内存) Step5: The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the user’s Reduce function. (这里隐含地说明每个reduce worker的工作是处理给定的intermediate key) Step6: When all map tasks and reduce tasks have been completed, the master wakes up the user program. Remark: The data structure of the master is quite special. It restores the state of the task (idle, in-progress, complete) as well as the identity of the worker machine. The master should also restore the location of R partitions (习题：为什么不用保存M个data split的地址？自证不难). The information of the location is pushed to workers that have in-progress reduce tasks. (Keep an eye on the granularity!) 2. Fault toleranceThe library must tolerate machine failures gracefully. 2.1 The identification of worker failureThe master pings every worker periodically. If no response is received from a worker in a certain amount of time, the master marks the worker as failed. Any map tasks completed by the worker are reset back to their initial idle state, and therefore become eligible for scheduling on other workers. Similarly, any map task or reduce task in progress on a failed worker is also reset to idle and become eligible for rescheduling. Note that completed map tasks are re-executed on a failure because their output is stored on the local disks of the failed machine and is therefore inaccessible. Completed reduce tasks do not need to be re-executed since their output is stored in a global file system.(再次说明map步骤和reduce步骤的关系) 2.2 The failure of masterWrite periodic checkpoints!","link":"/2021/05/12/MapReduce/"},{"title":"Metric Embedding-Notes-Part 1-An overview and Basic Theorems","text":"Metric Embedding plays an important role in unsupervised machine learning and algorithm analysis. Embedding methods are also considered as one of the most important methods in the design of approximation algorithms. 1. MotivationsWe sometimes have to deal with data without a vector space representation. Metric embeddings help us measure the relationship of samples in a more convenient way. Moreover, one the most important applications of metric embedding is dimensionality reduction, which greatly increase the computational efficiency. 2. Metric spaces and embedding2.1 The concept of Metric spaceMetric space is a pair $(X, \\rho)$, where $X$ is a set of points (can be uncountable) and a function $\\rho$: $X \\times X \\rightarrow \\mathbb{R}^{\\ge 0}$ such that: (a) $\\rho(x, y) = 0 \\leftrightarrow x=y$ (b) $\\rho(x,y) = \\rho(y,x)$ (symmetry) (c) $\\rho(x,y) + \\rho(y,z) \\ge \\rho(x,z)$ (triangle inequality) Note the difference between metric space and normed space. Every normed space is a metric space, but a metric space may not be a normed space. Moreover, (b) and (c) implicate that for any two points $x, y \\in X$, we can get $\\rho(x,y) \\ge 0$. This can be quickly proved due to the fact that: $\\rho(x,y) + \\rho(y,x) \\ge \\rho(x,x) = 0$ $\\rho(x,y) + \\rho(x,y)\\ge 0$ $\\rho(x,y)\\ge 0$ $\\therefore Q.E.D$ If a mapping $\\rho$ is call distance if it satisfies (a) and (b). If a distance function satisfies (c), then this function is called metric. Moreover, if there exists different points, where $\\rho(x, y) = 0$, then we call this function semi-metric or pseudometric. It is natural for us to think whether we can find a function mapping from metric space to Euclidean space (a normed space, which the definition of norm is exactly inner product) and the keep the distance define in the original space. Unfortunately, it is not always possible to achieve that transformation. 2.2 EmbeddingWe want to find a simple metrics with some nice property. For example, we may reduce the dimension though metric embedding techniques, or a metric generated by simpler structures such as a tree, or a circle. The task is to find a function transferring the elements from the original metric space to the preferred one. Definition:A finite metric space $(X, \\rho)$ is realized in $\\mathcal{L}^k$ if there is a function $f$: $X \\rightarrow \\mathbb{R}^K$ so that $\\rho(x,y)=||f(x) - f(y)||_p$. A metric space is also called an $\\mathcal{L}_p-metric$ if it can be realized in $\\mathcal{L}_p^k$ for some k. We sometimes cannot find a nice embedding. Therefore, we need to measure the similarity of metric spaces in order to find a good enough one. Let $f$ be an embedding from the finite metric space $(X, \\rho)$ into another finite metric $(Y, \\mu)$. We define: $expansion(f)$ = $max \\frac{ \\mu (f(x), f(y)) }{ \\rho (x,y)}$ $contraction(f)$ = $max \\frac{\\rho (x,y)}{\\mu (f(x), f(y))}$ The distortion of an embedding is defined as the product of $expansion(f)$ and $contraction(f)$. An embedding $f$ with $distortion(f) = 1$ is called $isometric$. An alternative brief definition is: Definition:Given two metric space $(X, \\rho)$, $(Y, \\sigma)$. A mapping f: $X \\rightarrow Y$ is called a D-embedding of $X$ into $Y$ (where $D > 1$) if there exists some $r > 0$ such that $\\forall x, x’ \\in X$, $r \\cdot \\rho (x, x’) \\le \\sigma (f(x), f(x’)) \\le D \\cdot r \\cdot \\rho(x, x’)$ $D$ is the aforementioned distortion of an embedding. We can easily prove that (at least in finite metric space) every metric space embeds isometrically into $\\mathcal{L}_{\\infty}$. (Frechet) 2. Basic theorems2.1 Incompressibility of general metric spacesIf $\\mathbf{Z}$ is a normed space that D-embeds all n-points metric space, then, $dim(\\mathbf{Z}) = \\Omega(n)$ for $D","link":"/2020/04/03/Metric-Embedding-Notes-1/"},{"title":"Orthogonal Increment Process","text":"Keywords: stochastic integral, spectral representation 定义在正交增量随机过程上的随机积分算子是一个测度为F的Hilbert空间的同构(或者说具有Borel集的拓扑空间)， 且这个测度F和正交增量随机过程是一一对应的。我们可以证明这个同构well-defined。这里都是基于某个特定的随机变量的内积定义来的。如果我们换了随机变量的内积，这里都要变化（例如，我们要研究一个更大的Hilbert空间，例如$L^1$, 即强大数率的tech条件）。 1. DefinitionsAgain, let’s consider a $L^2[-\\pi, \\pi]$ Hilbert space, whose inner product of random variable $X$ and $Y$ are defined as $\\mathbb{E}(X\\bar{Y})$, and $\\int_{\\Omega} X(\\omega) \\bar{Y}(\\omega)P(d\\omega)$ 1.1 Orthogonal-increment processAn orthogonal-increment process on $[-\\pi, \\pi]$ is a complex-valued stochastic process $\\{Z(\\lambda), -\\pi \\le \\lambda \\le \\pi\\}$ such that $(Z(\\lambda), Z(\\lambda)) < \\infty$, $(Z(\\lambda), 1) = 0$, $(Z(\\lambda_4) - Z(\\lambda_3), Z(\\lambda_2) - Z(\\lambda_1)) = 0$ if $(\\lambda_1, \\lambda_2] \\cap (\\lambda_3, \\lambda_4] = \\phi$ 1.2 Right-continuous processThe process $\\{Z(\\lambda, -\\pi \\le \\lambda \\le \\pi)\\}$ will be called right-continuous if for all $\\lambda \\in [-\\pi, \\pi)$ $|Z(\\lambda + \\delta) - Z(\\lambda)|^2 = \\mathbb{E}(|Z(\\lambda + \\delta) - Z(\\lambda)|^2) \\xrightarrow{} 0$ as $\\lambda \\xrightarrow{} 0 $ Proposition 1.1: If $\\{Z(\\lambda), -\\pi < \\lambda < \\pi\\}$ is an orthogonal-increment process, then there is a unique distribution function $F$ (i.e. a unique non-decreasing, right-continuous function) such that $F(\\lambda) = 0, \\lambda \\le -\\pi$ $F(\\lambda) = F(\\pi), \\lambda \\ge \\pi$ $F(\\mu) - F(\\lambda) = |Z(\\mu) - Z(\\lambda)|^2$ Proof :(构造法证明) $F(\\mu) = |Z(\\mu) - Z(-\\pi)|^2$, $-\\pi \\le \\mu \\le \\pi$ According to the orthogonality of $Z(\\mu) - Z(\\lambda)$ and $Z(\\lambda) - Z(-\\pi)$, $-\\pi \\le \\lambda \\le \\mu \\le \\pi$ $F(\\mu) = |Z(\\mu) - Z(\\lambda) + Z(\\lambda) - Z(-\\pi)|^2 = |Z(\\mu) - Z(\\lambda)|^2 + |Z(\\lambda) - Z(-\\pi)||^2 \\ge |Z(\\lambda) - Z(-\\pi)|^2 = F(\\lambda)$ $F(\\mu + \\delta) - F(\\mu) = |Z(\\mu + \\delta) - Z(\\mu)|^2 \\xrightarrow{} 0$ as $\\delta \\xrightarrow{} 0$ Remark： In many field the properties of proposition 1.1 will be written as $\\mathbb{E}(dZ(\\lambda)d\\bar{Z}(\\mu)) = \\delta_{\\lambda, \\mu} dF(\\lambda)$ 2. Stochastic Integration wrt an Orthogonal Increment ProcessSuppose $\\{Z(\\lambda), -\\pi \\le \\lambda \\le \\pi\\}$ is an orthogonal-increment process defined on the probability space $(\\Omega, \\{\\mathcal{F}\\}_{\\lambda}, P)$. Note that the inner-product defined on this Hilbert space is: $(Z_{\\lambda}, Z_{\\lambda}) = \\int_{\\Omega}Z_\\lambda(\\omega) P(d\\omega)$ Let $f$ be a Borel measurable function on complex-valued space Hilbert space $([-\\pi, \\pi], \\mathcal{B}, F)$. Let $I(f)$ denote $\\int_{(-\\pi, \\pi])}f(\\lambda) dZ(\\lambda)$ , which is a random variable of the $L^2(\\Omega, \\mathcal{F}, P)$ (of course there exist some technical condition like $\\int_{(-\\pi, \\pi])} f(\\lambda)^2 dF(\\lambda)$ to make sure this random variable is square integrable. We can easily verify this condition based on the isomorphism later). We want to solve the unknown measure $F(\\lambda)$, such that $(I(f), I(g)) = \\int_{(-\\pi, \\pi])} f(\\lambda)\\bar{g}(\\lambda) dF(\\lambda)$ Note that any measurable function can be written as the summation of simple function. Therefore, let $f(\\lambda) = \\sum_{i = 0}^n f_i I_{(\\lambda_i, \\lambda_{i + 1}]}(\\lambda)$, where $-\\pi = \\lambda_0 < \\lambda_1 < \\dots < \\lambda_{n + 1} = \\pi$. Therefore, we can rewrite the stochastic integral $I(f) = \\int_{(-\\pi, \\pi]}f(v)dZ(v)$ as $I(f) = \\sum_{i = 0}^n f_i[Z({\\lambda_{i + 1}}) - Z(\\lambda_{i})]$. The measure F can be solved based on the definition of isomorphism can be shown as: $(I(f), I (g)) = (\\sum_{i = 0}^{n}f_i[Z(\\lambda_{i + 1}) - Z(\\lambda_{i})], \\sum_{i = 0}^{n}g_i[Z(\\lambda_{i + 1}) - Z(\\lambda_{i})]) = \\sum_{i = 0}^{n} f_i\\bar{g}_i(F(\\lambda_{i + 1} - F(\\lambda_i)))$ The measure $F$ is nothing but the associated distribution function we defined in the proposition 1.1. Remark: $F$ is unique due to the proposition 1.1. The continuous version can be written as $\\int_{-\\pi, \\pi} f(v)\\bar{g}(v)dF(v) = (f, g)_{L^2(F)}$ We can further prove the technical condition of $f$ that $I(f)$ belongs to $L^2(\\Omega, \\mathcal{F}, P)$, and verify that $F$ is the measure of a Hilbert space. (我们必须要证明这样的一个Hilbert空间是存在的，或者说是well-defined。 这种well-defined必须通过概率空间的well-defined证明。这里的过程虽然在逻辑上重要，但是在应用上可以忽略） 2.1 Definition of stochastic integralIf $\\{Z(\\lambda)\\}$ is an orthogonal-increment process on $[-\\pi, \\pi]$ with associated distribution function $F$ and if $f \\in L^2(F)$ (F is constructed in Proposition 1.1), then the stochastic integral $\\int_{(-\\pi, \\pi])} f(\\lambda) d Z(\\lambda)$ is defined as the random variable $I(f)$ constructed above, i.e. $\\int_{(-\\pi, \\pi]} f(v)dZ(v) = I(f)$ 2.2 Properties of the Stochastic Integral(a) The stochastic integral operator $I$ is a linear operator, i.e, $I(af + bg) = aI(f) + bI(g)$ (b) Isometry: $\\mathbb{E}(I(f)\\bar{I(g)}) = \\int_{(-\\pi, \\pi])}f(v)\\bar{g}(v)dv$ (c) $\\mathbb{E}(I(f)) = 0$ (正交增量过程的期望是0) Note that if $Z(\\lambda)$ is any orthogonal increment process on $[-\\pi, \\pi]$ with associated distribution function $F$, then $X_t = I(e^{it \\cdot}) = \\int_{(-\\pi, \\pi]} e^{itv} dZ(v)$ $t \\in \\mathbb{Z}$ is a stationary process with mean zero and autocovariance function: $\\mathbb{E}(X_{t + h}\\bar{X}_t) = \\int_{(-\\pi. \\pi]} e^{iv(t+h)}e^{-ivt}dF(v) = \\int_{(-\\pi. \\pi]} e^{ivh}dF(v)$ Note that the converse also holds: if $\\{X_t\\}$ is any stationary process, then $\\{X_t\\}$ has the representation for an appropriately chosen orthogonal process whose associated distribution function is the same as the spectral distribution function of $\\{X_t\\}$","link":"/2021/01/26/Orthogonal-Increment-Process/"},{"title":"Orthonormal Set and Bessel's Inequality","text":"Keywords: Orthonormal set, Bessel’s inequality, Parseval’s identity, Fourier Coefficients 1. DefinitionsOrthonormal Set: A set ${e_t, t \\in T}$ of elements of an inner-product space is said to be orthonormal if for every $s, t \\in T$, $(e_s, e_t) = 1 $ when $s = t$; otherwise $0$. Closed span: The closed span ($\\bar{sp}(x_t)$) of any subset $\\{x_t, t \\in T\\}$ of a Hilbert space $\\mathcal{H}$ is defined to be the smallest closed subspace of $\\mathcal{H}$ which contains each element $x_t, t \\in T$. (The set of all linear combinations. 这是因为Hilbert 空间是向量空间，及线性空间，对数乘封闭；另外在更加General的形况下考虑$\\sigma$-algebra) Remark for Closed Span in Hilbert space: Note that if $M = \\bar{sp}(x_t, t \\in T)$, then for any $x \\in \\mathcal{H}$, $P_{\\mathcal{M}}x = \\alpha_1 x_1 + \\dots + \\alpha_t x_t$ 2. Fourier CoefficientsIf $\\{e_1, \\dots, e_k\\}$ is an orthonormal subset of the Hilbert space $\\mathcal{H}$ and $\\mathcal{M} = \\bar{sp}\\{e_1, \\dots, e_k\\}$ Then: $P_{\\mathcal{M}} x = \\sum_{i = 1}^k (x, e_i)e_i$ for all $x \\in \\mathcal{H}$ Proof: According to prediction equation, $(x - P_{\\mathcal{M}}x, y) = 0$, i.e. $(x, y) = (P_{\\mathcal{M}}x, y)$, where $y$ is any element in $\\mathcal{M}$. It is clear that the prediction equation holds. Based on the property of projection mapping, we can further get more property of orthonormal set. Comment: Prediction equation真好用！注意这都是内积空间的性质（当然也需要线性空间），不依赖于其他结构。 Definition: The numbers $(x, e_i)$ are sometimes called the Fourier Coefficients. Bessel inequality: $\\sum_{i = 1}^k |(x, e_i)|^2 \\le |x|^2$ (这个不等式在证明Fourier级数收敛的时候还挺重要的) Proof of Bessel inequality From prediction equation, we know that $P_{\\mathcal{M]}} x = \\sum_{i = 1}^k (x, e_i) e_i$. Therefore, $|P_{\\mathcal{M]}} x|^2 = \\sum_{i = 1}^k |(x, e_i)|^2 \\le |P_{\\mathcal{M]}} x|^2 + |(I - P_{\\mathcal{M]}}) x|^2 = |x|^2$ 3. Parseval’s IdentityComplete Orthonormal Set: $e_i$ such that $\\mathcal{H} \\in \\bar{sp}\\{e_i\\}$ (sometimes complete Orthonormal Set is also call orthonormal basis 标准正交基) Separable Hilbert Space: The orthonormal basis is finite or countable. Parseval’s Identity can be claimed as: If $\\mathcal{H}$ is the separable Hilbert space $\\mathcal{H} = \\bar{sp}\\{e_1, e_2, \\}$ where $\\{e_i\\}$ is an orthonormal set, then (1) For each $x \\in \\mathcal{H}$ and $\\epsilon > 0$, there exists a positive integer k and constants $c_1,\\dots ,c_k$, such that $|x - \\sum_{i =1}^k c_i e_i| < \\epsilon$ (可数可以被有限任意逼近, 也算是一种直观的体现, 可以找一列递增并且逼近正交基的集合序列证明) (2) $x = \\sum_{i = 1}^{\\infty} (x, e_i)e_i$ (Bessel inequality 和 内积空间的连续性可以证得) (3) $(x, y) = \\sum_{i = 1}^{\\infty} (x, e_i)(e_i, y)$ (4) $x = 0$ iff $(x, e_i) = 0$ Most of the aforementioned properties are obvious. The proof of (3) can be written as: $\\lim\\limits_{n \\xrightarrow \\infty} \\sum_{i = 1}^n (x, e_i) e_i \\xrightarrow x$ $\\lim\\limits_{n \\xrightarrow \\infty} \\sum_{i = 1}^n (y, e_i) e_i \\xrightarrow y$ According to the continuity of inner-product space, we can prove that: $(x, y) = \\lim\\limits_{n \\xrightarrow \\infty} (\\sum_{i = 1}^n (x, e_i) e_i, \\sum_{i = 1}^n (y, e_i) e_i ) = \\sum_{i = 1}^{\\infty} (x, e_i)(e_i, y)$ Comment: 这一部分可以说是傅里叶分析为代表的信号分解的基石。如果我们只考虑离散时间的时间序列(大多是时候都是)信号分解的前提是有符合内积定义的内积，以及一套标准正交基。在连续傅里叶变换中，我们可以理解为这一些列都在某个函数的基下进行，本质上其实是不变的(Recall: Dirac function should be considered in a space whose density is generated by a Lebesgue integrable function)","link":"/2021/01/23/Orthonomal-set/"},{"title":"Probabilistic programming-Notes-KL Divergence-Part3","text":"Keywords: support of distribution, forward KL, reverse KL, absolute continuity (greatly influence the properties of approximation) 1. A review of KL divergenceIf $P$ and $Q$ are probability measures over a set $\\chi$, and $P$ is absolutely continuous with respect to $Q$, which means $P < Q$ i.e. $\\forall \\epsilon \\quad Q(\\epsilon) = 0 \\Rightarrow P(\\epsilon) = 0$. Then the KL divergence can be defined as:","link":"/2020/11/02/Probabilistic-programming-Notes-KL-Divergence-3/"},{"title":"Periodogram","text":"Keywords: Inference for the spectrum Consider an arbitrary set of (can be complexed-valued) observations $x_1, \\dots, x_n$ made at times $1, \\dots, n$ respectively. The vector $\\mathbf{x} = (x_1, \\dots, x_n)$ belongs to a n-dimensional complex space $\\mathbb{C}^n$. If $u$ and $v$ are two elements of $\\mathbb{C}^n$, define the inner product as $< u, v > = \\sum_{i=1}^n u_i\\bar{v}_i$ 1. Definitions1.1 Orthonormal set of this complex space$\\mathbf{x} = \\sum_{j \\in F_n} a_j \\mathbf{e}_j$, where $\\mathbf{e_j} = n^{-1/2} (e^{i\\omega_j}, e^{i2\\omega_j}, \\dots, e^{in\\omega_j})$, $F_n = \\{j \\in \\mathbb{Z}: -\\pi < \\omega_j \\le \\pi\\} = \\{-[(n-1)/2], \\dots, [n/2]\\}$ Note that $< \\mathbf{e}_j, \\mathbf{e}_k > = n^{-1} \\sum_{r=1}^n e^{ir(\\omega_j - \\omega_k)}$ $< \\mathbf{e}_j, \\mathbf{e}_k > = 0$ if $j =k$, otherwise $n^{-1} e^{i(\\omega_j - \\omega_k)}\\frac{1- e^{in(\\omega_j - \\omega_k)}}{1- e^{i(\\omega_j - \\omega_k)}} = 0$ (等比数列求和, note $n(w_j - w_i) = n \\frac{2\\pi}{n}(j -k) = (j-k) 2\\pi$, moreover $e^{k \\cdot 2\\pi i} = 1$) Just like the inversion formula in the spectral analysis of time series, we can rewrite $x = \\sum_{j \\in F_n} a_j \\mathbf{e}_j$, where $a_j = < \\mathbf{x}, \\mathbf{e}_j > = n^{-1/2} \\sum_{t = 1}^n x_t e^{-it\\omega_j}$. Note that $< \\mathbf{x}, \\mathbf{e}_j > = < \\sum_{g=1}^n a_j e_j, e_j > = a_j$. This is also called discrete Fourier transformation of $\\mathbf{x} \\in R^n$. Comment: 其实还应该证明下closed span, 同Fourier sequence。 1.2 The periodogram of xThe value $I(\\omega_j)$ of the periodogram of $\\mathbf{x}$ at frequency $w_j = 2\\pi j/n, j \\in F_n$ is defined as: $I(\\omega_j) = |a_j|^2 = |< x, e_j >|^2 = n^{-1} |\\sum_{t=1}^n x_t e^{-itw_j}|^2$ Note that $|x|^2 = \\sum_{j \\in F_n} I(w_j)$. Comment: Note that the definition of periodogram depends on the definition of random variables. 2. The Periodogram in terms of the sample autocovariance function2.1 The estimation of periodogramTheorem: If $\\omega_j$ is any non-zero Fourier frequency, then $I(w_j) = \\sum_{|k|< n} \\hat{\\gamma}(k) e^{-ik\\omega_j}$, where $\\hat{\\gamma}(k) = n^{-1} \\sum_{t=1}^{n-k} (x_{t + k} - m)(\\bar{x}_{t} - \\bar{m})$, $m = n^{-1} \\sum_{t=1}^n x_t$ Proof: 易证。 2.2 Testing for the presence of Hidden Periodicities$H_0$: The data is generated by a Gaussian white noise sequence. $H_1$: The data is generated by a Gaussian white noise sequence with a superimposed deterministic periodic component.","link":"/2021/02/04/Periodogram/"},{"title":"Probabilistic programming-Notes-Variational Inference-Part1","text":"Compared to MCMC, variational Inference is a faster method to approximate difficult-to-compute posterior distribution. Variational Inference is an optimization problem, while MCMC is an asymptotic method. 1. NotationConsider a joint density of latent variables $\\mathbf{z}=z_{1:m}$ and observation $\\mathbf{x}=x_{1:m}$: $$\\mathbb{P}(\\mathbf{x}, \\mathbf{z}) = \\mathbb{P}(\\mathbf{x}|\\mathbf{z}) \\mathbb{P}(\\mathbf{z})$$ The above equation unveils the data generation process. The inference process can be written as the following form: $$\\mathbb{P}(\\mathbf{x}, \\mathbf{z}) = \\mathbb{P}(\\mathbf{z}|\\mathbf{x}) \\mathbb{P}(\\mathbf{x})$$ The job is to find the posterior probability of latent variable $\\mathbf{z}$. MCMC constructs an ergodic Markov Chain on $\\mathbf{z}$$ whose stationary distribution is the posterior $\\mathbb{P}(\\mathbf{z}|\\mathbf{x})$, while VI consider the following optimization problem: $$q^{*}(\\mathbf{z}) = argmin_{q \\in \\mathcal{Q}} d(q(\\mathbf{z}), p(\\mathbf{z}|\\mathbf{x}))$$ , where $\\mathcal{Q}$ is a family of approximation densities $\\mathcal{Q}$, $d$ is a function which measure the difference of two distribution. The common choices of $d$ are KL divergence and Wasserstein distance. Note that VI cannot guarantee the we are sampling from the exact posterior distribution but one which is similar to the posterior distribution. In this note, the $d$ function is KL divergence in the following context. 2. KL divergenceNote that KL divergence is not a good enough ‘distance’, although KL is always positive , which means if KL is zero, then two distributions are the same almost surely. The following proof will be an exercise for the application of variational calculus. $\\mathbf{Proof: KL(q(z) | p(z|x)) >=0}$ Note that g, f are pdf (measurable functions) over $(\\Omega, \\sigma_{\\Omega})$, we can freely exchange derivative and integral. Let’s rewrite this problem as: Given $\\int_{x \\in \\Omega} g(x)dx = 1$: $$\\max \\int_{x \\in \\Omega} g(x) log(f(x))dx$$ $$s.t. \\int_{x \\in \\Omega} f(x)dx = 1$$ The Lagrange equation for this optimization problem is: $$F(\\lambda) = \\int_{x\\in\\Omega} log(f(x))dx - \\lambda (\\int_{x \\in \\Omega}f(x)dx - 1)$$ For the optimal $\\bar{f}$, if we add a disturb to this function: $$F(\\theta, \\lambda) = \\int_{x\\in\\Omega} log(\\bar{f}(x) + \\theta y(x))dx - \\lambda (\\int_{x \\in \\Omega}\\bar{f}(x)+ \\theta y(x)dx - 1)$$ The first order condition can be written as: $$\\frac{\\partial F}{\\partial \\theta}\\Big | _{\\theta = 0} = 0 \\Leftrightarrow \\forall y \\quad \\int (\\frac{g(x)}{\\bar{f}(x)} - \\lambda)y(x)dx = 0 \\Leftrightarrow \\frac{g(x)}{\\bar{f}(x)} = \\lambda \\quad a.s.$$ $$\\frac{\\partial F}{\\partial \\lambda} \\Big |_{\\theta = 0} \\Leftrightarrow \\int \\frac{1}{\\lambda} g(x)dx = 1 \\Leftrightarrow \\lambda = 1$$ Therefore, the necessary condition for reaching the maximum of $\\int_{x \\in \\Omega} g(x) log(f(x))dx$ is $\\bar{f} = g$ a.s. Based on the definition of KL divergence, we can easily conclude $KL(q(z) | p(z|x)) >=0$. Kl divergence is closely related to the concept of entropy. A high self-entropy means chaos(high volatility), while a low self-entropy means lower volatility. KL divergence has the following drawbacks: 1) KL divergence tends to pay more emphasis on the region where $q$ has little mass (caused by the log function), which means KL underestimate the variance. 2) KL divergence is not symmetric, so it is not a metric. 3. ELBOAccording to the definition: $KL(q(z)|p(z|x)) = \\mathbb{E}_{q}(q(z)) - \\mathbb{E}_{q}(p(z|x)) = \\mathbb{E}_{q}(q(z)) - \\mathbb{E}_{q}(p(z, x)) + log p(x)$ We can further conclude that $log p(x) = KL(q(z) | p(z|x)) + ELBO(q)$, which means $log p(x) \\ge ELBO(q)$ (Lower bond for $log p(x)$) Note that in VI, the KL divergence cannot be $KL(p(z|x)|q(z)) $ since the posterior distribution is difficult to sample or approximate. However, given a variational family we are easy to sample from $q$. In the computation process, reparameterization trick (or MC integral for applied mathematics guys) will be helpful. Therefore, minimize the difference between $q$ and posterior distribution is equivalent to maximize the ELBO: $$ELBO(q) = \\mathbf{E}_{q}(p(z, x)) - \\mathbf{E}_{q}(q(z)) $$ (recall EM algo) Note that we need the computable gradient of ELBO if we want to use the first order condition of maximization. The most important trick here is MC integral. Let $\\Phi$ denote the parameters of variational family $\\mathcal{Q}$. The gradient computation can be written as: $$\\nabla_{\\Phi}ELBO = \\nabla_{\\Phi} \\mathbf{E}_{q}[log(p(x, z))] - \\nabla_{\\Phi} \\mathbf{E}_{q}[log(p(z))]$$ $$= \\nabla_{\\Phi} \\int [log(p(x,z)) - log(q)]q(z) dz = \\int \\nabla_{\\Phi}\\Big([log(p(x,z)) - log(q)]q(z)\\Big) dz$$ $$=\\int \\nabla_{\\Phi} \\Big(log(p(x, z) - log(q(z))\\Big)q(z)dz + \\int \\nabla_{\\Phi} \\Big(q(z)\\Big)[log(p(x, z)) - log(q(z))]dz$$ Note that the first term can be easily calculated if we notice the following fact: $$\\mathbb{E}_{q}\\Big[\\nabla_{\\Phi} log[q(z)]\\Big] = \\mathbb{E}_{q}\\Big[\\ \\frac{\\nabla q(z)}{q(z)} \\Big] = \\int \\frac{\\nabla q(z)}{q(z)} q(z) dz = 0$$ Therefore, the first term is 0. The gradient of ELBO can be further written as: $$\\nabla_{\\Phi}ELBO = \\int \\nabla_{\\Phi} \\Big(q(z)\\Big)[log(p(x, z)) - log(q(z))]dz$$ Given the equation: $$\\nabla_{\\Phi}q(z) = q(z) \\nabla_{\\Phi} log(q(z))$$ The gradient can be written in a form of expectation: $$\\nabla_{\\Phi}ELBO = \\int q(z) \\nabla_{\\Phi} log(q(z))[log(p(x, z)) - log(q(z))]dz = \\mathbf{E}(\\nabla_{\\Phi} log(q(z))[log(p(x, z)) - log(q(z)))$$ $$\\simeq \\frac{1}{S} \\sum_{i=1}^S \\nabla_{\\Phi} log(q(z_i))[log(p(x, z_i)) - log(q(z_i))$$ The above process is also called vanilla BBVI algorithm. Note that we can further decrease the variance of the estimation with some variance reduction method. ReferenceVariational Inference: A review for Statisticians","link":"/2020/10/11/Probabilistic-programming-Notes-Varational-inference-1/"},{"title":"进程管理","text":"keywords: 进程管理 进程是在现代操作系统保证执行并发，资源共享和用户随机的特点下，用来描述程序执行下的资源分配基本单位（进程用于描述资源分配的基本单位，这是功能而非定义）。 为了讨论进程管理，首先应该先对程序下定义。程序是有独立功能的，在时间次序上严格相继（上一条指令执行结束是下一条指令开始执行的充分必要条件）的指令集合，是一个静态的概念。程序具有顺序性，封闭性和可重复性的特征。与程序相对的，进程是一个动态执行过程，或者说程序在CPU上的执行活动被称为进程。进程在执行过程中动态地创建，并在调度执行中消亡。逻辑上，这种动态过程应该存在时间轴上的静态切片，系统中也对应应该存在描述进程存在的物理实体，或者说进程的静态描述（事实上也去是如此，任何动态在某一时间点上都应该存在静态切片，否则动态，或者说另外一个维度上的参数化，就不是well-defined）。进程的静态描述包括：进程控制块（PCB，掌握进程的状态，是系统掌握进程的唯一实体，部分或全部存于内存里），有关程序段和对该数据段进行操作的数据结构集合。 注意并发和并行的区别：并发是指在一段时间内同时执行，而并行是指在某一时间点上同时执行。并发实际上是CPU的分时行为，通过快速在时间上分片使得某一特定程序在其他程序执行没有结束前就开始执行。并行只有在有多个计算通道存在的情况下才能真实发生。（Recall: python 全局解释器锁）。 1 程序的并发执行多个程序可以执行并发的条件是：任意程序的读和剩余程序的写的交集为空；任意程序的写和其余程序的写的交集为空（对同时读取是没有要求的，这里主要是考虑到并发执行过程中的随机性，Bernstein 1966）。竞争条件等概念可参考C++并发编程。 2 进程的描述 （PCB 和上下文）进程是被PCB标识和描述的。PCB全部或部分地存在内存中，其记录了进程页表指针和CPU现场(Recall 陷阱处理机构)。除此之外，进程的静态描述除了当前时点的信息外（注意联系PCB中的信息和正文的关系， PCB中记录了进程的标识，控制信息，资源管理信息和CPU现场），还包括程序段和数据集的上下文，体现静态关联顺序。例如，在进程执行过程中，由于中断，等待和程序出错等原因导致进程中断时，操作系统需要记忆进程运行到了什么阶段；另外一个常见的情景是条用子程序。在等待子程序后，进程将返回何处执行，执行结果将放在何处都需要进行记忆。 已经执行过的进程指令和数据在相关寄存器和堆栈中的内容被称为上文，正在被执行的指令和数据在相关寄存器和堆栈中的内容被称为正文，在时间序列上将要被执行的指令和数据被称为进程的下文。 进程的上下文切换发生在不同进程而不是相同进程中（进程不能唤醒自己）。进程上下文切换总共包括三个部分，并涉及到三个进程。第一部分为转移被切换进程的正文部分至有关存储区（保存上文）；第二个进程是操作系统进行分配资源，选取新的进程，第三部分是将被切换线程的资源从相关存储器中取出，并送到相关寄存器和堆栈中，激活被选中的新进程。 3 进程的控制原语就是原子类型的（不可分割的）语句。 进程控制就是使用一些工具来创建，撤销以及完成进程各状态间（5状态2队列）的转换，从而达到多进程高效率并发协调，实现资源共享的目的。 原语是指系统态下执行的具有特定功能的程序段。原语可以分为机器指令级原语（执行期间不准许间断），另一类是功能级原语（不能并发执行）。由于进程执行的随机性，操作系统中的进程控制程序段会被做成原语。 操作系统对进程的行为主要包括进程的创建，进程的撤销，进程的阻塞和唤醒。 进程可以由系统或者父进程创建，由父进程创建的子进程享有父进程所有的资源。进程撤销时，父进程需要检查子线程是否还有子线程，并撤销这些线程的资源。进程被创建后处于就绪状态，如果被调度（虽然教材中没有明确说明，但是我意识到调度特别指代从就绪状态到执行状态这一环节）系统选中则进入执行状态，如果未被系统选中则进入等待状态（存入进程的等待队列，进程归根到底可以用正文，上下文描述工作状态，并由进程控制块PCB唯一标识）。 进程自身可以在未达到特定条件的情况下自己阻塞自己，但不能自己唤醒自己（因为在等待过程中进程是以正文和上下文的形式储存的，不能执行）。进程的阻塞行为通过阻塞原语实现。一般的阻塞流程是：保留执行状态下的CPU现场，随后在PCB中将进程的状态设置为阻塞状态（等待状态）并加入等待队列，此后操作系统会再次转至进程调度（即从就绪队列中执行一个就绪状态下的进程，防止运算资源空转）。 进程的唤醒由操作系统的唤醒原语执行。进程可以被系统唤醒（recall 并发）或者被在某一时间发生时被操作系统唤醒，从等待队列中转移到就绪队列，随后等待调度程序或者直接返回值。 4 进程的互斥和实现在现实开发中，Bernstein条件常容易被打破。定义不允许多个并发进程交叉执行的一段程序为临界部分，或者临界区域。对于公共对象的读写如果不是原子操作的，那么在读取或者写入中，为了保证程序的独立性和可重复性不被打破，必须阻塞一些程序，对并发执行速度造成进一步的制约(下减记为间接约束)。系统分配和注销相应共有资源的管理办法就是互斥。由用户程序执行开始的随机性可知，把临界区的各个过程按照不同的时间排列调用是不现实的。因此，有些硬件设施设置了test and set指令来解决互斥问题，但是一种更加节约资源的做法是使用信号量和P,V原语。 在共享资源的时候嵌套一层接口，设计访问该共享资源的流程为: P(semaphore) 读取数据 V(semaphore)。 信号量(semaphore)是一个能够反映共享资源状态的数值。在互斥锁的情境下，这个值被初始化为1。P原语是如下操作的集合（是原子的）：信息量 -= 1；如果信息量 $\\ge 0$，调度程序将该进程设置为执行状态，并开始执行；如果信息量$< 0$，则该进程进入就绪队列，等待调度程序唤醒（在清华这本书上，有时候就绪队列和等待队列是替代使用的，稍微有些让人费解）。这样设计的结果是，只要有进程在使用资源，那么semaphore就不可能 = 1，即semaphore最大就是0。此时如果另有进程尝试调用该数据，只要经过P原语，semaphore数值就必定小于0。P原语起到了判断是否有进程在访问共享数据的作用，这也意味着P原语不能在用于唤醒别的进程，因为P原语不能判断进程是否已经调用共享数据结束。 V 原语是以下指令的集合：semaphore += 1， 如果semaphore $\\le$ 0, 此时调度程序唤醒就绪队列中的一个进程，开始读取共享数据；如果semaphore $> 0$， 此时该进程返回调用处继续执行。 注意这种设计是结合了原语设计上的局限性构建的。可以想象，如果原语本身可以通过进程上的操作来实现，那么也不会存在竞争条件。事实上，原语是在硬件层面上的设计。 5 同步和生产者-消费者问题 上述的过程实际上是异步的（这里是指开始时间的随机性和运行时间的独立性），即各个进程之间的调用没有先后顺序上的要求：一个进程执行的充分必要条件不由任何在等待队列中的进程产生。如果两个（或多个）进程之间的发生顺序存在约束，例如一个进程需要另一个进程对数据修改完毕后再调用，此时进程必须不断检查另一个进程是否运行结束，此时会造成大量的资源浪费(这个共享资源已经被P, V原语封装过了)。这种异步环境下一组并发进程因直接制约而互发消息，互相等待，使得各进程按一定的速度执行的过程称为进程间的同步。P,V原语同样可以用来实现进程同步，技巧就是用多个信号量交叉使用。同步问题可以抽象为producer-consumer problems。","link":"/2021/05/05/ProcessManagement/"},{"title":"Probabilistic programming-Notes-Varational inference-Part2","text":"This note includes mean-field and structured variational families. Besides VI, mean-field and other variational family can also be used in the inference of probabilistic neural network. 1. Mean-field variational familyAs the introduction in the last note, the variational family $\\mathcal{Q}$ determines the optimization problem. Although we have AD algorithm, it is easier to optimize over a simple family. The mean-field variational family assumes all the latent variables are mutually independent. A generic member of the mean-field variational family can be written as: $$q(z) = \\Pi_{i=0}^{i=n} q_{i}(z_i)$$ For mean-field variational family, we can use coordinate ascent algorithm(CAVI) to solve the optimal problem. First, note that ($p(\\cdot)$) is the prior distribution. $$ELBO(q) = \\mathbf{E}_q(p(x,z)) - \\mathbf{E}_q(q(z)) = \\mathbf{E}_q(p(x|z)) + \\mathbf{E}_q(p(z))- \\mathbf{E}_q(q(z)) $$ $$= \\mathbf{E}_q(p(x|z)) - KL_q(q(z) |p(z))$$ The above equation shows that $q$ tends to approximate both the likelihood and priori distribution of z, which is a balance between terms. The maximization of $ELBO$ possess the properties of Bayesian estimations to some extent. The nice properties of mean-field variational family introduce an efficient optimization algorithm, which is the well know coordinate ascent VI (CAVI). 1.1 CAVIAccording to the definition of expectation (it is obvious if you rewrite the ELBO as integral), we can conclude that: $$ELBO(q_j) = \\mathbb{E}_{j}\\Big[\\mathbb{E}_{-j}[\\text{log} p(z_j, \\mathbf{z}_{-j}, \\mathbf{x})] \\Big] - \\mathbb{E}_{j}\\Big[\\text{log} q_j(z_j)\\Big] + const$$ Again, I will solve this with variational calculus intentionally. Let $f(z_j) = \\mathbb{E}_{-j}[\\text{log} p(z_j, \\mathbf{z}_{-j}, \\mathbf{x})] $. At the optimal pdf $q^{*}(z_j)$, let’s disturb the function with function $y(z_j)$. The objective function: $$F(\\theta) = \\int_{z_j} f(z_j)(q^{*}(z_j) + \\theta y(z_j))dz_j - \\int_{z_j} (q^{*}(z_j) + \\theta y(z_j)) \\text{log}(q^{*}(z_j) + \\theta y(z_j))dz_j$$ The first-order condition is: $$\\frac{dF}{d \\theta} \\Big | _{\\theta = 0} = \\int_{z_j} f(z_j) y(z_j)dz_j - \\int_{z_j} y(z_j)dz_j - \\int_{z_j} \\text{log}q^{*}(z_j)y(z_j)dz_j = 0$$ $$q^{*}(z_j) = \\text{exp}(f(z_j) - 1)$$ Therefore, for each step of CAVI, we can just set $q^{*}(z_j) \\propto \\text{exp}(f(z_j))$ (recall the Gibbs sampling) to reach a local optimum. Note that mean-field approximation cannot fit complex posterior (like XOR). However, the mean field approach yields a globally consistent set of moments. 2. Structured variational familyAlthough mean-field variational family make the problem tractable, it reduce the fidelity. A natural idea is to add a “new layer” to the model. In structured variational family the joint distribution can be factorized as: $$p(y,z,\\beta) = p(\\beta)\\Pi p(y_n, z_n | \\beta)$$ , which means $(y_n, z_n)$ is conditionally independent given global parameter $\\beta$. Note that we can also assume $p(y,z,\\beta) = p(\\beta)\\Pi p(y_n| z_n)q(z_n | \\beta)$, which means $y_n$ is independent of $\\beta$ given $z_n$. Most literature restrict their attention on conditionally conjugate models. 2.1 A review of the exponential distribution familyExponential distribution family admit conjugate priors. An exponential family distribution can be parameterized by a vector of expectation parameters $\\mu = \\mathbb{E}_{p(y)}\\Big[T(y)\\Big]$, where $T(y)$ is the vector of sufficient statistics of the data. We can assign a conjugate prior distribution $p(\\mu)$to these parameters, which is parameterized by the prior expectation $\\bar{\\mu_0} = \\mathbb{E}_{p(\\mu)}[\\mu]$. Upon observing N independently sampled data points, the posterior expectation parameters are convex combination of the prior parameters and the maximum likelihood estimators: $$\\bar{\\mu} = \\lambda \\circ \\mu_0 + (1-\\lambda) \\circ \\mu_{ML}$$ The exponential family distribution admits the conjugate prior. That’s the reason why we want to use this function 2.2 Conditionally conjugate modelAssume: $$p(\\beta) = h(\\beta) \\text{exp}(\\eta \\cdot t(\\beta) - A(\\eta))$$ $$p(y_n, z_n|\\beta) = \\text{exp} \\Big(t(\\beta) \\cdot \\eta_{n}(y_n, z_n) + g_n(y_n, z_n)\\Big )$$ , where the base measure $h(\\cdot)$ and log-normalizer $A(\\cdot)$ are scalar-valued functions, $\\eta$ is a vector of natural parameters, $t(\\beta)$ is a vector-valued sufficient statistic function, $g_n$ is a scalar-valued function and $\\eta_n$ is a vector-valued function. The posterior distribution can be written as: $$p(\\beta | y, z) = h(\\beta) \\text{exp} \\Big( (\\eta + \\sum_{n} \\eta_n (y_n, z_n)) \\cdot t(\\beta) - A(\\eta + \\sum_n \\eta_n (y_n - z_n))\\Big)$$ The goal is to approximate the intractable posterior $p(z, \\beta | y)$ with a distribution $q(z, \\beta)$ in some tractable family by solving an optimization problem. Note that this assumption is about how we believe the real data is generated. 2.2.1 Method 1: Mean-field assumption for $q(z, \\beta)$$$q(z, \\beta) = q(\\beta) \\Pi_n \\Pi_m q(z_n,m)$$ Recall that $z$ is a $m$ dimensional vector. 2.2.2 Method 2: Structured Stochastic VIIn this framework, the variational distribution $q$ is of the form: $$q(z, \\beta) = (\\Pi_{k} q(\\beta_k)) \\Pi_{n} q(z_n | \\beta)$$ The papers restrict $q(\\beta)$ to be in the same exponential family as the prior $p(\\beta)$, so that: $$q(\\beta) = h(\\beta) \\text{exp} \\Big(\\lambda \\cdot t(\\beta) - A(\\lambda)\\Big)$$, where $\\lambda$ is a vector of parameters that controls $q(\\beta)$. We also require that any dependence under $q$ between $z_n$ and $\\beta$ be mediated by some vector-valued function $\\gamma_n(\\beta)$, so that we may write $q(z_n|\\beta) = q(z_n | \\gamma_n(\\beta))$. Review the probabilistic graph in case you feel confused about some dependency relationships. Objective for structured stochastic VI Our goal is to find a distribution $q(\\beta, z)$ that has low KL divergence to the posterior $p(\\beta, z | y)$. The KL divergence between $q$ and full posterior is: $$KL(q_{z, \\beta} | p(z, \\beta |y)) = -\\mathbb{E}_q[\\text{log} p(y, z, \\beta)] + \\mathbb{E}_q[\\text{log} p(z, \\beta)] + \\text{log} p(y)$$ The ELBO can be further written as: $$ELBO = \\mathbb{E}_q[\\text{log} p(y, z, \\beta)] - \\mathbb{E}_q[\\text{log} p(z, \\beta)] = \\mathbb{E}_q [\\text{log} \\frac{p(\\beta)}{q(\\beta)}] + \\sum_n \\mathbb{E}[\\text{log} \\frac{p(y_n, z_n |\\beta)}{q(z_n | \\beta)}]$$ $$= \\int_{\\beta} q(\\beta) (\\text{log}\\frac{p(\\beta)}{q(\\beta)} + \\sum_n \\int_{z_n} q(z_n | \\beta) \\text{log} \\frac{p(y_n, z_n | \\beta)}{q(z_n | \\beta)} dz_n)d\\beta \\le \\text{log} p(y)$$ (Remark: Marginal distribution in the second step) Note that the second term is a ELBO of the marginal probability of the $n$th group of observations: $$\\int_{z_n} q(z_n | \\beta) \\text{log} \\frac{p(y_n, z_n | \\beta)}{q(z_n | \\beta)}d z_n = -KL(q_{z_n | \\beta} | p_{z_n|y_n, \\beta}) + \\text{log} p(y_n | \\beta) \\le \\text{log} p(y_n | \\beta)$$ (Remark: the definition of conditional probability) Therefore, given this nice structure, we can further conclude that the maximization of $ELBO(q)$ is equivalent to minimizing the ‘local’ KL divergence between $q(z_n|\\beta)$ and $q(z_n|y_n, \\beta)$. Note that we also assume that the function $\\gamma_n{\\beta}$ (that’s smart!) that controls $q(z_n | \\beta) = q(z_n | \\gamma_n(\\beta))$ is defined to ensure that: $$\\nabla_{\\gamma_{n}} \\int_{z_n} q(z_n | \\gamma_{n}(\\beta)) \\text{log} \\frac{p(y_n, z_n |\\beta)}{q(z_n | \\gamma_n(\\beta)}dz_n = 0$$ Note that the $ \\gamma_{n}(\\beta)$ can be implicitly expressed. (I really love this idea) ReferenceStructured Stochastic Variational Inference","link":"/2020/10/11/Probabilistic-programming-Notes-Varational-inference-2/"},{"title":"Statistical Inference-Notes-Part1-Decision Theory","text":"Keywords: Parameter space, sample space, risk function For noncommercial propose only. Some figures may be subject to copyright. 1. The formulation of decision theory1.1 Elements in decision theory1) Parameter space $\\mathcal{\\Theta}$ which will be a subset of $\\mathbb{R}^d$ for some $d > 1$. It is the admissible set of unknown parameters in the decision problem. It is heuristic in this framework. 2) Sample space $\\mathcal{X}$. The admissible set of X. For example, the sample space of stock price must be non-negative real number. The sample space is the possible value for observation. 3) A family of probability measure on the sample space $\\mathcal{X}$. By convention, the probability measure can be written as $\\{\\mathbb{P}_{\\theta}(x), x \\in \\mathcal{X}, \\theta \\in \\mathcal{\\Theta}\\}$. If we recall the definition of probability space, we can easily find out that the decision theory only consider a subspace of probability measure, which is parameterized by $\\mathcal{\\Theta}$. 4) Action space $\\mathcal{A}$. This represents the set of all actions or decisions available to the experimenter. For a hypothesis testing problem, the action set is $\\{“accept H_0”， “accept H_1”\\}$. In an estimation problem, the action set $\\mathcal{A} = \\mathcal{\\Theta}$. 5) A loss function $L: \\mathcal{\\Theta} \\times A \\xrightarrow{} \\mathbb{R}$. The function implies the property of a specific action $\\mathcal{a}$ when the parameters are $\\mathcal{\\Theta}$. 6) A set $\\mathcal{D}$ of decision rules. An element d: $\\mathcal{X} \\xrightarrow{} \\mathcal{D}$. Each point x in $\\mathcal{X}$ is associated with a specific action $d(x) \\in A$. It is easily to connect this formulation with the reinforcement learning. Decision rule is a function mapping from the observation to action, which further implies that the risk function measure the coincidence of the parameter and the observation. 1.2 The definition of risk functionRisk function is $\\mathcal{\\Theta} \\times \\mathcal{D} \\xrightarrow{} \\mathbb{R}$. Note that the difference between the risk function and loss function is that the loss function measure the relationship of parameters and actions, which is deterministic, while the risk function is defined on the decision rule (since $x \\in \\mathcal{X}$ is a random variable). Of course, we can use other criterion besides expectation. However, expectation of loss function is the most reasonable one (we can further penalize the variance etc.). The risk function is trying to consider the influence of random variables. Heuristically, the risk function is defined as: $$R(\\theta, d) = \\mathbb{E}_{\\theta} L(\\theta, d(X))$$ Note that the definition of risk function is based on the hypothesis of probability measure space. Once we determine the family of probability measure and decision rule, we can exactly determine the risk function. Risk function is similar to the concept of utility function in the Economics. 2. The criterion for a good decision ruleThe final output of the decision theory is the decision rule, which is a function of $\\mathcal{X} \\xrightarrow{} \\mathcal{A}$. We need to distinguish the decision rules, and find the better one. 2.1 AdmissibilityGiven two decision rules $d$ and $d^{‘}$, if $R(\\theta, d) \\le R(\\theta, d^{\\prime})$ for all $\\theta \\in \\mathcal{\\Theta}$, then $d$ strictly dominates $d^’$. If there exists a decision rule $d$ which strictly dominate decision rule $d^\\prime$, then $d^\\prime$ is said to be inadmissible (otherwise admissible). However, in practice, it is impossible to get an analytical solution of risk function. (A closed-form solution means we can extend the properties of risk function to any other point in the space). Therefore, it is very hard to determine if a decision rule is admissible or not. 2.2 Minimax decision rulesThe maximum risk of a decision rule is defined as: $MR(d) = \\sup\\limits_{\\theta \\in \\mathcal{\\Theta}}R(\\theta, d)$ A decision rule is minimax if it minimizes the maximum risk: $MR(d) \\le MR(d^{\\prime})$ for all decision rules $d^{\\prime} \\in D$ The idea behind the minimax decision rule is that we want to optimize the worst scenario. Note that minimax may not be the admissible decision rule and, apparently, may not be unique. However, if you are trying to choose the decision rule based on this criterion, you must follow the minimax idea. Note that $\\max_{\\theta \\in \\Theta} = \\min\\limits_{d^\\prime \\in \\mathcal{D}} \\max\\limits_{\\theta \\in \\Theta} R(\\theta, d^\\prime)$ 2.3 UnbiasednessA decision rule $d$ is defined as unbiased if $\\mathbb{E}_{\\theta} {L(\\theta^{\\prime}, d(X))} \\ge \\mathbb{E}_{\\theta} {L(\\theta, d(X))} $ Note that this notation is somehow confusing. $\\mathbb{E}_{\\theta} {L(\\theta^{\\prime}, d(X))} = \\mathbb{E}_{X}(R(\\theta, \\delta)|\\theta)$. Since $\\delta$ is the function of $X$, we consider the connection of randomness of $X$. The decision here is Note that this definition is quite tricky. We are calculating the expectation over the probability measure of $\\mathcal{X}$, which is parameterized by $\\theta$ (that’s why we use subscript $\\theta$). The unbiasedness is a criterion that does not depend on risk function only. The definition of unbiasedness depends on the parameters. This criterion makes the risk function self-consistent. However, the unbiasedness is somehow between a distraction and a total irrelevance. (原来不止我一个人这么想) 2.4 Bayes decision ruleWe must specify a prior distribution, which represents our prior knowledge on the value of the parameter $\\theta$, and is represented by a function $\\pi(\\theta), \\theta \\in \\mathcal{\\Theta}$. The prior distribution to be absolutely continuous, meaning that $\\pi(\\theta)$ is taken to be some probability density on $\\mathcal{\\Theta}$. In the continuous case, the Bayes risk of a decision rule $d$ is defined to be $r(\\pi, d) = \\int_{\\theta \\in \\mathcal{\\Theta}}R(\\theta, d)\\pi(\\theta)d\\theta$. The decision rule $d$ is said to be a Bayes rule, with respect to a given prior $\\pi(\\dot)$, if it minimizes the Bayes risk, so that $r(\\pi, d) = \\inf \\limits_{d^\\prime \\in \\mathcal{D}} r(\\pi, d^\\prime) = m_{\\pi}$ Note that if the $\\inf$ cannot be reached, we can further consider the $\\epsilon$ decision rule, i.e. $R(\\pi, d_{\\epsilon}) < m_{\\pi} + \\epsilon$. It is nothing but the definition of infimum with a more computable result. 2.5 Randomized decision ruleThe Bayes decision rule consider the probability measure over $\\theta$, while the randomized decision rule considers the probability measure over decision rules. Note that in the traditional mechanism, the probability measure is independent of the data. The risk function under the randomized decision rule can be written as: $R(\\theta, d^*) = \\sum_{i=1}^l p_i R(\\theta, d_i)$ If we consider the independent (wrt to data), it is easy to construct examples in which $d^{*}$ is formed by randomizing the rules $d_1, \\dots, d_l$ but $\\sup_{\\theta} R(\\theta, d^{*}) \\le \\sup_{\\theta} R(\\theta, d_i)$ for each i. The statement can be easily proved. However, it is only holds for risk neutral decision maker. We also should consider the second order moment (the variance). We can formulate the metric like expected loss in this scenario. It will be very interesting if we combine the Bayesian rules and randomized decision rule together. 3. A finite parameter space exampleThe discussion in the reference is not accurate. Based on the statement in the reference, we must construct an algebra over decision set. However, if we consider the risk function a measurable function, we can avoid the construction of an algebra. If we consider the limited number of parameters $\\theta$, we can better understand the property of the decision set. Let’s consider a parameter space which is a finite set: $\\mathcal{\\Theta} = \\{\\theta_1, \\theta_2 \\dots, \\theta_t\\}$. Let’s consider a $t$ dimensional space. For a specific decision policy $d$ (including randomized policy), there is a specific generic point in this space, which can be written as $((R(\\theta_1, d), R(\\theta_2, d), \\dots, R(\\theta_3, d)))$. For any decision rule in set $\\mathcal{D}$, let’s consider $d_1$ and $d_2$. Since we can always find a randomized decision rule given a specific probability measure, the risk function space is convex over decision. For any $0","link":"/2021/01/02/Statistical-inference-Part1-Decision-Theorem/"},{"title":"Statistical Inference-Notes-Part2-Bayesian Method","text":"Keywords: Posterior distribution, Bayesian decision problems, James-Stein estimator, Stein’s Lemma 1. A review of Bayesian riskThe Bayesian risk of decision rule $d$ can be written as: $r(\\pi, d) = \\int_{\\Theta} R(\\theta, d) \\pi(\\theta) d\\theta = \\int_{\\Theta} \\int_{\\mathcal{X}} L(\\theta, d(x)) f(x; \\theta) dx d\\theta$ $\\quad = \\int_{\\Theta} \\int_{\\mathcal{X}} L(\\theta, d(x)) f(x; \\theta) dx \\pi(\\theta)d\\theta = \\int_{\\Theta} \\int_{\\mathcal{X}} L(\\theta, d(x)) f(x; \\theta) \\pi(\\theta) dx d\\theta$ $\\quad = \\int_{\\mathcal{x}} f(x) \\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta | x) d\\theta dx$ Note that if $\\theta$ is deterministic, $f(x; \\theta) = f(x |\\theta)$ by the definition of conditional probability (Indicator function for conditional expectation). Since $f(x)$ is the same for every decision rule, it suffices to minimize the posterior loss, i.e. for each $x$ we choose $d(x)$ to minimize $\\int_{\\Theta} L(\\theta, d(x)) \\pi(\\theta | x) d\\theta$. Some well-known decision rule is actually based on the action set and loss function. For example, for the mean-square loss function, the decision should be the mean of the posterior distribution. However, for an indicator form loss function, the optimal decision will be the MAP (in this framework, someone call the highest of posterior distribution HPD) estimator. 2. From Bayes rule to Minimax rulein the part1 of these notes, we proved that if Bayes rule (or $\\epsilon$ extended Bayes rule) is an equalizer decision rule, then this decision rule is minimax. (Equalizer is the sufficient condition of minimax principle). Example 1: Find a minimax estimator of $\\theta$ based on a single observation $X \\sim \\text{Bin}(n, \\theta)$ with n known, under squared error loss $L(\\theta, d) = (\\theta - d)^2$ Solution: Although the space of prior distribution is all admissible, let’s select the conjugate distribution as the prior distribution. The conjugate distribution of Binomial distribution is Beta distribution (for multinomial distribution, the conjugate distribution is Dirichlet distribution). We can use the conclusion that the Bayesian estimator is the mean of the posterior distribution given squared error loss. The Bayesian risk can be written as $\\int_{\\theta} (\\theta - d(X))^2 \\pi(\\theta | X) d\\theta$. Let $d_1$ denote the $d(x) = \\int_{\\theta} \\theta \\pi(\\theta | X) d\\theta$, and $d_2$ denote any other decision rule which $\\neq d_1$. $r(\\pi, d_1) - r(\\pi, d_2) = \\mathbb{E}_{X|\\theta}[(\\bar{\\theta} - \\bar{\\theta})^2] - \\mathbb{E}_{X|\\theta}[(\\bar{\\theta} - d_2)^2] = - \\mathbb{E}_{X|\\theta}[(\\bar{\\theta} - d_2)^2] \\leq 0$. Therefore, we can never find a better decision rule apart from the mean of the $\\theta$ given mean-error loss function. Given the conjugate prior, we can conclude that $\\bar{\\theta} = \\frac{a + \\sum x_i}{a + b + n}$ ($x_i = 0$ or $1$). We want to find a prior which make the decision rules an equalizer decision. Let $C$ denote $a + b + n$ The risk function can be written as (recall the definition of the equalizer decision rule): $\\mathbb{E}[(\\frac{a + X}{c} - \\theta)^2] = \\frac{1}{c^2}\\mathbb{E}[(X + a - c\\theta)^2]$ $\\quad = \\frac{1}{c^2}(n\\theta(1-\\theta) + n^2 \\theta^2 + 2n\\theta (a - c\\theta) + (a - c\\theta)^2$ It is obvious that it is of the quadratic form of $\\theta$. We need to select proper $a$ and $b$ to make the coefficient of $\\theta$ and $\\theta^2$ be zero. Therefore, the result can be written as $d(x) = \\frac{\\sum x_i + \\sqrt{n}/2}{n + \\sqrt{n}/2}$ Example 2: A review of tank problem. The tanks are identified by the ID range from 1 to an unknown number N. We observe a tank whose ID is k. What is the minimax estimation of N? The loss function is $(N - d(k))^2$. With the same logic, the Bayesian estimator should be $\\hat{d(k)} = \\sum_{N=1}^{+\\infty} N \\text{Pr} (N |k)$. We need to find a good enough prior distribution to make sure the expectation converge. The risk function can be written $\\mathbb{E}[(N - d(x))^2|N] = N^2 - 2N \\mathbb{E}(d(x)|N) + \\mathbb{E}(d(x)^2|N)$. Let’s assume the number of tanks is uniformly distributed over $[k, \\Omega]$, we want to find the proper $\\Omega$ to make the decision rule minimax. Therefore, the decision rule can be written as $d(k) = \\sum_{N=k}^{\\Omega} N\\frac{\\text{Pr}(k|N) \\text{Pr}(N)}{\\text{Pr}(k)}$. Note that $\\text{Pr}(k) = \\sum_{N=k}^{\\Omega} \\text{Pr}(k|N) \\text{Pr}(N) = \\sum_{N=k}^{\\Omega} \\frac{1}{N} \\frac{1}{\\Omega - k}$. Therefore $d(k) = \\frac{\\Omega - k}{\\sum_{N=k}^{\\Omega} \\frac{1}{N}}$. The risk function can be written as: $N^2 - \\sum_{k=1}^N \\frac{2(\\Omega - k)}{\\sum_{i=k}^\\Omega \\frac{1}{i}} + \\sum_{k=1}^N \\frac{(\\Omega - k)^2}{(\\sum_{i=k}^\\Omega \\frac{1}{i})^2N}$, which is almost impossible to solve the $\\Omega$ analytically. Therefore, let’s consider a numerical methodology. We want to find the $\\Omega$ that maximize the Bayesian risk. The classic method is the use the frequentist’s view to construct the unbiased sufficient estimator. We will further discuss this problem in the following notes.","link":"/2021/01/10/Statistical-inference-Part2-Bayesian-Method/"},{"title":"Projection Theorem in the Hilbert space","text":"Keywords: Closed Subspace, Orthogonal Complement, The Projection Theorem 1. DefinitionsClosed Subspace A linear subspace $\\mathcal{M}$ of a Hilbert space $\\mathcal{H}$ is said to be a closed subspace of $\\mathcal{H}$ if $\\mathcal{M}$ contains all of its limit points (i.e. if $x_n \\in \\mathcal{M}$ and $|x_n - x| \\xrightarrow{} 0$ imply that $x \\in \\mathcal{M}$). Orthogonal Complement: The orthogonal complement of a subset $\\mathcal{M}$ of $\\mathcal{H}$ is defined to be the set $\\mathcal{M}^{\\perp}$ of all elements of $\\mathcal{H}$ which are orthogonal to every element of $\\mathcal{M}$. Thus $x \\in M^{\\perp}$ if and only if $(x, y) = 0$ for all $y \\in \\mathcal{M}$ Proposition: If $\\mathcal{M}$ is any subset of a Hilbert space $\\mathcal{H}$, then its orthogonal complement is a closed subspace of $\\mathcal{H}$. Proof: For any Cauchy sequence $x_n \\in \\mathcal{M}^{\\perp}$, $(x_n, y) = 0 \\forall y \\in \\mathcal{M}$. According to the continuity of inner product (triangle inequality), if $|x_n - x| \\xrightarrow{} 0$, we can conclude that $(x, y) = 0$. Therefore, the $\\mathcal{H}^{\\perp}$ must be complete. QED. Lemma Parallelogram law: If $\\mathcal{\\Hilbert}$ is an inner-product space, then $|x + y|^2 + |x - y|^2 = 2 |x|^2 + 2 |y|^2$ (显然) Projection Theorem : If $\\mathcal{M}$ is a closed subspace of the Hilbert space $\\mathcal{H}$ and $x \\in \\mathcal{H}$, then (1) There is a unique element $\\hat{x} \\in \\mathcal{M}$ such that $|x - \\hat{x}| = \\inf\\limits_{y \\in \\mathcal{M}} |x - y|$ (2) $\\hat{x} \\in \\mathcal{M}$ and $|x - \\hat{x}| = \\inf\\limits_{y \\in \\mathcal{M}} |x - y|$ if and only if $\\hat{x} \\in \\mathcal{M}$ and $(x - \\hat{x}) \\in \\mathcal{M}^{\\perp}$ Proof: (1) By the definition of closed subspace, there exist a Cauchy sequence $y_n \\in \\mathcal{M}, |y_n| \\xrightarrow{} y$, such that $|y_n - x| \\xrightarrow{} \\inf\\limits_{y \\in \\mathcal{M}} |y - x|$. Therefore, by Cauchy criterion there exist $\\hat{x} \\in \\mathcal{H}$, such that $|x - \\hat{x}|^2 = \\lim\\limits_{n \\xrightarrow{} \\infty} |x - y_n|^2 = d$. Existence proved. To prove the uniqueness, we need to prove that for any $\\hat{y} \\in \\mathcal{M}$ and that $|x - \\hat{y}| = |x - \\hat{x}| = d$, $|\\hat{x} -\\hat{y}| = 0$. Apply the parallelogram law: $$ $0 \\le|\\hat{x} - y|^2 = -4|(\\hat{x} + \\hat{y})/2 - x|^2 + 2(|\\hat{x} - x|^2 + |\\hat{y} - x|^2) \\le -4d + 4d = 0$ Remark: 三角不等式一下会把上面的式子的右边放得太大，在证明唯一性的时候一定要优先使用严格等号。 Hence $\\hat{y} \\overset{a.s.}{=} \\hat{x}$ (2) step 1: $\\hat{x} \\in M$ and $(x - \\hat{x}) \\in \\mathcal{M}^{\\perp} \\xrightarrow{} |x - \\hat{x}| = \\inf |x-y|$: (易证) step 2: $\\hat{x} \\in M$ and $(x - \\hat{x}) \\in \\mathcal{M}^{\\perp} \\xleftarrow{} |x - \\hat{x}| = \\inf |x-y|$: Prove it by contradiction. (是构造性证明，技巧上类似Cauchy-Schwarz) Remark: 映射的存在性和唯一性都是基于Closed subspace上的性质成立的。 2. Projection mappingIf $\\mathcal{M}$ is a closed subspace of the Hilbert space $\\mathcal{H}$ and $I$ is the identity mapping on $\\mathcal{H}$, then there is a unique mapping $P_{\\mathcal{M}}$ of $\\mathcal{H}$ onto $\\mathcal{M}$ such that $I - P_{\\mathcal{M}}$ maps $\\mathcal{H}$ on to $\\mathcal{M}^{\\perp}$. Proof: By projection theorem, for each $x \\in \\mathcal{H}$ there is a unique $\\hat{x} \\in \\mathcal{M}$ such that $x-\\hat{x} \\in \\mathcal{M}^{\\perp}$. The required mapping is therefore $P_{\\mathcal{M}}x = \\hat{x}$ Remark: 这里的映射的定义来源于距离该元素最近的点是唯一的。映射定理也可以因此表述为，一定存在一个映射，使得Hilbert空间中任何的元素都能够被映射到某个Closed subspace中，并且$I - P_{\\mathcal{M}}$可以把这个元素映射到Closed subspace的正交补空间上。 Properties (1) $P_{\\mathcal{M}}(\\alpha x + \\beta y) = \\alpha P_{\\mathcal{M}}(x) + \\beta P_{\\mathcal{M}}(y)$ (2) $|x|^2 = |P_{\\mathcal{M}}x|^2 + |(I - P_{\\mathcal{M}})x|^2$ (3) $x = P_{\\mathcal{M}}x + (I - P_{\\mathcal{M}})x$ (4) If $|x_n - x| \\xrightarrow{} 0$, $P_{\\mathcal{M}}x_n \\xrightarrow{} P_{\\mathcal{M}}x$ (5) $x \\in \\mathcal{M}$ iff $P_{\\mathcal{M}} x=x $ (6) (5) $x \\in \\mathcal{M}^{\\perp}$ iff $P_{\\mathcal{M} x=0 $ Remark: 在内积空间的连续性中，$|x_n - x| \\xrightarrow{} 0$可以推出$|x_n| \\xrightarrow |x|$, 反之不成立，除非$x_n$可以被$x$线性表示。 Prediction Equation: Given a Hilbert space $\\mathcal{H}$, a closed subspace $\\mathcal{M}$, and an element $x \\in \\mathcal{H}$. The element of $\\mathcal{M}$ closest to x, is the $\\hat{x}$ such that $(x - \\hat{x}, y) = 0 \\forall y \\in \\mathcal{M}$ Remark: Note that $P_{\\mathcal{M}}(P_{\\mathcal{M}}x) = P_{\\mathcal{M}}x$. Therefore, the projection mapping must satisfy that $P_{\\mathcal{M}}^n = P_{\\mathcal{M}}$. Recall 线性回归中的一些投影矩阵，实际就是$\\mathbb{R}^n$空间中的$P_{\\mathcal{M}}$, 而$\\mathcal{M}$ 是有observation 构成的closed span.","link":"/2021/01/23/Projection-Theorem/"},{"title":"Statistical Inference-Notes-Part3-Hypothesis Testing","text":"Keywords: Neyman-Pearson Theorem, monotone likelihood ratio (MLR), LRT, Bayes factors, Frequentist VS Bayesian 1. FormulationFor a parameter space $\\Theta$, and consider the hypothesis of the form: $H_0: \\theta \\in \\Theta_0$ vs. $H_1: \\theta \\in \\Theta_1$ where $\\Theta_0$ and $\\Theta_1$ are two adjoint subsets of $\\Theta$. Note that if $\\Theta_0 = \\{\\theta_0\\}$, then we call this simple hypothesis; otherwise composite hypothesis. (Watch out for nuisance parameters). Note that when we are testing hypothesis based on the decision theorem, we have to calculate the risk function given a specific $\\theta$. Since $\\theta$ can at most in one of the hypothesis set, it is asymmetric between those two hypothesis. The usual way is to construct a test statistic $t(X)$ and a critical region $C_\\alpha$, then reject $H_0$ based on $X = x$ if and only if $t(x) \\in C_\\alpha$. The critical region must be chosen to satisfy $\\text{Pr}_{\\theta}\\{t(X) \\in C_\\alpha\\}$. The critical region must be chosen to satisfy: $\\text{Pr}_{\\theta}\\{t(X) \\in C_{\\alpha}\\} \\le \\alpha $ for all $\\theta \\in \\Theta_0$. 2. Power of testingWe need to consider if a test function $\\phi$ is better then others. This criterion is call power of test. The definition of power function can be written as: $w(\\theta) = \\text{Pr}\\{Reject H_0\\} = \\mathbb{E}_{\\theta} \\{\\phi(X)\\}$ In most situation, the power of a test is the value of power function on alternative hypothesis, i.e. $w(\\theta_1)$. When we are considering a simple test, the power can be rephrased as the probability to reject the null hypothesis when the alternative hypothesis is true. The idea of formulating a good test is to make $w(\\theta)$ as large as possible on $\\Theta_1$, while satisfying the constraint $w(\\theta) \\le \\alpha$ for all $\\theta \\in \\Theta_0$. (在保证原假设被拒绝的情况下，让critical region在备择假设下得measure尽可能得大) In general, we will consider the following three scenarios. 1) Simple $H_0$ and simple $H_1$: use Neyman-Pearson Theorem to construct the best test. 2) Simple $H_0$ and composite $H_1$: use a representative $\\theta$ in $\\Theta_1$, and apply Neyman-Pearson theorem. This is called UMP (uniformly most powerful) test. 3) Composite $H_0$ and composite $H_1$: Harder. Define the test function (loss function in Hypothesis test): $\\phi(x) = 1 \\quad \\text{if}\\quad t(x) \\in C_\\alpha$ otherwise $0$. Comments: The definition of power shows the logic of minimax. The setting is favorable to null hypothesis. If we can reject the null hypothesis in the worst situation, then we should be more confident to our decision. Therefore, people always choose the hypothesis they are more interested in to be the alternative hypothesis. In conclusion, finding the most powerful testing function can be written as: $\\max w(\\theta_1)$ $s.t. w(\\theta_0) = \\alpha $ 2.1 Likelihood ratio test in simple testConsider simple null hypothesis and simple alternative hypothesis. Define the likelihood ratio $\\Lambda(x)$ by: $\\Lambda(x) = \\frac{f(x; \\theta_1)}{f(x;\\theta_2)}$ , where $f$ is the probability density function or probability mass function. According to NP theorem, the best test of size $\\alpha$ is of the form: reject $H_0$ when $\\Lambda(X) > k_\\alpha$. The randomized test with test function $\\phi_0$ is said to be a likelihood ratio test if it is of the form: $\\phi_0(x) = 1$ if $f_1(x) > Kf_0(x)$, $\\gamma(x)$ if $f_1(x) = Kf_0(x)$, $0$ if $f_1(x) < Kf_0(x)$. $0< \\gamma(x) < 1$ and $K \\ge 0$ Note: Test function is the loss function and decision rule in decision theory, while power is the risk function in the decision theory 2.2 Neyman-Pearson TheoremGiven the LRT define in 2.1, we can conclude that: (a) (Optimality). For any K and $\\gamma(x)$, the test $\\phi_0$ has maximum power among all tests whose size are no greater than the size of $\\phi_0$. (b) (Existence). Given $\\alpha \\in (0, 1)$, there exist constants $K$ and $\\gamma_0$ st the LRT defined by this K and $\\gamma(x) = \\gamma_0$ for all x has size exactly $\\alpha$. (c) (Uniqueness). If the test $\\phi$ has size $\\alpha$, and is maximum power amongst all possible tests of size $\\alpha$, then $\\phi$ is necessarily a likelihood ratio test, except possibly on a set of values of x which has probability 0 under $H_0$ and $H_1$. (给定$\\alpha$和最强检验，则这个test function几乎处处是likelihood ratio test.) Technical Condition: Probability density function or probability mass function of alternative hypothesis should Absolute continuous wrt the null hypothesis. The proof follows common tricks used on decision theorem. Proof: (a) Let $\\phi$ denote any test for $\\mathbb{E}_{\\theta_0} \\phi(X) \\le \\mathbb{E}_{\\theta_0} \\phi_0(X)$ (any test function which has smaller size of critical region). Define $U(x) = [\\phi_0(x) - \\phi(x)][f_1(x) - kf_0(x)]$. Note that the state space of test function is 0 or 1 but with different domain. When $f_1(x) - Kf_0(x) > 0$, $\\phi_0(x) = 1$, $U(x) \\ge 0$. We also prove that $U(x) \\ge 0$ when $f_1(x) - kf_0(x) < 0$. Therefore, $0 \\le \\int [\\phi_0 - \\phi][f_1(x) - K f_0(x)] dx = \\mathbb{E}_{\\theta_1} \\phi_0(X) - \\mathbb{E}_{\\theta_1} \\phi(X) + K(\\mathbb{E}_{\\theta_0} \\phi(X) - \\mathbb{E}_{\\theta_0} \\phi_0(X))$ $\\therefore \\int [\\phi_0 - \\phi][f_1(x) - K f_0(x)] dx = \\mathbb{E}_{\\theta_1} \\phi_0(X) - \\mathbb{E}_{\\theta_1} \\phi(X) \\ge 0$ The difference of power between two test function is always larger then K times the difference of size. (b) Can be easily proved based on the fact that the cdf is right continuous, non-decreasing, and monotonous. (c) Prove by contradiction. （读者自证真的不难） 2.3 Uniform most powerful (UMP) test and MLRJust the same as the continuous and uniformly continuous, the UMP test is a global version of the most powerful test. A uniformly most powerful or UMP test of size $\\alpha$ is a test $\\phi_0(\\cdot)$ for which (i) $\\mathbb{E} \\phi_0(X) \\le \\alpha$ for all $\\theta \\in \\Theta_0$ (ii) given any other test $\\phi(\\cdot)$ for which $\\mathbb{E}_{\\theta} \\phi(X) \\le \\alpha$ for all $\\theta \\in \\Theta_0$, we have $\\mathbb{E}_{\\theta} \\phi_0(X) \\ge \\mathbb{E}_{\\theta} \\phi (X)$. For some family of parameters, the LRT is UMP in one-side testing problem. However, we can never assume LRT is UMP in the composite hypothesis. Such families are said to have monotone likelihood ratio or MLR. Definition: The family of densities $\\{f(x; \\theta), \\theta \\in \\Theta \\in \\mathbb{R}\\}$ with real scalar parameter $\\theta$ is said to be of monotone likelihood ratio if there exist a function t(x) such that the likelihood ratio $\\frac{f(x; \\theta_1)}{x; \\theta_0}$ is a non-decreasing function of $t(x)$ whenever $\\theta_0 \\le \\theta_1$ Examples of MLR: $f(x; \\theta) = c(\\theta)h(x)e^{\\theta \\tau(x)}$ or the density distribution is irrelevant to $x$. Theorem: Suppose $X$ has a distribution from a family of MLR with respect to a statistic $t(X)$, and that we wish to test $H_0 : \\theta \\le \\theta_0$ against $H_1: \\theta > \\theta_0$. Suppose the distribution function of $t(X)$ is continuous. (a) The test $\\phi(x) = 1 $ if $t(x) > t_0$ or 0 otherwise is UMP among all test of size $\\le \\mathbb{E}_{\\theta_0}[\\phi_0(X)]$ (b) Given some $\\alpha$, where $0","link":"/2021/01/11/Statistical-inference-Part3-Hypothesis-Testing/"},{"title":"Statistical Inference-Notes-Part4-Lehmann Scheffe Theorem","text":"Keywords: Lehmann-Scheffe Theorem, Sufficient statistics, Minimal Sufficient statistics, Complete statistics, Lehmann Scheffe Theorem, Rao-Blackwell Theorem 1. A review of Sufficient statistics1.1 The definitions of Sufficient statisticsLemma: Let $t(x)$ denote some function of x. Then the following are equivalent: (1) There exist functions $h(x)$ and $g(t; \\theta)$ such that $f(x; \\theta) = h(x) g(t(x); \\theta)$ (2) If $t(x) = t(x^\\prime)$, for the likelihood ratio defined $lr(x) = \\frac{f(x; \\theta_1)}{f(x;\\theta_2)}$, we can conclude that $lr(x) = lr(x^\\prime)$ Proof: (1) $\\xrightarrow{}$ (2) is obvious. (2) $\\xrightarrow{}$ (1): For some parameter $\\theta_0 \\in \\Theta$, we can conclude that $f(x; \\theta_1) = f(x; \\theta_0) g^{*}(t(x), \\theta_1, \\theta_0)$. Let $h(x) = f(x; \\theta_0)$ and $g(t(x), \\theta_1) = g^{*}(t(x), \\theta_1, \\theta_0)$, which is exactly of the from of (1). Definition: The statistic $T = T(x)$ is sufficient for $\\theta$ is the distribution of $X$, conditional on $T(X) = t$, is independent of $\\theta$. The two criteria for sufficiency are: (a) Factorization Theorem: $t(X)$ is sufficient for $\\theta$ if and only if Lemma.1 holds. (b) Likelihood ratio Criterion: $T(x)$ is sufficient for $\\theta$ if and only if Lemma.2 holds. Note that the sufficient statistics may have different dimension of $\\theta$. 1.2 The minimal sufficient statisticsWe want to find a ‘minimal’ sufficient statistics that include all the sufficient statistics, i.e. there exist a function mapping from sufficient statistics to the minimal sufficient statistics. Lemma: If T and S are minimal sufficient statistics, then there exist injective functions $g_1$ and $g_2$ such that $T = g_1(S)$ and $S = g_2(T)$. The injective function means for any $y_1, y_2 \\in Y$, for the function $X \\xrightarrow{} Y$, if $y_1 = y_2$, then $x_1 = x_2$. Proof: The definition of minimal sufficient statistics means that there exist functions $g_1$ and $g_2$, such that $T = g_1(S)$, $S = g_2(T)$. Consider $x_1, x_2$ s.t. $T(x_1) = g_1(S(x_1)) = g_2(S(x_2)) = T(x_2)$, therefore $S(x_1) = g_1(T(x_1)) = g_1(T(x_2)) = S(x_2)$. Theorem: A necessary and sufficient condition for a statistic $T(X)$ to be minimal sufficient is that $T(x) = T(x^\\prime)$ if and only if $\\Lambda_x(\\theta_1, \\theta_2) = \\Lambda_{x^\\prime}(\\theta_1, \\theta_2)$ for all $\\theta_1$ and $\\theta_2$. This theorem shows that $x = x^{\\prime}$ iff they define the same likelihood ratio for any $\\theta_1, \\theta_2 \\in Theta$. Note that the conclusion only holds in a finite Euclidean space. 2. CompletenessA sufficient statistic $T(X)$ is complete if for any real measurable function $g$, $\\mathbb{E}_{\\theta}[g(T)] = 0$ for all $\\theta$. implies $\\textbf{Pr}(g(T) = 0) = 1$ for all $\\theta$. 3. Lehmann-Scheffe TheoremSuppose $X$ has density $f(x; \\theta)$ and $T(X)$ is sufficient and complete for $\\theta$. Then $T$ is minimal sufficient. Suppose $T(X)$ is a sufficient statistic for $\\theta$, and let $f_{T}(t; \\theta)$ be a complete family. If $\\varphi$: $\\mathbb{E}[\\varphi(Y)] = \\theta$ then $\\varphi(Y)$ is the unique MVUE of $\\theta$. (The optimality can be easily proved by Rao-Blackwell. The uniqueness is determined by the complete family. ) （对带估计参数无偏的充分完全统计量（充分完全统计量是最小充分统计量）是无偏估计中MSE最小的估计量；完全性之影响是否唯一性） 4. Convex loss function and Rao-Blackwell TheoremSuppose we want to estimate a real-valued parameter $\\theta$ with an estimator $d(X)$. Suppose the loss function is a convex function of $d$ for each $\\theta$. Let $d_1(X)$ be an unbiased estimator for $\\theta$ and suppose $T$ is a sufficient statistic. Then the estimator $\\mathcal{X}(T) = \\mathbb{E}\\{d_1(X)|T\\}$ (充分性保证了这里还是充分统计量的函数， T的结构不会被破坏) is also unbiased and is at least as good as $d_1$. Proof: $\\mathcal{X}(T)$ is unbiased from the tower law. $\\mathbb{E}(\\mathcal{X}(T)) = \\mathbb{E}\\{d_1(X)|T\\} = \\mathbb{E}_{\\theta} d_1(X) = \\theta$. For the risk function, we have $R(\\theta, d_1) = \\mathbb{E}_{\\theta}\\{L(\\theta, d_1(X))\\} = \\mathbb{E}_{\\theta}\\{\\mathbb{E}[L(\\theta, d_1(X))|T]\\} \\ge \\mathbb{E}_{\\theta}[L(\\theta, \\mathcal{X}(T))] = R(\\theta, \\mathcal{X}(T))$ （注意倒数第二步使用了Jenson 不等式和充分统计量的定义/性质） ReferenceEssentials of Statistics Inference","link":"/2021/01/13/Statistical-inference-Part4-Lehmann-Scheffe/"},{"title":"Statistical Inference-Notes-Part5-Likelihood method","text":"Keywords: Score function, Fisher Information, Cramer-Rao Lower Bound, asymptotic properties of likelihood estimator 1. Score function and Fisher information matrixThe score function is defined as the first order derivatives of the log likelihood function. Note that the log likelihood function or score function themselves are random variables. Let $u(x; \\theta) = \\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta}$ $\\mathbb{E}(u(x;\\theta)) = \\int_{\\mathcal{X}} \\frac{1}{f(x; \\theta)} f(x; \\theta) \\frac{\\partial f(x; \\theta)}{\\partial \\theta} dx = \\frac{\\partial}{\\partial \\theta}\\int_{\\mathcal{X}} f(x; \\theta) dx = \\frac{\\partial}{\\partial \\theta} 1 = 0$ $\\frac{\\partial}{\\partial \\theta^T}\\mathbb{E}(\\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta}) = \\mathbb{E}(\\frac{\\partial l(x; \\theta)^2}{\\partial \\theta \\partial \\theta^T}) + \\mathbb{E}(\\frac{\\partial l(x; \\theta)}{\\partial \\theta} \\frac{\\partial l(x; \\theta)}{\\partial \\theta^T}) = 0$ Therefore, $\\mathbb{E}(\\frac{\\partial l(x; \\theta)^2}{\\partial \\theta \\partial \\theta^T}) =- \\mathbb{E}(\\frac{\\partial l(x; \\theta)}{\\partial \\theta} \\frac{\\partial l(x; \\theta)}{\\partial \\theta^T}) = -var(u(x; \\theta))$ Note that $-\\frac{\\partial l(x; \\theta)}{\\partial \\theta} \\frac{\\partial l(x; \\theta)}{\\partial \\theta^T}$ is called observed information matrix(denoted by $e_{i}$ for the ith observation in n iid rv)), and its expectation is called Fisher information matrix (denoted by $i_{n}$ for n iid rv). If the observations are independent, it will be obvious that $i_n(\\theta) = n i_1(\\theta)$ 2. The Cramer-Rao Lower BoundLet $W(X)$ be any estimator of $\\theta$ and let $m(\\theta) = \\mathbb{E}_{\\theta}\\{W(x)\\}$. Let $Y = W(X)$ and $Z = \\frac{\\partial}{\\partial \\theta} \\log f(X; \\theta)$. Based on Cauchy inequality, we can conclude that: $\\text{var}(Y)\\text{var}{Z} \\ge \\{\\text{cov}(Y, Z)\\}^2$ Note that $\\text{cov}(Y, Z) = \\int w(x) \\big\\{ \\frac{\\partial}{\\partial \\theta} \\log f(x; \\theta)\\big\\} f(x; \\theta)dx = m^{\\prime}(\\theta)$ （微分积分换序的一个充分条件是含参积分对参数一致收敛，更严格的条件有被积函数在参数上的偏导数连续；另外这里的cov里没有减去均值，但是这里的均值可以被 $\\bar{W}(X)\\nabla_{\\theta}\\int_{\\mathcal{X}}f(x; \\theta)dx = 0$消除。这里还有另外一个有趣的事情是 $\\int_{\\mathcal{X}} (W(X) - \\bar{W}(X)) \\frac{\\partial f(X; \\theta)}{\\partial \\theta} dx \\neq \\frac{\\partial}{\\partial \\theta}\\int_{\\mathcal{X}} (W(X) - \\bar{W}(X)) f(X; \\theta) dx$。这里体现了规范书写的重要性，偷懒省略了参数$\\theta$时就要想到会有求导忘了它的一天。–再论善恶终有报。） Note that $\\text{var}(Z) = \\mathbb{E}\\Big\\{\\frac{\\log f(X; \\theta)}{\\partial \\theta} \\frac{\\log f(X; \\theta)}{\\partial \\theta^T}\\Big\\}$ Therefore $\\text{var}\\{W(X)\\} \\ge \\frac{\\{m^{\\prime}(\\theta)\\}^2}{i(\\theta)}$. If we have an unbiased estimator, we can further conclude that $m(\\theta) = \\theta, m^{\\prime}(\\theta) = 1$. For any unbiased estimator which achieves the lower bound can be seen to be a MVUE, a minimum variance unbiased estimator. Let‘s further consider the condition that the equality rather than the inequality holds. Note that $\\text{cov(Y, Z)} = \\text{var}(Y)\\text{var}(Z)$ iff $\\text{corr}(Y, Z) = 1, -1$, which means $Y$ must be proportional to $Z$. Thus $\\frac{\\partial}{\\partial \\theta} \\log f(X;\\theta) = a(\\theta) W(X) - b(\\theta)$, and $\\log f(X;\\theta) = A(\\theta)W(X) + B(\\theta) + C(X)$, which is of the form of exponential family. (指数分布族性质太好了，当然Gaussian 尤其好， 对于指数分布族来说，我们只需要找到一个$\\frac{m^{\\prime}(\\theta)^2}{i(\\theta)}$)最小的就能天然获得MUE. ） For multi-dimension parameter space, the CRB can be written as: $cov_{\\theta}(W(X)) \\ge \\frac{\\partial m(\\theta)}{\\partial \\theta} [I(\\theta)]^{-1}\\frac{\\partial m(\\theta)}{\\partial \\theta}^T$. , where $\\frac{\\partial m(\\theta)}{\\partial \\theta}$ is the Jacobian matrix, and $A \\ge B$ means $A-B$ is semi-definite. 3. Asymptotic properties of maximum likelihood estimators3.1 A review of asymptotic theoremsThe strong law of large number (SLLN) says the sequence of random variables $Y_n = n^{-1} (X_1 + X_2 + \\dots + X_n)$ converges almost surely to $\\mu$ iff $\\mathbb{E} (|X_i|)$ is finite. The weak law of larger number (WLLN) says that $Y_n = n^{-1} (X_1 + X_2 + \\dots + X_n)$ converge to $\\mu$ with probability if $X_i$ have finite estimation $Y_n \\xrightarrow{p} \\mu$. Note that we the technical condition for WLLN and SLLN are the same, i.e. finite estimation (the finite variance is not necessary). CLT says that under the condition that $X_i$ are of finite variance $\\sigma^2$, $Z_n = \\frac{\\sqrt{n}(Y_n - \\mu)}{\\sigma}$, converges in distribution to a random variable $Z$ having the standard normal distribution $N(0, 1)$($Z_n \\xrightarrow{d} N(0, 1)$). (SLLN和WLLN的要求是不同的，一些更加复杂的SLLN需要用Borel-Cantelli构造序列；CLT是最弱的收敛（recall：特征函数泰勒展开忽略高阶项），而且分布函数依分布收敛到正态分布，不一定能保证密度函数也以分布收敛到正态。) Slutsky’s Theorem: if $Y_n \\xrightarrow{d} Y$ and $Z_n \\xrightarrow{p} c$(WLLN, let alone SLLN), where finite constant $c$. If $g$ is a continuous function (recall the $\\epsilon-\\delta$ language for continuous, it is obvious since we can always control the difference in state space by control the domain) , $g(Y_n, Z_n) \\xrightarrow{d} g(Y, c))$. For example, $Y_n + Z_n \\xrightarrow{d} Y + c$, $Y_n Z_n \\xrightarrow{d} cY$ , $Y_n/Z_n \\xrightarrow{d} Y/c$ 3.2 Consistency of MLE estimator$\\hat{\\theta}_n \\xrightarrow{p} \\theta$ is called weak consistent; $\\hat{\\theta}_n \\xrightarrow{a.s.} \\theta$ is called strong consistent. Suppose $f(x; \\theta)$ is a family of probability densities or probability mass functions and let $\\theta_0$ denote the true value of the parameter $\\theta$. For any $\\theta \\neq \\theta_0$ we have by Jenson’s inequality: $\\mathbb{E}_{\\theta_0}\\{\\log \\frac{f(X; \\theta)}{f(X; \\theta_0}\\} \\le \\log\\mathbb{E}_{\\theta_0}\\{ \\frac{f(X; \\theta)}{f(X; \\theta_0}\\} = \\log \\int_{\\mathcal{X}} f(x; \\theta) dx = 0$. Note that the inequality is strict unless $\\frac{f(X; \\theta)}{f(X; \\theta_0)} = 1$ Let $\\mu_1 = \\mathbb{E}_{\\theta_0} \\{\\log \\frac{f(X; \\theta_0- \\delta)}{f(X; \\theta_0)}\\} \\le 0$, $\\mu_2 = \\mathbb{E}_{\\theta_0} \\{\\log \\frac{f(X; \\theta_0+ \\delta)}{f(X; \\theta_0)}\\} \\le 0$ Note that $\\frac{l_n(\\theta_0)}{n} = \\frac{l(x_1; \\theta_0) + \\dots + l(x_n; \\theta_0) }{n}$ if $\\mathbb{E}_{\\theta_0}(|l(x; \\theta_0)|) < +\\infty$, $\\frac{l_n(\\theta_0)}{n} \\xrightarrow{a.s.} \\mathbb{E}_{\\theta_0}(\\log f(X; \\theta_0))$. lemma 3.2.1 $\\mathbb{E}_{\\theta_0}(|\\log f(x; \\theta_0)|)$ is finite. Proof: $\\mathbb{E}_{\\theta_0}(|\\log f(x; \\theta_0)|) = \\mathbb{E}_{\\theta_0}(|\\log f(x; \\theta_0)|\\Big |f(x; \\theta_0) \\ge 1) \\textbf{Pr}\\{f(x; \\theta_0) \\ge 1\\}+ \\mathbb{E}_{\\theta_0}(|\\log f(x; \\theta_0)|\\Big |0 \\le f(x; \\theta_0) < 1)\\textbf{Pr}\\{0 \\le f(x; \\theta_0) < 1\\} \\le \\mathbb{E}_{\\theta_0}(|\\log f(x; \\theta_0)|\\Big |f(x; \\theta_0) \\ge 1) + \\mathbb{E}_{\\theta_0}(|\\log f(x; \\theta_0)|\\Big |0 \\le f(x; \\theta_0) < 1)$ Let’s further consider $-\\int_{0 \\le f(x; \\theta) < 1} \\log (f(x; \\theta)) f(x; \\theta) dx \\le \\int_{0 \\le f(x; \\theta) < 1} \\log (e) e dx = e \\textbf{Pr}(0 \\le f(x; \\theta) < 1) \\le e$ Therefore $\\mathbb{E}_{\\theta_0}(|\\log f(x; \\theta_0)|) \\le 2e$, and we can apply SLLN (let alone WLLN). Since $\\mu_1 = \\mathbb{E}_{\\theta_0} \\{\\log \\frac{f(X; \\theta_0- \\delta)}{f(X; \\theta_0)}\\} \\le 0$, according to Slutsky’s theorem, we can conclude that $\\frac{i_n(x; \\theta_0 - \\delta) - i_n(x; \\theta)}{n} \\xrightarrow{a.s.} \\mu_1 < 0$. By the same logic, we can conclude that $\\frac{i_n(x; \\theta_0 + \\delta) - i_n(x; \\theta)}{n} \\xrightarrow{a.s.} \\mu_2 < 0$ We can control the $\\mu_1, \\mu_2 \\xrightarrow{} 0$, by controlling $\\delta \\xrightarrow{} 0$ with $n \\xrightarrow{} \\infty$. Note that it is a general result. Not necessarily need differentiable over $\\theta$ (but need continuous i.e. not necessarily absolute continuous). Conclusion: SLLN and continuity guarantees the strongly asymptotic consistency of likelihood estimator. （这个强行构造零测集的办法真是暴力美学…）. Although we cannot make any assumptions about the uniqueness of the likelihood estimation, it is unique on any sufficiently small neighborhood. 3.3 The asymptotic distribution of the maximum likelihood estimatorThe strong consistency can be guaranteed without any differentiation assumptions on log-likelihood function on $\\theta$. However, if we want to figure out the asymptotic properties of distribution, we need assumptions that the log-likelihood function is twice continuously differentiable. Let’s further assume there is a solution for the $l^{\\prime}_n(\\theta) = 0$. According to the Taylor expansion with Lagrange remainder, we can conclude that: $-l^{\\prime}_n(\\theta_0) = l^{\\prime}_n(\\hat{\\theta}_n) - l^{\\prime}_n(\\hat{\\theta}_0) = (\\hat{\\theta}_n - \\theta_0) l^{\\prime\\prime}_n(\\theta_n^*)$ Let’s first consider a scalar scenario. $\\sqrt{n i_1(\\theta_0)}(\\hat{\\theta}_n - \\theta_0) = \\frac{l_n^\\prime(\\theta_0)}{\\sqrt{n i_1(\\theta_0)}} \\cdot \\frac{l^{\\prime\\prime}_n(\\theta_0)}{l^{\\prime\\prime}_n(\\theta_n^*)} \\cdot \\Big\\{-\\frac{l^{\\prime\\prime}_n(\\theta_0)}{n i_1(\\theta_0)} \\Big\\}^{-1}$. lemma 3.3.1: $\\frac{l_n^\\prime(\\theta_0)}{\\sqrt{n i_1(\\theta_0)}} \\xrightarrow{d} N(0, 1)$ Let’s consider the random variable $l_n^\\prime(\\theta_0)$. We have known that $\\mathbb{E}_{\\theta_0}(l_1^\\prime(\\theta_0)) = 0$ and $\\mathbb{E}_{\\theta_0}(l_1^\\prime(\\theta_0) l_1^\\prime(\\theta_0)) = \\mathbb{E}_{\\theta_0}(-\\frac{\\partial l_1(\\theta_0^2)}{\\partial \\theta^2}) = i_1(\\theta_0)$. Therefore, according to $CLT$, $\\frac{\\sum l_i^{\\prime}(\\theta_0)}{\\sqrt{n i_1(\\theta_0)}} \\sim N(0, 1)$. lemma 3.3.2: $\\frac{l^{\\prime\\prime}_n(\\theta_0)}{l^{\\prime\\prime}_n(\\theta_n^*)} \\xrightarrow{p} 1$ $\\frac{l^{\\prime\\prime}_n(\\theta_0)}{l^{\\prime\\prime}_n(\\theta_n^*)} - 1 = \\frac{l^{\\prime\\prime}_n(\\theta_0) - l^{\\prime\\prime}_n(\\theta_n^*)}{n} \\cdot \\Big \\{\\frac{l^{\\prime\\prime}_n(\\theta_n^*)}{n}\\Big\\}^{-1}$ Note that $l^{\\prime\\prime}_i(\\theta_n^*)$ is also a random variable, and it follows the SLLN. Therefore $\\Big \\{\\frac{l^{\\prime\\prime}_n(\\theta_n^*)}{n}\\Big\\}^{-1} \\xrightarrow{a.s} -\\frac{1}{i_1(\\theta_0)}$ Further note that $|\\frac{l^{\\prime\\prime}_n(\\theta_0) - l^{\\prime\\prime}_n(\\theta_n^*)}{n}| \\le |\\theta_n^* - \\theta_0| \\frac{\\sum g(X_i)}{n}$, where $|\\frac{\\partial^3 \\log f(x; \\theta)}{\\partial \\theta^3}| \\le g(x)$, i.e. we need further assume $|\\frac{\\partial^3 \\log f(x; \\theta)}{\\partial \\theta^3}|$ is uniformly bounded. Since we know the strong consistency, $|\\theta_n^{*} - \\theta_0| \\xrightarrow{a.s.} 0$. Therefore, $\\frac{l^{\\prime\\prime}_n(\\theta_0)}{l^{\\prime\\prime}_n(\\theta_n^*)} \\xrightarrow{p} 1$ lemma 3.3.3: $\\Big\\{-\\frac{l^{\\prime\\prime}_n(\\theta_0)}{n i_1(\\theta_0)} \\Big\\}^{-1} \\xrightarrow{p} 1$ (自证不难) Remark：基于score function良好的性质，以及CLT 和 Slutsky’s Lemma. 在做asymptotic的分析时应该多从score function上找。 ReferenceEssentials of Statistics Inference","link":"/2021/01/14/Statistical-inference-Part5-Likelihood/"},{"title":"Spectral representation of stationary stochastic process","text":"Keywords: Complex-valued ACF, Spectral Distribution, Herglotz’s Theorem, Isomorphism, Orthogonal-increment process 1. Definitions and PropertiesComplex-valued stationary process: The process $\\{X_n\\}$ is a complex values stationary process if $\\mathbb{E}|X_t|^2 < \\infty$, $\\mathbb{E}X_t$ is independent of $t$ and $\\mathbb{E}(X_{t+h}\\bar{X}_t)$ is independent of $t$. Note that in the complex vector space, the inner product is defined as $(x, y) = \\mathbb{E}(X\\bar{Y})$, and the autocovariance function(ACF) $\\gamma(\\cdot)$ of a complex-valued stationary process $\\{X_t\\}$ is $\\gamma(h) = (X_{t+h}, X_t) - (X_{t + h}, 1)(1, X_t)$, i.e. $\\gamma(h) = \\mathbb{E}(X_{t + h}\\bar{X_t}) - \\mathbb{E}(X_{t + h})\\mathbb{E}(\\bar{X_t})$ (复数域一定要小心顺序和复共轭的问题) The properties of complex-valued ACF includes: (1) $\\gamma(0) \\ge 0$ (2) $|\\gamma(h)| \\le \\gamma(0)$ (Complex-valued Cauchy-Schwarz) (3) $\\gamma(\\cdot)$ is a Hermitian function, i.e. $\\gamma(h) = \\bar{\\gamma(-h)}$ Theorem 1: The relationship between ACF and Stochastic process A function $K(\\cdot)$ defined on the integers is the autocovariance function of a stationary time series if and only if $K(\\cdot)$ is Hermitian and non-negative definite, i.e. if and only if $K(n) = \\bar{K(-n)}$ and $\\sum\\limits_{i, j}^n a_i K(i - j) \\bar{a_j} \\ge 0$ for any integer $n$ and all vectors $a = (a_1, \\dots, a_n)^\\prime \\in \\mathbb{C}^n$ (还是用构造法设计一个符合Kolmogorov‘s Theorem的离散时间随机过程) 2. Fourier SeriesConsider the complex Hilbert space $L^2[-\\pi, \\pi] = L^2([-\\pi, \\pi], \\mathcal{B}, U)$, where $\\mathcal{B}$ is the Borel set of $[-\\pi, \\pi]$, $U$ is the uniform probability measure $U(dx) = d(U(x)) = \\frac{1}{2\\pi} dx$. The inner product in the complex space is defined as usual by $(f, g) = Ef\\bar{g} = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} f(x)\\bar{g}(x)dx$. Consider $\\{e_n(x) = e^{inx}\\}, n \\in \\mathbb{Z}$. Note that $(e_n(x), e_m(x)) = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} e^{i(m - n)x}dx = 0$, therefore $e_i(x), i \\in \\mathbb{Z}$ is the orthonormal set of this complex $L^2[-\\pi, \\pi]$. Fourier approximation: The nth order Fourier approximation to any function $f \\in L^2[-\\pi, \\pi]$ is defined to be the projection of f onto $\\bar{sp}(e_j)$, with the same logic of Parseval’s identity, we can write the nth order of Fourier approximation as: $S_n f = \\sum_{i = -n}^{i = n} (f, e_i) e_i$. The Fourier coefficients can be written as $(f, e_j) = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} f(x) e^{-ijx}dx$ Theorem $Sf = f$: (a) The sequence $\\{S_n f\\}$ has a mean square limit as $n \\xrightarrow{} \\infty$. Let $Sf$ denote $\\lim\\limits_{n \\xrightarrow{} \\infty}S_nf = \\lim\\limits_{n \\xrightarrow{} \\infty} \\sum_{|j| < n} (f, e^{ijt}) e^{ijt}$. (b) $Sf = f$. Proof: (a) From Bessel inequality, we have $\\sum_{|j| < n}|(f, e_j)|^2 \\le |f|^2$. (Note that $(f, f) = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi}f(x)\\bar{f}(x)dx$). Note that we are working in the $L^2$ space. Therefore, we have a converge Cauchy sequence. (b) For $|j| < n$, $(S_n f, e_j) = (f, e_j)$ (by the definition of orthonormal set), $(Sf, e_j) = \\lim\\limits_{n \\xrightarrow{} \\infty} (S_nf, e_j) = (f, e_j)$ ($Sf \\xrightarrow{} f$ and continuity of inner-product space) 3. The spectral distribution of a linear combination of sinusoidsLet’s consider a specific stochastic process in this section. Let’s assume $X_t$ can be written as: $X_t = \\sum_{j = 1}^n A(\\lambda_j)e^{it\\lambda_j}$, where $-\\pi < \\lambda_1 < \\lambda_2 < \\dots < \\lambda_n < \\pi$, and $A(\\lambda_i)$ are uncorrelated complex-valued random coefficients (may follow Dirac distribution) such that $\\mathbb{E}(A(\\lambda_j)) = 0$, $j \\in \\mathbb{Z}$ and $\\mathbb{E}(A(\\lambda_j)\\bar{A(\\lambda_j)}) = \\sigma_j^2$ Note that the autocorrelation function can be written via a distribution function, i.e., $\\gamma(h) = \\int_{(-\\pi, \\pi]} e^{ihv} d F(v)$, where $F(\\lambda) = \\sum\\limits_{j:\\lambda_j < \\lambda} \\sigma^2_j$ (Spectral distribution function) The remarkable feature of this example is that every zero-mean stationary process has a representation which is a natural generalization $X_t = \\int_{(-\\pi, \\pi]} e^{itv}dZ(v)$, which is a stochastic integral w.r.t. an orthogonal-increment process. The ACF of a stationary process can be written as $\\gamma_X(h) = \\int_{(-\\pi, \\pi]} e^{ihv} dF(v)$, where $F(\\cdot)$ is a distribution function with $F(-\\pi) = 0$ and $F(\\pi) = \\mathbb{E}(X_t^2)$ Herglotz’s Theorem: A complex-valued function $\\gamma(\\cdot)$ defined on the integers is non-negative definite *if and only if * $\\gamma(h) = \\int_{(-\\pi,\\pi]} e^{ihv} d F(v)$, where $F(\\cdot)$ is a right-continuous, non-decreasing, bounded function on $[-\\pi, \\pi]$ and $F(-\\pi) = 0$. (Function F is called the spectral distribution function of $\\gamma$ and if $F(\\lambda) = \\int_{-\\pi}^\\lambda f(v) dv, -\\pi < \\lambda < \\pi$, then f is called a spectral density function.) 结合Herglotz’s Theorem和Theorem 1， 我们得到了谱概率分布和时间序列之间的关系。 Inverse Transform: If $K(\\cdot)$ is any complex-valued function on the integers such that $\\sum_{-\\infty}^{\\infty}|K(n)| < \\infty$ Then $K(h) = \\int_{-\\pi}^{\\pi} e^{ihv} f(v) dv$, where $f(v) = \\sum_{-\\infty}^{\\infty} e^{-in\\lambda}K(n)$ Proof: Use Dirac function Corollary: An absolutely summable complex-valued function $\\gamma(\\cdot)$ defined on the integers is the autocovariance function of a stationary process iff $f(\\lambda) = \\frac{1}{2\\pi} \\sum_{-\\infty}^{\\infty} e^{-in\\lambda}\\gamma(n)$, where $f(\\lambda)$ is the spectral density of the $\\gamma(\\cdot)$, i.e. $\\gamma(h) = \\int_{-\\pi}^{\\pi}e^{ihv}f(v)dv$ 4. Corollaries for stationary time seriesTheorem 4.1If $\\{Y_t\\}$ is any zero-mean, possibly complex-valued stationary process with spectral distribution function $F_{Y}(\\cdot)$, and $\\{X_t\\}$ is the process $$X_t = \\sum_{j = -\\infty}^{+\\infty} \\varphi_{j}Y_{t - j}$$, where $\\sum_{j = -\\infty}^{+\\infty} |\\varphi_{j}| < + \\infty$ (绝对可积老条件了，一个充分条件是指数衰减，保证某个N之后的和有界即可) then $\\{X_t\\}$ is stationary with spectral distribution function $F_{X}(\\lambda) = \\int_{(-\\pi, \\lambda]} |\\sum_{j = -\\infty}^{\\infty} \\varphi_j e^{-ijv}|^2 dF_y(v)$ Proof: $\\mathbb{E}(X_{t + h} \\bar{X}_{t}) = \\mathbb{E}(\\sum_{j = -\\infty}^{j = \\infty}\\varphi_{t + h + j}Y_{t + h + j} \\sum_{k = -\\infty}^{k = \\infty}\\bar{\\varphi}_{t + k}\\bar{Y}_{t + k}) = \\sum_{j, k}^{\\infty}\\varphi_{t + h + j}\\bar{\\varphi}_{t + k} \\mathbb{E}(Y_{t + h + j}\\bar{Y}_{t + k}) = \\sum_{j, k}^{\\infty}\\varphi_{t + h + j}\\bar{\\varphi}_{t + k} \\mathbb{E}(Y_{t + h + j}\\bar{Y}_{t + k}) = \\sum_{j, k}^{\\infty}\\varphi_{t + h + j}\\bar{\\varphi}_{t + k} \\gamma_Y(h + j - k) = \\gamma_X(h)$ $\\gamma_X(h) = \\sum_{j, k}^{\\infty}\\varphi_{t + h + j}\\bar{\\varphi}_{t + k} \\gamma_Y(h + j - k) = \\sum_{j, k}^{\\infty}\\varphi_{t + h + j}\\bar{\\varphi}_{t + k} \\int_{(-\\pi, \\pi]} e^{i(h + j - k)v}dF_Y(v) = \\int_{(-\\pi, \\pi]} |\\sum_{j = -\\infty}^{\\infty} \\varphi_j e^{-ijv}|^2 e^{ihv}dF_Y(v) = \\int_{(-\\pi, \\pi]} e^{ihv}dF_X(v)$ Theorem 4.2 Spectral Density of an ARMA(p, q) ProcessLet $\\{X_t\\}$ be an ARMA(p, q) process (not necessarily causal and invertible) satisfying $\\phi(B) X_ t= \\theta(B) Z_t$, $Z_t \\sim WN(0, \\sigma^2)$. $\\phi(\\cdot)$ has no zeroes on the unit circle. Then $\\{X_t\\}$ has spectral density $f_{X}(\\lambda) = \\frac{\\sigma^2 |\\theta(e^{-i\\lambda})|^2}{2\\pi |\\phi(e^{-i\\lambda})|^2}$ Proof: $U(t) = \\phi(B)X_t = \\theta(B)Z_t$ (滞后算子值得借鉴). According to Theorem 4.1, we can conclude that: $f_U(\\lambda) = |\\phi(e^{-iv})|^2 f_X(\\lambda) = |\\theta(e^{-iv})|^2 f_Z(\\lambda)$ $f_X(\\lambda) = \\frac{|\\theta(e^{-iv})|^2}{|\\phi(e^{-iv})|^2}f_Z(\\lambda)$ Note that $Z_t$ is white noise, the autocovariance function is $\\gamma_Z(h) = \\sigma^2\\delta_0(h)$ According to the last corollary in the last section, we can conclude that $f_z(\\lambda) = \\frac{1}{2\\pi} \\sum_{n = -\\infty}^{+\\infty} \\gamma_z(h) e^{-in\\lambda} = \\frac{\\sigma^2}{2\\pi}$ $\\therefore f_X(\\lambda) = \\frac{|\\theta(e^{-iv})|^2}{|\\phi(e^{-iv})|^2}\\frac{\\sigma^2}{2\\pi}$ 5. Spectral representation from a isomorphism perspectiveLet $\\{X_t\\}$ be a zero mean stationary process with spectral distribution function $F$, i.e. $\\mathbb{E}[X_{t + h}\\bar{X_t}] = \\int_{(-\\pi, \\pi]}e^{ihv}dF(v)$ (according to Herglotz’s theorem, such spectral distribution function $F$ is a sufficient and necessary condition for a non-negative definite function). We need to find a isomorphism between the subspace $\\mathcal{H} = sp\\{X_t, t \\in \\mathbb{Z}\\} \\subset L^2(\\Omega, \\mathcal{F}, P)$ and $\\mathcal{K} = sp\\{e^{it\\cdot}, t\\in \\mathbb{Z}\\} \\subset L^2([-\\pi, \\pi], \\mathcal{B}, F)$, which connected the random variables in the time domain and the functions in the frequency domain. Note that the mapping $T(\\sum_{j = 1}^n a_j X_{tj}) = \\sum_{j = 1}^n a_je^{it\\cdot}$ defines an isomorphism between $\\mathcal{H}$ and $\\mathcal{K}$. This mapping T has the property that: $(T(\\sum_{j = 1}^n a_j X_{tj}), T(\\sum_{k = 1}^m b_k X_{sk})) = (\\sum_{j = 1}^n a_j e^{it_j \\cdot}, \\sum_{k = 1}^m b_k e^{is_k \\cdot})_{L^2(F)} = \\sum_{i, j = 1}^{n, m} a_j \\bar{b}_k(e^{it_j \\cdot}, e^{is_k \\cdot})_{L^2(F)} = \\sum_{i, j = 1}^{n, m} a_j \\bar{b}_k \\int_{(-\\pi, \\pi]}e^{iv(t_j - s_k)}dF(v) = \\sum_{i, j = 1}^{n, m} a_j \\bar{b}_k \\mathbb{E}[X_{t_j}\\bar{X}_{s_k}] = \\sum_{i, j = 1}^{n, m} a_j \\bar{b}_k (X_{t_j}, X_{s_k}) = (\\sum_{j = 1}^n a_j X_{tj}, \\sum_{k = 1}^m b_k X_{sk})$ If $(\\sum_{j = 1}^n a_j X_{tj}, \\sum_{k = 1}^m b_k X_{sk}) \\xrightarrow{} 0$, $(T(\\sum_{j = 1}^n a_j X_{tj}), T(\\sum_{k = 1}^m b_k X_{sk}))$ also converge to zero. Therefore, this isomorphism is well-defined. We can further extend the mapping to a complete inner-product space. Theorem 5.1: If $F$ is the spectral distribution function of the stationary process $\\{X_t, t \\in \\mathbb{Z}\\}$, then there is a unique isomorphism T of $\\bar{sp}\\{X_t, t\\in \\mathbb{Z}\\}$ onto $L^2(F)$ such that $T(X_t) = e^{it\\cdot}$ (证明略，这里的unique大概又是范数为零，a.s.搞过来的) Comment: T是一个把随机变量映射(概率空间映射到状态空间的函数)到deterministic function的映射. 我们希望基于T找到一个正交增量过程，用它来表示平稳随机过程$X_t$. Theorem 5.2 (The spectral Representation Theorem) If $\\{X_t\\}$ is a stationary sequence with mean-zero and spectral distribution function $F$, then there exists a right-continuous orthogonal-increment process $\\{Z(\\lambda), -\\pi < \\lambda < \\pi\\}$ such that: (i) $\\mathbb{E}|Z(\\lambda) - Z(-\\pi)|^2 = F(\\lambda)$ (Recall: $F(\\lambda) = \\frac{1}{2\\pi} \\sum_{h = -\\infty}^{\\infty} e^{-ih\\lambda} \\gamma(h)$, where $\\gamma(\\cdot)$ is the autocovariance function) (ii) $X_t = \\int_{(-\\pi, \\pi]} e^{itv} dZ(v)$ Comment: 谱分解定理表明，我们总是可以找到一个定义在某概率空间上的正交增量过程，使得由改正交增量过程构成的最小闭子空间与$L^2([-\\pi, \\pi], \\mathcal{B}, F)$同构。($\\{Z_{\\lambda}\\}$)同样是定义在 Proof: Lemma 5.2: If T is defined as in Theorem 5.1, then the process $\\{Z(\\lambda), -\\pi \\le \\lambda \\le \\pi\\}$ defined by $Z(\\lambda) = T^{-1}(I_{(-\\pi, \\lambda]}(\\cdot))$ ($I$ is the identification function), $-\\pi\\le \\lambda \\le \\pi$ is an orthogonal increment process. Moreover the distribution function associated with $\\{Z(\\lambda)\\}$ is exactly the spectral distribution function $F$ of $\\{X_t\\}$. Proof of Lemma 5.2: Since $Z(\\lambda) \\in \\bar{sp}\\{X_t, t\\in \\mathbb{Z}\\}$ there is a sequence $\\{Y_n\\}$ of elements of $\\bar{sp}\\{X_t, t\\in \\mathbb{Z}\\}$ (Note: $\\bar{\\cdot}$ means closed subspace) such that $|Y_n - Z(\\lambda)| \\xrightarrow{} 0$ as $n \\xrightarrow{} \\infty$. By the continuity of inner-product, we can conclude that $(Z(\\lambda), 1) = \\lim\\limits_{n \\xrightarrow{} \\infty}(Y_n, 1) = 0$ since each $\\{Y_n\\}$ has zero mean. Finally, if $-\\pi \\le \\lambda_1 \\le \\lambda_2 \\le \\lambda_3 \\le \\lambda_4 \\le \\pi$, $(Z(\\lambda_4) - Z(\\lambda_3), Z(\\lambda_2) - Z(\\lambda_1))_{L^2(\\Omega, \\mathcal{F}, P)} = (TZ(\\lambda_4) - TZ(\\lambda_3), TZ(\\lambda_2) - Z(\\lambda_1))_{L^2(F)} = (I_{(\\lambda_3, \\lambda_4]}(\\cdot), I_{(\\lambda_1, \\lambda_2]}(\\cdot))_{L^2(F)} = 0$. Therefore, $Z(\\lambda)$ is an orthogonal increment process. Moreover, if we take $(Z(\\mu) - Z(\\lambda), Z(\\mu) - Z(\\lambda))_{L^2(\\Omega, \\mathcal{F}, P)} = (TZ(\\mu) - TZ(\\lambda), TZ(\\mu) - Z(\\lambda))_{L^2(F)} = \\int_{(-\\pi, \\pi]}I_{(\\lambda, \\mu]}(v)I_{(\\lambda, \\mu]}(v)dF(v) = F(\\mu) - F(\\lambda)$ $\\therefore Q.E.D$ Let $\\{Z_t\\}$ be the process defined in Lemma 5.2. and let $I$ (not the identification function we used in Lemma 5.1) be the isomorphism $I(f) = \\int_{(-\\pi, \\pi]} f(v)dZ(v)$. In the blog ‘Orthogonal-increment process’, we know if $\\mathcal{D} = L^2(F)$ onto $I(\\mathcal{D}) \\subset L^2(\\Omega, \\mathcal{F}, P)$. For a Borel measurable function $f \\in \\mathcal{D}$, we can represent f as: $I(f) = \\sum_{i = 0}^n f_{i}(Z(\\lambda_{i + 1}) - Z(\\lambda_{i})) = \\sum_{i = 0}^n f_{i}T^{-1}I_{(\\lambda_{i}, \\lambda_{i + 1}]} = T^{-1}(f)$. Therefore, we can conclude that both stochastic integral operator $I(\\cdot)$ and $T^{-1}$ are isomorphisms between $L^2(F)$ and $L^2(\\Omega, \\mathcal{F}, P)$. Theorem 5.2 Q.E.D. Comment: 谱表示实际上是一个构造性的证明。这一整套来源于两个Hilbert空间上的两个同构。其中$\\mathcal{K} = L^2(F)$是一个辅助空间, 对于概率空间$\\mathcal{H} = L^2(\\Omega, \\mathcal{F}, P)$ $T: \\mathcal{H} \\xrightarrow{} \\mathcal{K}$, 和$I: \\mathcal{K} \\xrightarrow{} \\mathcal{H}$ 是两个同构，而且恰好 $TI(f) = f \\quad \\forall f \\in \\mathcal{K}$ Corollary: If $\\{X_t\\}$ is a zero-mean stationary sequence then there exists a right continuous orthogonal increment process $\\{Z(\\lambda), -\\pi \\le \\lambda \\le \\pi\\}$ such that $Z(-\\pi) = 0$ and $X_t = \\int_{-\\pi, \\pi} e^{itv}dZ(v)$ with probability one. If $\\{Y(\\lambda)\\}$ and $\\{Z(\\lambda)\\}$ are two such processes then $P(Y(\\lambda) = Z(\\lambda)) = 1$ We can easily prove the above statement with the help of $L^2(F)$. The spectral presentation does not reveal how to construct $\\{Z(\\lambda)\\}$. We need the inversed transformation to solve the $\\{Z(\\lambda)\\}$ 6. Inversion FormulaeWith the same logic, we want to find a random variable which equals $Z(\\lambda)$ almost surely (mean square convergence). Recall that $T(Z(w) - Z(v)) = I_{(v, w]}(\\cdot)$ and $TX_t = e^{it\\cdot}$. Therefore, if we can find $\\sum_{|j| < n} \\alpha_j e^{ij\\cdot}\\xrightarrow{L^2(F)} I_{(v, w]}(\\cdot)$, then by the isomorphism, $\\sum_{|j| \\le n} \\alpha_j X_j \\xrightarrow{m.s.} Z(w) - Z(v)$. Let $h_n(\\lambda) = \\sum_{|j| < n} \\alpha_je^{ij\\lambda} $ denote the nth-order Fourier series approximation to $\\frac{1}{2\\pi}\\int_{(-\\pi, \\pi]}I_{(v, w]}(\\lambda) e^{-ij\\lambda} d \\lambda = \\frac{1}{2\\pi}\\int_{(v, w]}e^{-ij\\lambda} d \\lambda$ Theorem: If $\\{X_n\\}$ is a stationary sequence with autocovariance function $\\gamma(\\cdot)$, spectral distribution function F, and spectral representation $X_t = \\int_{(-\\pi, \\pi]} e^{itv}dZ(v)$, and if v and w ($-\\pi < v < w < \\pi$) are continuity points of F, then as $n \\xrightarrow{} \\infty$, $\\frac{1}{2\\pi}\\sum_{|j| \\le n} X_j (\\int_v^w e^{-ijv}dv) \\xrightarrow{m.s.} Z(w) - Z(\\lambda)$ (Note it is the Fourier transformation of the indicator function, 易错) $\\frac{1}{2\\pi}\\sum_{|j| \\le n} \\gamma(j) (\\int_v^w e^{-ijv}dv) \\xrightarrow{} F(w) - F(\\lambda)$ 7. Prediction in Frequency DomainThe stochastic integral $I$ defined on the associated orthogonal increment process $\\{Z(\\lambda), -\\pi < \\lambda < \\pi\\}$ is an isomorphism of $L^2(F)$ on to the probability space, with the property that $I(e^{it\\cdot}) = X_t$ Therefore, we can find the best prediction on the $L^2(F)$. For example, the best linear predictor $P_{\\mathcal{M_n}}X_{n + h}$ of $X_{n + h}$ in $\\mathcal{M_n} = \\bar{sp}\\{X_t, -\\infty \\le t \\le n\\}$ can be written as: $P_{\\mathcal{M}_n}X_{n + h} = I(P_{\\bar{sp}\\{\\exp(it\\cdot)\\}}e^{i(n + h)\\cdot})$ ReferenceTime Series: Theory and Methods","link":"/2021/01/28/Spectral-representation-of-stationary-stochastic-process/"},{"title":"Stochastic Approximation Methods-Notes-Part 1-An Overview and Robbins Monro Algorithm","text":"Generally, stochastic approximation methods are a family of iterative methods. The goal of these algorithms is to recover some properties of a function depending on random variables. The application of stochastic approximation ranges from deep learning (e.g., SGD ) to online learning methods. 1. An introduction to Stochastic Approximation MethodsIn this section, I will show you some examples of stochastic approximation methods. 1.1 Example 1: ExpectationFor a given probability space $(\\Omega, \\mathcal{F}, P)$, we want to solve the expectation of $E^P(f(\\omega))$, where $f$ is a measurable function, and $\\omega \\in \\Omega$. A reasonable solution is to estimate the expectation via arithmetic average. The following equations provide us an iterative method to this problem. Let $\\mathbb{E}_n$ be the estimation of $\\mathbb{E}^P(f(\\omega))$ given $n$ observation. $n \\times E_n = \\sum_{i = 1}^n f(\\omega_i)$ $ (n+1) \\times E_{n+1} = \\sum_{i = 1}^{n+1} f(\\omega_i) = n \\times E_n + f(\\omega_{n+1}) $ $ \\implies (n + 1) \\times E{n+1} = (n+1) \\times E_n + f(\\omega_{n+1}) - E_n $ $ \\implies E_{n+1} = E_n + \\frac{1}{n+1} [f(\\omega_{n+1}) - E_n] $ 1.2 Example 2: SGDIn the framework of stochastic gradient descent (SGD), or on-line gradient descent, the true gradient $\\nabla Q(\\omega)$ is approximated by a gradient at a single sample. Let $\\omega_n$ be the estimation of true gradient in the $nth$ iteration, then $\\omega_{n+1} = \\omega_n - \\lambda \\nabla Q(\\omega)$ where $\\lambda$ is the learning rate. There are several popular extensions of the SGD method, including Momentum, Nesterov Momentum, and Adam. 1.3 Example 3: Q-Learning in reinforcement learningIn the field of reinforcement learning, function $Q: S \\times A \\xrightarrow{} \\mathcal{R}$ combines the reward with states and actions. At each time $t$, the agent select an action $a_t$, get an reward $r_t$ from the environment, and enter into the next state $s_{t+1}$. The discussions of convergence of RL cannot avoid the topics on the Q function or the value function. $$Q^{new}(s_t, a_t) = (1 - \\alpha)Q^{old}(s_t, a_t) + \\alpha (r_t + \\gamma \\max \\limits_a Q^{old}(s_{t+1}, a))$$ where $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor. 2. Robbins-Monro AlgorithmIn 1951, Robbins and Monro developed a methodology for solving a root finding problem given a function depends (or partially depends) on a random variable. Root finding problem is the generalization of several important topics, including optimizations and extrema finding. 2.1 Root finding problem without uncertaintyLet $M(x)$ be a given function and $\\alpha$ a given constant such that the equation $M(x) = a$ has a unique root $x = \\theta$. For any iterative method for this root finding problem, we find one or more initial values $\\ x_{initial}$, and then successively obtain new values as certain functions of the previous obtained $x$. The most famous algorithm is the Newton–Raphson method. 2.2 A stochastic generalization of the root finding problemLet’s consider how to solve this problem when $M(x)$ is unknown for some reason. Let’s assume the randomness of the function is captured by the probability space $(\\Omega, \\mathcal{F}, P)$, and there is a random variable $Y$ on $(\\Omega, \\mathcal{F}, P)$ which takes values in a state space $(S, \\mathcal{L})$. In this blog, we assume the state space is a d-dimensional Euclidean space equipped with the $\\sigma$-field of Borel sets. In this algorithm, $M(x)$ is a deterministic function which defined as follows: $M(x) = \\int_{y \\in S }ydH(y|x)$ where $H(y|x) = \\mathbb{P}[Y(\\omega) \\le y|x]$ ($\\omega \\in \\Omega$) is the distribution function of $Y$ given an unobservable deterministic variable $x$. In other word, $M(x)$ is the expectation of $Y$ for the given $x$. Although we know nothing about the nature of function $M(x)$ and the distribution function $H(y|x)$, but we assume (which mean this assumption will work as a known condition in the proof) that the equation $M(x) = a$ has a unique solution $x = \\theta$. Bobbins-Monro algorithm introduced a method to estimate $\\theta$ by making successive observations on $Y$ at given $x_1, x_2, …$, which are determined sequentially in accordance with some definite experimental procedure. The idea of this algorithm is to construct a transition probability between two states. Moreover, with this sequence, the difference between $\\lim \\limits_{k \\xrightarrow{} \\infty}x_k$ and $\\theta$ can be controlled. We can conclude this idea as finding a sequence $\\{x_k\\}$, such that $\\lim \\limits_{k \\xrightarrow{} \\infty}E(x_k - \\theta)^2 = 0$ This idea also implies the convergence in probability (or in $\\mathcal{L}^2$) of $x_n$ to $\\theta$. 2.3 The proof of convergence theoremThe computational origin of this algorithm is the first example (expectation estimation) in the Introduction section. If we review the expectation estimation problem in the framework of Robbins-Monro, then in this example, $x$ denotes the expectation and function $M(x) = \\mathbb{E}^P[Y - x]$, and we want to solve the equation $M(x) = 0$. I only present brief proof to show the idea. You may read this paper for more details. We want to prove that: Condition 1: For every $x$, a distribution function in $y$, and there exist a positive constant C such that $\\mathbb{P}[|Y(\\omega)| \\le C|x] = \\int_{-c}^cdH(y|x) = 1$ Condition 2: There exist finite constants $\\alpha, \\theta$ such that $M(x) \\le \\alpha$ for $x \\lt \\theta$, and $M(x) \\ge \\alpha$ for $x \\gt \\theta$ Condition 3: Let $\\{a_n\\}$ be a fixed sequence of positive constants such that $0","link":"/2020/01/14/Stochastic-Approximation-Methods-Notes-1/"},{"title":"Stochastic Control-Notes-Part1-Ito operator and its L2-adjoint","text":"Key words: Ito operator, Property of trace, Fokker-Planck equation, L2 adjoint, Hilbert space. 1. Ito operatorLet $X$ be the solution of the following SDE. $d X_t = b(X_t)dt + \\sigma(X_t)dW_t$ where $X_t \\in \\mathbb{R}^d$, and $W$ is an n-dimensional standard Brownian motion ($\\sigma$ is a $d \\times n$). Define $\\mathcal{L}$ as the infinitesimal generator of X on a $C^2$-function $f$: $\\mathcal{L} f(X_{t}) = f_{x}(X_{t})^{T} d X_{t} + \\frac{1}{2} dX_t^{T}f_{xx}(X_t) dX_t$ Note that the second term $\\in \\mathbb{R}$. Therefore, we can further rewrite the formula like $\\mathcal{L} f(X_{t}) = f_{x}(X_{t})^{T} d X_{t} + \\frac{1}{2} tr\\Big(dX_t^{T}f_{xx}(X_t) dX_t\\Big) $ $\\quad \\quad \\quad= f_{x}(X_aw{t})^{T} d X_{t} + \\frac{1}{2} tr\\Big(dX_tdX_t^{T}f_{xx}(X_t)\\Big) $ $\\quad \\quad \\quad= f_{x}(X_{t})^{T} d X_{t} + \\frac{1}{2} tr\\Big(\\sigma(X_t)^Tf_{xx}(X_t)\\sigma(X_t)\\Big) dt$ Note that $tr(ABC) = tr(BCA) = tr(CAB)$ (However, $tr(ABC)$ may not equal to $tr(ACB)$). Therefore, L can be written as: $L = \\sum\\limits_{i=1}^d b_i(t, x) \\partial_{i} + \\sum\\limits_{i, j= 1}^d a_{i, j}(t, x)\\partial_{i, j}$ , where $a_{i, j}$ is the $i$th row and $j$th columns of the matrix $\\sigma(X_t)^T\\sigma(X_t)$ (the transpose of $\\sigma(X_t)\\sigma(X_t)^T$). (Recall $tr(A^TB) = \\sum_{i, j}a_{i, j}b_{i, j}$) 2 Fokker-Planck equation (Kolmogorov forward equation)It is an intentional practice for the derivation of L2 adjoint. Theorem:Let H and K be Hilbert spaces, and let $A: H \\xrightarrow{} K$ be a bounded, linear map. Then there exist a unique bounded linear map $A^*: K \\xrightarrow{} H$ such that: $\\forall x \\in H, \\quad \\forall y \\in K, \\quad \\langle Ax, y \\rangle = \\langle x, A^*y \\rangle$ By the definition of linear operator, Ito operator is a linear operator. By the definition of Ito integral, we can ensure that it is bounded. Assume $X_t$ has a density distribution $p(t, \\cdot)$. Therefore, there exist a unique L2 adjoint such that: $\\frac{d}{dt} \\mathbb{E}\\Big(f(X_t)\\Big) = \\frac{d}{dt} \\int_{x} f(x) p(t, x)dx = \\int_{x} Lf(x) p(t, x) dx = \\int_{x}f(x)L^{*}p(t, x) dx$ By definition, $L^{*}p(t, x)$ should be the Fokker-Planck equation. $\\int_{x} Lf(x) p(t, x) dx = \\int_{x} \\Big(f_{x}^Tb(x, t) + \\frac{1}{2}tr(\\sigma^T f_{xx} \\sigma)\\Big) p(t, x) dx$ $\\int_{x} f_{x}^Tb(x, t) p(t, x)dx = \\int_{x} \\sum_{i}\\Big(p(t, x)b(t, x_i)\\frac{\\partial}{\\partial x_i}f(x, t)\\Big) dx$ Note that $ \\int_{x} p(t, x)b(t, x_i)\\frac{\\partial}{\\partial x_i}f(x) dx = \\int_{x} p(t, x)b_i(t, x) d f dx_{-i}$ $ = \\int_{x} p(t, x)b_i(t, x)f(x) |_{x_i = -\\infty}^{x_i= + \\infty} - f(x)d\\Big(p(t, x)b_i(t, x)\\Big) d x_{-i}$ $ = -\\int_{x}f(x)d\\Big(p(t, x)b_i(t, x)\\Big) d x_{-i} = -\\int_{x}f(x)p(t, x)\\frac{\\partial b_{i}(t, x)}{\\partial x_i} d x -\\int_{x}f(x)b_{i}(t, x)\\frac{\\partial p(t, x)}{\\partial x_i} d x$ Therefore, $\\int_{x} f_{x}^Tb(x, t) p(t, x)dx = -\\int_{x} f(x) \\sum\\limits_{i}\\frac{\\partial}{\\partial x_i} \\Big(p(t, x) b_{i}(t, x)\\Big) dx$ Similarly, the second term can be written as: $\\int_{x} \\frac{1}{2}tr(\\sigma^T f_{xx} \\sigma) p(t, x) dx = \\int_{x}\\frac{1}{2} \\sum_{i, j}^d \\frac{\\partial^2}{\\partial x_i\\partial x_j} a_{i, j}p(x, t) dx$.","link":"/2020/10/16/Stochastic%20Control-Notes-1/"},{"title":"Stochastic Approximation Methods-Notes-Part2-Flat Minima and Gradient Descent","text":"The parameterized function with similar training error widely diverge in the generalization performance. However, the flat minima may imply a low-complexity neural network structure. Some SGD methods have shown can converge to a flatter minima, which potentially make the solution of nonconvex optimization more robust. The first part of this note is a review of Flat minima( Hochreiter and Schmidhuber, 1997). The second part contains an introduction to Gradient Descent algorithms’ properties and visualization. 1. DefinitionsA flat minimum is a large connected region in weight space where the error remains approximately constant. It can be rigorously defined with the tolerance error and training error. 1.1 The General TaskWe want to approximate the an unknown function f: $\\mathbb{R}^n \\rightarrow \\mathbb{R}$(Note that in the original paper, it is $\\mathbb{R}^n \\rightarrow \\mathbb{R}^k$, and there is no difference if we consider multi-dimensional target space). More specifically, function f mapping a finite set of possible input $X \\subset \\mathbb{R}^n$ to a finite possible output $Y \\subset \\mathbb{R}^k$. This function is assumed perfectly describe the real data generation process. The training information is given by a finite set $D_0 \\subset D$. We call $D_0$ the training data. Let tuple $(x_p, y_p)$ denotes the pth element in the data set. Let $net(w)$ denotes a realized neural network structure parameterized by $w$. Note that $w_ij$ represent for the weight on the connection from unit $i$to unit $j$. 1.2 Training Error (Empirical risk)The squared error of a realized structure over a given training set is defined as: $E(net(w), D_0) = \\sum_{p}||y_p-f(w,x_p)||^2$ where $||\\cdot||$ is the Euclidean norm. 1.3 Tolerance ErrorNote that we want to define a region in weight space where each parameter in this region has small empirical risk and similar output. The so called structure with ‘small’ risk denotes those of lower empirical risk than a given constant $\\mathbf{E}_{tol}$. 1.4 Boxes and Flat minimaWe want to study the property of a large region of connected acceptable minima, where each weight w in a this set lead to an identical structure (The authors used almost identical here, but I cannot get the point since no definition about any measure or metric space over net function were mentioned in this article). Since accurately approximation to this region is an absolutely difficult task, we can consider a box (or Hypercuboid) in the parameter space. Assume the parameter space is an L-dimension Euclidian space, we can define the concept of box: For each acceptable minimum $w$, its box $M_w$ is an L-dimension hypercuboid with center w. Each edge of the box is taken to be parallel to one weight axis. Let $\\Delta w(x)$ be the maximal values such that for all L-dimensional vectors k whose components ${k_ij}_{i=1,\\dots,n}$ are restricted by $|k_ij| \\le \\Delta w_ij(x)$. We can find this box with the constraint: $\\mathbf{E}(net(w), net(w+k), X) = \\sum_{x \\in X} ||o(w,x) - o(w, x+k)||^2 < \\epsilon$ ,where the $\\epsilon$ represent for the tolerable output change. Note that we assume the target function f is continuous here. Box’s volume is defined as $V(\\Delta w(X)) = 2^L \\Pi_{i,j}\\Delta w_{i,j}(x)$ 2. Information entropy of the minimaLet $B$ denote the bits required to describe the weights, whereas the number to describe the $y_p$,given $w$, can be bounded by fixing $\\mathbb{E_{tol}}$. In other words, we can control the smoothness of the function by choosing tolerance error. $B$ can be written as: $B(w, X_0) = -log(\\frac{1}{2^L}V(\\Delta w)) = \\sum_{i,j} -log(\\Delta w_{i,j})$ Of course, we hope the entropy as low as possible. Therefore, the authors recommend a penalty term into the objective function: $E(w, D_0) = E(net(w), D_0) + \\lambda B(w, X_0)$ , where $B(w, X_0) = \\sum_{x_p \\in X_0} B(w, x_p)$ 3. An Introduction to Gradient DescentBefore the introduction to GD methods, let’s define the optimization problem first. We assume the prediction function $f$ has a fixed form and is parameterized by a real vector $w \\in \\mathbb{R}^d$. Formally, for some given $f: \\mathbb{R}^{d_x} \\times \\mathbb{R}^d \\rightarrow{} \\mathbb{R}^{d_y}$, the family of prediction function is: $\\mathcal{H} = {f(\\cdot,w):w \\in \\mathbb{R}^d}$ Ideally, we want to minimize the expected risk. We assume there exists a probability space $(\\Omega, \\mathcal{F}, \\mathbb{P})$, where $\\Omega=\\mathcal{X}\\times \\mathcal{Y}$, $\\mathcal{F}$ is the filtration of $(x,y)$, and $\\mathbb{P}$ is the joint probability measure of $(x, y) \\in \\Omega$. The expected risk is defined as: $R(w) = \\int_{\\mathbb{R}^{d_x}\\times\\mathbb{R}^{d_y}}l(f(x;w),y)d\\mathbb{P}(x,y) = \\mathbb{E}(l(f(x;w),y))$ ,where function $l(\\cdot, \\cdot)$ is the loss function, which measure the difference between prediction and labels. The following statements are all based on the optimization problem we defined above. (Reference: An overview of gradient descent optimization algorithms, Sebastian Ruder) It is worth explaining that the convergence speed depends both on the data set and hyperparameters of optimizers. Generally, if the optimizer converge at a slow speed or diverge sometimes, we can adjust the initial step scale. 3.1 Full Gradient DescentThe Gradient Descent is an first order iterative method for finding the local minimum of a differentiable function, which also provides the core optimization methodology in machine learning. Given a function $f(x)$, the basic GD method can be written as: $x^{t+1} = x^{t} -\\eta \\nabla f(x^{t})$ , where $\\eta$ is represented for learning rate. In full GD method, we have to run through all the data samples in our training set to accomplish one update for a parameter. Therefore, if the size of data set is very large, the update process will take a lot of time. More specifically, the full GD method can be written as: $w^{t+1} = w^{t} - \\eta \\nabla_w \\sum_{i = 0}^{n}{l(f(x_i;w_t), y_i)}$ 3.2 Stochastic Gradient DescentThe origin of Stochastic GD method is stochastic approximation. Stochastic GD method is a specific case of Robbins-Monro algorithm. The stochastic GD can be written as: $w^{t+1} = w^{t} - \\eta \\nabla_w l(f(x_i; w_t),y_i)$ In other words, we use only one sample (or a subset of samples) to update the parameter. Compared to full GD method, the SGD method often converges much faster. Note that ${w_t}_{t\\ge0}$ is a stochastic process (a family of random variables defined on the same probability space) which is defined on the filtration generated by $x_i$. Note that we actually want to reduce the expected risk function, which implies that the expectation of gradient should be the best choice. From this perspective, the variance of estimation based on a single data point is much higher than the average over whole samples. Therefore, SGD may lead to a more oscillated convergence compared to full GD (but still good enough). 3.3 SGDMEach update step in SGDM is a combination of the steepest descent direction and the most recent iterate displacement. The iteration process can be written as: $v_k = \\gamma v_{k-1} + \\eta \\nabla_w l(f(x_i; w_k),y_i)$$w_{k+1} = w_k - v_k$ The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. This method is most effective when the parameters work differently in the system. For example, the data generated linearly with $w_1 = 100$, and $w_2 = 0.1$. 3.4 AdaGradThe AdaGrad method adjusts the learning rate with respect to the the history gradients. The learning rate gradually decreases with the decrements of the risk function. We have articulated that the unsuitable learning rate may lead to oscillation when we approach the local optima. This method update the learning rate in adaptive way. More specifically, $ g_{k} = \\nabla_w{l(f(x_k;w_k), y_k)}$$ w_{k+1} = w_{k} - \\frac{\\eta}{\\sqrt{G_k+ \\epsilon}}\\odot g_{k} $ , where $G_{k}$ is the sum of the square of the gradients up to the $k$-th update, and $\\epsilon$ is a smoothing term that avoids division by zero. Adagrad’s main weakness is its accumulation of the squared gradients in the denominator, and this in turn causes the learning rate to shrink, which stop us from learning any information from the data. Moreover, although it is still reasonable in convex optimization problem, it is a bad property when we get stuck on the saddle point. However, AdaGrad performs well in sparse data set. 3.5 RMSPropAdaGrad accumulates all past gradients and the learning rate keeps decreasing, which may make the learning much slower over time. RMSProp does not shrink the learning rate as rapidly due to the use of the decaying average. The moving/decaying average of the $G_k$ in the $k$-th update is defined as:$g_k = \\nabla_w{l(f(x_k;w_k), y_k)}$$G_k = \\gamma G_{k-1} + (1-\\gamma) g_k^2$ The gradient update under the following method: $w_{k+1} = w_{k} - \\frac{\\eta}{\\sqrt{G_k + \\epsilon}}\\odot g_k$ According to Geoffrey Hinton’s lecture Notes, the $\\gamma$ here was 0.9. 3.6 AdaDeltaAdaDelta made some extra improvements compared to AdaGrad and RMSProp (actually, they are developed independently). It reduces AdaGrad’s rapidly decreasing learning rate by using the same exponentially decaying average for past squared gradients as RMSProp. In addition, AdaDelta is aware that, in RMSProp, the base unit of the gradient is $\\frac{1}{x’s; unit}$. That is if we assume WLOG that the value of risk function have no base unit, then consider $\\frac{\\partial f}{\\partial w}dw$ should have no base unit, which means the base unit of the gradient is $\\frac{1}{x’s; unit}$. We should notice that the RMSProp method changes the base unit of $w$. Therefore, the authors suggested to revised this problem with: $W_k = \\gamma W_{k-1} + (1-\\gamma)w_k^2$$w_{k+1} = w_{k} - \\frac{\\sqrt{W_k + \\epsilon}}{\\sqrt{G_k + \\epsilon}}\\odot g_k$ Learning rate is removed. Thus, we do not need to set a fixed number for learning rate. 3.7 ADAMAdaptive Moment Estimation (Adam) computes adaptive learning rates considering the first and second order information:$M_t = \\beta_1 M_{t-1} + (1-\\beta_1) g_{t}$$V_t = \\beta_2 V_{t-1} + (1-\\beta_2) g_{t}^2$ Since $M_t$ and $V_t$ are initialized with 0, then the calculation of the first moment and second moment are biased towards zero, which is can be problematic within the first few time steps and when $\\beta$’s are close to $1$. To deal with that, we can use the bias-corrected estimations of $V_t$ and $M_t$, which can be written as:$\\hat{M_t} = \\frac{M_t}{1-\\beta_1^t}$$\\hat{V_t} = \\frac{V_t}{1-\\beta_2^t}$,where $\\beta_1^t$ and $\\beta_2^t$ is the t power of the $\\beta_1$ and $\\beta_2$ Then update process can be written as: $w_{k+1} = w_{k} - \\eta\\frac{M_k}{\\sqrt{V_k} + \\epsilon}$ The authors proposed default values of 0.9 for $\\beta_1$, 0.999 for $\\beta_2$, and$1e-8$ for $\\epsilon$. 3.8 Disadvantages of SGD and full GDNote that GD and SGD method are sensitive to learning rate, and it is not easy to choose a optimal learning rate. Moreover, the schedule of learning rate (or the evolution of learning rate) is a even more difficult task. The pre-defined learning rate schedule may not adapt to the data set’s characteristics. Last but not least, the loss function or the feasible region of the optimization problem may not be convex, which means our algorithm has to avoid numerous saddle and local minima.","link":"/2020/03/23/Stochastic-Approximation-Methods-Notes-2/"},{"title":"Stochastic Control-Notes-Part2-The solution of SDE","text":"Keywords: Stochastic Differential Equation, Verification theorem 1. Definition of SDEsConsider a filtered probability space $(\\Omega, \\mathbb{F}, \\mathbb{P}; \\{\\mathcal{F_t}\\})$ on which there is a d-dimensional Brownian motion. (Note that this filtration can be larger than the filtration generated by Brownian motion, which also may include a zero measured set. Review Brownian Motion and Stochastic Calculus). The following equation systems: $d X_t^{(i)} = \\mu_i(t, X_t)dt + \\sum_{j = 1}^d \\sigma_{i, j}(t, X_t)dW_t^{j}$ , where $\\mu(\\cdot, \\cdot)$ and $\\sigma(\\cdot, \\cdot)$ are measurable functions and $\\mu(t, X_t)$ and $\\sigma(t, X_t)$ are \\{\\mathcal{F_t}\\} process. Note that measurable and adapted are not equivalent since the structure of filtration is an addition to the probability space. 2. Strong Solution and Weak SolutionStrong solution: Given the filtered probability space and $\\{\\mathcal{F_t}\\})$-adapted Brownian motion, an $\\{\\mathcal{F_t}\\})$-adapted continuous process $X = \\{X_t\\}$ is called a strong solution if: $X_t^{(i)} = X_0^{(i)} + \\int \\mu_i(t, X_t)dt + \\sum_{j = 1}^d \\int \\sigma_{i, j}(t, X_t)dW_t^{j}$ Weak solution: Compared to strong solution, the probability measure $\\mathbb{P}$ is also a part of the solution. There exists a probability measure $\\mathbb{P}$ and an $\\{\\mathcal{F}_t\\}$-adapted, $\\mathbb{P}$-Brownian motion such that: $X_t^{(i)} = X_0^{(i)} + \\int \\mu_i(t, X_t)dt + \\sum_{j = 1}^d \\int \\sigma_{i, j}(t, X_t)dW_t^{j}$ 3. UniquenessThe uniqueness of strong solution requires Lipschitz condition. While the uniqueness in law only requires bounded $\\mu$ and $\\sigma$. The uniqueness in law means under different probability measure, solution $X$ and $\\tilde{X}$ have the same finite-dimensional distribution. For the verification theorem in the field of Stochastic control, we should notice that most of the objective function is of the form of expectation. Therefore, the weak solution of SDE (dynamic system) is good enough to for stochastic control problem.","link":"/2020/12/10/Stochastic-Control-Notes-2/"},{"title":"Statistical Inference-Notes-Part6-Wilks' Theorem","text":"Keywords: Likelihood ratio tests, Wilks’ Theorem, profile likelihood 1. Likelihood ratio test on an open subset of $\\mathbb{R}^d$We have shown the likelihood ratio test function is the most powerful test for simple hypothesis. Let’s consider a multiparameter problem in which $\\theta = (\\theta_1, \\dots, \\theta_d) \\in \\Theta$ forming an open subset of $\\mathbb{R}^d$ and suppose we want to test of the from: $H_0: \\theta_1 = \\theta_1^0, \\dots, \\theta_m = \\theta_m^0$ against the alternative $H_1$ in which $\\theta_1, \\dots, \\theta_d$ are unrestricted. Here $1 \\le m \\le d$ and $\\theta_1^0, \\dots, \\theta_m^0$ are known prescribed values. (联合假设检验) Let $L_1$ denote $\\sup\\{L(\\theta): \\theta \\in \\Theta\\}$ and $L_0$ denote $\\sup\\{L(\\theta): \\theta \\in \\Theta_0\\}$. Define the likelihood ratio statistic $T_n = 2\\log \\Big(\\frac{L_1}{L_0}\\Big)$ The notation indicates dependence on the sample size $n$. 2. Wilks’ TheoremLet’s assume the $L(\\theta)$ be at least twice continuously differentiable in all its components in some neighborhood of the true value of $\\theta$, and that the Fisher information matrix be well-defined and invertible. Suppose $H_0$ is true. Then, as $n \\xrightarrow{} +\\infty$, then $T_n \\xrightarrow{} \\chi^2_{m}$ (卡方分布的自由度是参数约束的个数) Proof: $T_n = 2\\{l_n(\\hat{\\theta}_n) - l_n(\\theta_0)\\} = 2(\\hat{\\theta_n} - \\theta_0)l_n^{\\prime}(\\hat{\\theta}_n) - \\frac{2}{2}(\\hat{\\theta_n} - \\theta_0)^2l^{\\prime\\prime}(\\theta_n^{*})$ (Lagrange remainder of $\\log$, where $\\theta_n^{*} \\in N(\\theta_0, |\\theta_n - \\theta_0|)$) Since we assume the $\\hat{\\theta}_n$ is the solution of regular function, we can quickly seen that: $T_n = n i_1(\\theta_0) (\\hat{\\theta}_n - \\theta_0)^2 \\frac{l^{\\prime\\prime}(\\theta_n^{*})}{l^{\\prime\\prime}(\\theta_0 )}\\frac{l^{\\prime\\prime}(\\theta_0)}{-n i_1(\\theta_0)}$. By the SLLW and strong consistency of likelihood estimator, $\\frac{l^{\\prime\\prime}(\\theta_n^{*})}{l^{\\prime\\prime}(\\theta_0 )}, \\frac{l^{\\prime\\prime}(\\theta_0)}{-n i_1(\\theta_0)} \\xrightarrow{} 1$. According to CLT, we can conclude that $n i_1(\\theta_0) (\\hat{\\theta}_n - \\theta_0)^2$ follows $\\chi^2$ distribution. The Slutsky’s Lemma established the required result. ReferenceEssentials of Statistics Inference","link":"/2021/01/16/Statistical-inference-Part6-Wilks-Theorem/"},{"title":"Technical debt in machine learning system","text":"Keywords: QC, boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues 1. The boundary erosion caused by complex modelStrict abstraction boundaries help express the invariants and logical consistency of the information input and output from an given component. Note the ML is required in exactly those cases when the desired behavior cannot be effectively expressed in software logic without dependency on external data. We can hardly assume any independency of the object we extract from the real world (all in all, it is a black box system). (总结的说，就是boosting产生的影响，bagging产生的影响，和可能的未定义行为产生的ensembles的影响) 1.1 EntanglementWe can hardly find a set of signals which any of the signal is orthogonal to the closed span generated by other signals. The phenomenon can be described as CACE(change anything change everything) principle. One of the solution is to isolate the model and serve ensembles(Questions: how should we defined the isolation of the model?). This solution works well when the subproblems decompose naturally such as in disjoint multi-class settings (Recall the structure of LDA). Ensembles work well when the errors in the component models are uncorrelated. However, the ensembles make the prediction worse when we improve one of the basis model but increase the correlation within the basis model system. The second solution is to focus on detecting changes in prediction behavior. 1.2 Correction cascadesThere are often situations in which model $m_{\\alpha}$ for problem A exist, and we are tempting to use the model $m_{A}$ to solve a slightly different problem $A^{\\prime}$. (随机变量的向量表示是能够通过同构说明的，这里实际上是在考虑只对A到$A^\\prime$)的残差建模). The problem is we create a new system dependency on the model $m_{A}$, which makes the evaluation of the improvement on the model $m_{A}$ more difficult (A离B 很近，C离A很近，如果改变C使得C离A更近，并不能保证C离B很近). Therefore, it is possible that the improvement of any model in the system will lead to a systematic detriments. The mitigation is to augment $m_{\\alpha}$ to learn the corrections directly within the same model by adding features to distinguish among the cases, or to accept the cost of creating a separate model for $A^\\prime$. 1.3 Undeclared ConsumersThe above two scenarios may be caused by the undeclared consumers. They may be the source of the hidden technical debt (entanglement and correction cascade). The only way to prevent from these detrimental events is to set up strict service-level agreement. 2. Data dependencies2.1 Unstable data dependency类似于onehot编码中加入了新的一类，导致数据输入不稳定。One common strategy is to create a versioned copy of a given signal. Create a frozen version of this mapping, and freeze it until the quality of the next version is fully controlled. Note that version control has it own cost. 2.2 Underutilized data dependencyUnderutilized data are the data sets with little increment to the prediction task. The typical examples of such kind of data includes: a) A feature $F$ is included in a model early in the development. However, we include a new features, for example $f(F) = F + 1$, which makes the feature $F$ redundant. b) Bundled features: we know a group of features is evaluated and found to be beneficial. However, some of the signals are actually has little or no value. (Question: how to define “has little value”) c) $\\epsilon-$Features: 糟糕的工程目标设定会导致研究员追求增益小，复杂度高的模型。 d) Correlated Features: (Important) the machine learning model can hardly recognize the differences between two correlated model A and B. For example, A is more naturally causal, while B is temporarily more correlated. Then model will think B is a better signal than A, and the residual of the projection on B will has limited projection on A. The result is brittleness if world behavior later changes the correlations. (这里作者的描述其实是缺乏概念上的严密性，然而实际上其实是这样的特征使得模型不够稳健，容易受到特征变化微弱的影响。) Underutilized dependencies can be detected via exhaustive leave-one-feature-out evaluations (feature sampling). These should be run regularly to identify and remove unnecessary features. One solution is to use the feature management system, which enables data sources and features to be annotated. This kind of tooling can make migration and deletion much safer in practice. 3. Feedback loops The key features of the live machine learning system is that research can hardly predict the performance of a machine learning system before it is released. （在线机器学习模型的表现是难于分析的）The reasons include that: a) Direct feedback loops: the models will influenced by their own choices. Can be solved by the randomized policy (but costly to analyze). b) Hidden feedback loops: two system influence each other indirectly through the world. (例如市场冲击问题，多个预测模型之间的博弈) Remark: 机器模型的表现实际受到许多因素的影响。 4. ML-system Anti-patternAvoid the patterns listed below. a) Glue code: Using generic packages often results in a glue code system design pattern, in which a massive amount of supporting code is written to get data into and out of general-purpose packages. Create a clean native solution rather than re-use a generic package. An important strategy is to wrap black-box packages into common API’s. (反对过度设计，从你我开始) b)不要写太多的数据管道。数据管道的一项重要问题就是问题的定位困难。 c) Dead Experimental Codepaths: 这是过多的通用package和数据管道的后果，一旦管道发生变化，可能会产生系统性的灾害。一个要求的预防措施是做好严格的代码test。 5. Common smella) Plain-old data type smell. A robust system is all to often encoded with plain data types like raw floats and integers. b) Multiple-language smell. Hard to transfer ownership to other individuals. c) Prototype smell: 在小样本上的测试可能会导致在大样本上的鲁棒性确实。一直使用小样本数据在落地时是有额外成本的。 6. Dealing with maintenance cost in the External World a) Fixed threshold in Dynamic systems: Pick a decision threshold for a given model to perform some action. Learn the threshold via simple evaluation on held out validation data. b) 测试问题：端对端测试很难检测一个实时系统是否按照预期工作。一些检测指标包括: ​ Prediction Bias: $\\mathbb{E}(x) = \\int_{\\Omega_{z}}\\mathbb{E}(x | z) dP(z)$。 ​ Action limits: 一旦出发某种行为(action function specified)应该自动触发人工检查。 ​ Up-stream producers: 通过监控上游app的行为来确定底层逻辑的问题。这种上游app的检查应该层层传递。","link":"/2021/05/17/TechnicalDebtInMachineLearningSystem/"},{"title":"Trickster-Part1-Random Walk with Shrinking Steps","text":"Keywords: Characteristic Function, Fourier Transformation Consider a one-dimensional straight line. Someone stays at point $0$ at time $t = 0$. He will randomly go left ($-1$) or go right ($+1$) with the same probability. The length of each step at the nth step will be $\\lambda^n$. What is the distribution at the $n$th step? 1. The domain of the distributionRecall the fundamental theorems in the real analysis: 1.1 The cardinality of the N and $2^N$ (Cantor’s Theorem)Let $N$ denote the set of natural number, and the $2^N$ denote the power set of the natural number. The Cantor’s Theorem is one of the most tricky one I learned in real analysis. It is simple but tricky for sure. Cantor claimed that $N$ and $2^N$ cannot have equal cardinality. Suppose for the sake of contradiction that the sets $N$ and $2^X$ had equal cardinality. Then there exist a bijection $f: N \\xrightarrow{} 2^N$ between $N$ and the power set of $N$. Now consider the set: $A = \\{x \\in N: x \\notin f(x)\\}$. Note that since A is a subset of $N$, then it is an element of the power set. Since f is a bijection, there must exist $x \\in X$ such that $f(x) = A$. (The set is well-defined.) Let $x$ denote $f^{-1}(A)$. If $x \\in A$, then $x \\notin f(x) = A$, which is a contradiction. If $x \\notin A$, then $x \\in f(x) = A$, which is a contradiction again. Therefore, this is not such bijection. Proposition: $2^N$ has the same cardinality of $\\mathbb{R}$. (读者自证不难) 1.2 ConclusionWe can conclude that the domain is uncountable (In the $n$th step, the number of the reachable points is $2^n$). 2. The solution of the equation generated by the ‘first step analysis’Let $P_n(x)$ denote the probability mass function at the $n$th step at the point n. I believe almost everyone will follow the first step analysis like at the first glance. However, a convolutional form will be much more easily to solve the problem. (That’s why the name of this series will be named as Trickster.) $P_n(x) = \\int_{\\mathbb{R}} P_{n - 1}(x - x^\\prime)f_{n}(x^\\prime) dx^\\prime$ where $f_n(x^\\prime) = \\frac{1}{2} [\\delta_{-\\lambda^n}(x) + \\delta_{\\lambda^n}(x)]$ Comment: 一般来说，我们考虑Markov Chain的时候会从状态出发，甚至我们的转移概率都是使用从状态A到状态B定义的。这种做法是符合直觉并具有一般性的。但是有时如果将状态与距离相联系，将会产生卷积的形式，此时在傅里叶变化中会更加容易处理。 Let $F_n(w)$ denote the Fourier transformation of $P_n(x)$, $G_n(w)$ denote the Fourier transformation of $f_n(x)$. $G_n(v) = \\int_{-\\infty}^{\\infty} \\frac{1}{2} [\\delta_{-\\lambda^n}(x) + \\delta_{\\lambda^n}(x)] e^{ivx} dx = \\frac{1}{2}[cos(v \\lambda^n) + cos(-v \\lambda^n)] = cos(v\\lambda^n)$ $F_n(v) = F_{n-1}(v)G_n(v) = F_0(v)\\Pi_{n = 1}^NG(v) = \\Pi_{n = 1}^N cos(v\\lambda^n)$ (Note that $F_0(v) = \\int_{\\mathbb{R}} \\delta_0(x) e^{ivx}dx = 1$) Therefore, the probability mass function at the $N$th step can be written as: $P_n(x) = \\frac{1}{2\\pi}\\int_{\\mathbb{R}} \\Pi_{n = 1}^N cos(v\\lambda^n) e^{-ivx}dv$ Example: $\\lambda = \\frac{1}{2}$: $\\Pi_{n = 1}^N \\cos(\\frac{v}{2^n}) = \\Pi_{n = 1}^N \\frac{\\sin(v/2^{n-1})}{2\\sin(v/2^n)} = \\frac{\\sin(v)}{2^N \\sin(v/2^N)}$ According to L’Hopital’s Law, $2^N \\sin(v/2^N) \\xrightarrow{N = \\infty} v$ Therefore, $\\lim\\limits_{N \\xrightarrow{} \\infty} P_N(x) = \\frac{1}{2\\pi}\\int_{\\mathbb{R}} \\frac{\\sin(v)}{v}e^{-ivx}dv$ , which is the uniform distribution.","link":"/2021/01/27/Trickster-Part1-Random-Walk-with-Shrinking-Step/"},{"title":"Differentiation Programming-Notes","text":"Keywords: AD algorithm, evaluation trace, Dual number 1. Prerequisite1.1 Numerical differentiationAccording to Tayler extension with Lagrange reminder: $$f(x + h) = f(x) + hf^{\\prime}(x) + \\frac{1}{2}h^2f^{\\prime\\prime}(\\xi_1)$$ $$f(x - h) = f(x) - hf^{\\prime}(x) + \\frac{1}{2}h^2f^{\\prime\\prime}(\\xi_2)$$ ,where $\\xi_{*} \\in (0, h)$ or $(h, 0)$. Remarka) The error of forward (or backward) definition of the numerical differentiation is $O(h)$. b) The error of $\\frac{f(x + h) - f(x - h)}{2h}$ is $O(h^2)$. The difference of the reminders is $O(h^2)$. 1.2 Symbolic differentiationChains rule. Note that AD is not numerical differentiation or symbolic differentiation. 2. Main modes in ADAll numerical computations are ultimately compositions of a finite set of elementary operations for which derivatives are known (Verma, 2000; Griewank and Walther, 2008). Notation:Let’s consider a function $f: \\mathbb{R}^n \\xrightarrow{} \\mathbb{R}^m$ a) $v_{i - n} = x_{i}, \\quad i = 1, \\dots, n$ are the input variables. b) $v_{i} \\quad i = 1, \\dots, l are the working variables$ c) $y_{m-i} = v_{l-i}, \\quad i=m-1, \\dots, 0$ Note that any numerical code will eventually result in a numerical evaluation trace. Therefore, the AD can differentiate not only the closed-formed expressions in the classical sense, but also the algorithms making use of control flow such as branching, loops, recursion and procedure. Evaluation trace includes all the binary and unary operations that generate the function. 2.1 Forward ModeEach forward pass of AD (or evaluation trace) is initialized by setting only one of the variables $\\dot{x}_i = 1$ and setting the rest to zero. Therefore, a forward run will generate a column of Jacobian matrix. 2.1.1 Propertiesa) Need n evaluations to compute the Jacobian matrix. b) A very efficient method to computing Jacobian-vector products. (Simply by initializing with $\\dot{x} = r$) c) Efficient for $f: \\mathbb{R} \\xrightarrow{} \\mathbb{R}^m$. NOT decent for $f: \\mathbb{R}^m \\xrightarrow{} \\mathbb{R}$. 2.1.2 Dual numbersForward AD can be viewed as evaluating a function using dual numbers, which can be defined as truncated Taylor series of the form $v + \\dot{v}\\epsilon$, where $v, \\dot{v} \\in \\mathbb{R}$ and $\\epsilon$ is a nilpotent number such that $\\epsilon^2 = 0$ and $\\epsilon \\neq 0$. With this notation, the operations between $v$ and $\\dot{v}$ can be written as: $v + \\dot{v} \\epsilon + u + \\dot{u} \\epsilon = (u + v) + (\\dot{u} + \\dot{v}) \\epsilon$ $(v+\\dot{v} \\epsilon)(u + \\dot{u}\\epsilon) = vu + (v\\dot{u} + \\dot{v}u)\\epsilon$ Note that the coefficient of nilpotent mirror the symbolic differentiation rules. We can further define that: $f(v + \\dot{v} \\epsilon) = f(v) + f^{\\prime}(v)\\dot{v}\\epsilon$. Therefore, the coefficient of nilpotent can express the chain rules. Recall the differences between variables and constant in Pytorch or TF. The dual number of variables can be written as $v + 1\\epsilon$, while the constant can be written as $v + 0 \\epsilon$. The chains rule can be expressed as: $g(f(v + \\dot{v}\\epsilon)) = g(f(v) + f^{\\prime}(v)\\dot{v}\\epsilon) = g(f(v)) + g^{\\prime}(f(v))f^{\\prime} \\dot{v} \\epsilon$ 2.2 Reverse mode of ADReverse mode of AD correspond to a generalized backpropagation algorithm. The reverse mode propagates derivatives backward from a given output. In the Dual number model, we can compute the derivatives based on the value of intermediate variables. However, in the reverse mode of AD, we require the value of the output. For each variables $v_i$, there is an adjoint variable $\\bar{v_i}$, where $\\bar{v_i}$ is defined as the sensitivity of an interested output $y$ wrt the changes in $v_i$. $\\bar{v_i} = \\frac{\\partial y}{\\partial v_i}$ Note that we need a pre-defined function here, which means we can easily get the expression of $\\bar{v_i}$. Clearly, this mode is much more quickly for a function $\\mathbb{R}^m \\xrightarrow{} \\mathbb{R}^n$, where $m >> n$ ReferenceAutomatic Differentiation in Machine Learning: a Survey","link":"/2020/11/01/differentiation-programming-notes/"},{"title":"Trickster-Part2-The Number of Integer Solution","text":"Keywords: Tricks 1. Problem Description$x, y, z \\in \\mathbb{N}$. For an integer $n$, $x + y + z = n$. Let $f(n)$ denote the number of solutions, which satisfies $x \\le y \\le z$. What’s the value of $\\lim\\limits_{n \\xrightarrow{} \\infty}\\frac{f(n)}{n^2}$? The core of this problem is the number of solutions given an integer constraint. Due to the symmetricity, $x \\le y \\le z$ accounts for $\\frac{1}{3!}$ of the total number of the solutions. There are two methods to solve the total number of the solutions. 1.1 CombinationConsider this problem as the dividing the n balls into three parts. Let $x^{\\prime} = x + 1 \\in \\{1, \\dots\\}$, $y^{\\prime} = y + 1 \\in \\{1, \\dots\\}$, and $z^{\\prime} = z + 1 \\in \\{1, \\dots\\}$. The origin problem then is equivalent to $x^{\\prime} + y^{\\prime} + z^{\\prime} \\le n + 3$. According to stars and bars (插板法), we can easily find the number of combinations are $C^{2}_{n + 2}$. Therefore, the value of the limits should be $\\frac{1}{2 \\times 3!} = \\frac{1}{12}$ 1.2 Geometry$f(n)$ is the number of points on the surface of $x + y + z = 1$ in a unit cubic. Note that it is not equivalent to the area of surface (more specifically, the triangle, you know what I mean). The trickiest part is that the density of points on the surface is not equivalent to the number of point on the $(x, y, 0)$. One method to solve this problem is to use the area of the projection of the surface onto $(x, y, 0)$. We can get the same answer $\\frac{1}{2 \\times 3!} = \\frac{1}{12}$. 1.3 IntuitionThe number of point is equivalent to the volume instead of area. Therefore, given a constraints, we should apply the same metric to the numerator and denominator. 2. GeneralizationConsider a $K$ dimensional vector space $(x_1, x_2, \\dots, x_K)$. What is the number of integer solutions of $\\sum_{i = 1}^K a_i x_i = n$? 2.1 Recursive methodLet $f(n, k)$ denote the number of solutions given k variables which sum up to n. The recursive formula can be written as: $f(n, k) = \\sum_{i = 1}^n f(n - a_k \\cdot i, k-1)$ 2.2 Generator functionLet $f(x)$ denote $\\prod_{i=1}^K (1 + x_i^{a_i} + x_i^{2a_i} + x_i^{3a_i} + \\dots) = \\prod_{i=1}^K \\frac{1}{1 - x_i^{a_i}}$. The number of integer solutions is $f^{(n)}(x)|_{x = 0}$ ReferenceMy roommate Ivan Ye.","link":"/2021/02/07/Trickster-Part2-The-number-of-integer-solution/"}],"tags":[{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Finance","slug":"Finance","link":"/tags/Finance/"},{"name":"Portfolio selection","slug":"Portfolio-selection","link":"/tags/Portfolio-selection/"},{"name":"Stochastic Control","slug":"Stochastic-Control","link":"/tags/Stochastic-Control/"},{"name":"Rational agent","slug":"Rational-agent","link":"/tags/Rational-agent/"},{"name":"Time series analysis","slug":"Time-series-analysis","link":"/tags/Time-series-analysis/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"PAC","slug":"PAC","link":"/tags/PAC/"},{"name":"Coding","slug":"Coding","link":"/tags/Coding/"},{"name":"Functional analysis","slug":"Functional-analysis","link":"/tags/Functional-analysis/"},{"name":"Game theory","slug":"Game-theory","link":"/tags/Game-theory/"},{"name":"Statistics","slug":"Statistics","link":"/tags/Statistics/"},{"name":"GMM","slug":"GMM","link":"/tags/GMM/"},{"name":"IO","slug":"IO","link":"/tags/IO/"},{"name":"Time Series","slug":"Time-Series","link":"/tags/Time-Series/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Katex","slug":"Katex","link":"/tags/Katex/"},{"name":"Abstract algebra","slug":"Abstract-algebra","link":"/tags/Abstract-algebra/"},{"name":"Cauchy-Schwarz inequality","slug":"Cauchy-Schwarz-inequality","link":"/tags/Cauchy-Schwarz-inequality/"},{"name":"Triangle inequality","slug":"Triangle-inequality","link":"/tags/Triangle-inequality/"},{"name":"Kalman Filter","slug":"Kalman-Filter","link":"/tags/Kalman-Filter/"},{"name":"Optimal Control","slug":"Optimal-Control","link":"/tags/Optimal-Control/"},{"name":"Probability theory","slug":"Probability-theory","link":"/tags/Probability-theory/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Mathematical Programming","slug":"Mathematical-Programming","link":"/tags/Mathematical-Programming/"},{"name":"N-factors Gaussian Model in Finance","slug":"N-factors-Gaussian-Model-in-Finance","link":"/tags/N-factors-Gaussian-Model-in-Finance/"},{"name":"SDE","slug":"SDE","link":"/tags/SDE/"},{"name":"Stochastic Analysis","slug":"Stochastic-Analysis","link":"/tags/Stochastic-Analysis/"},{"name":"Numpy","slug":"Numpy","link":"/tags/Numpy/"},{"name":"Programming","slug":"Programming","link":"/tags/Programming/"},{"name":"Linear Algebra","slug":"Linear-Algebra","link":"/tags/Linear-Algebra/"},{"name":"Functional programming","slug":"Functional-programming","link":"/tags/Functional-programming/"},{"name":"Metric Embedding","slug":"Metric-Embedding","link":"/tags/Metric-Embedding/"},{"name":"Stochastic analysis","slug":"Stochastic-analysis","link":"/tags/Stochastic-analysis/"},{"name":"Orthogonal Process","slug":"Orthogonal-Process","link":"/tags/Orthogonal-Process/"},{"name":"Probabilistic Programming","slug":"Probabilistic-Programming","link":"/tags/Probabilistic-Programming/"},{"name":"Machine learning","slug":"Machine-learning","link":"/tags/Machine-learning/"},{"name":"Computer science","slug":"Computer-science","link":"/tags/Computer-science/"},{"name":"Statistical Inference","slug":"Statistical-Inference","link":"/tags/Statistical-Inference/"},{"name":"Decision Theory","slug":"Decision-Theory","link":"/tags/Decision-Theory/"},{"name":"Abstract Algebra","slug":"Abstract-Algebra","link":"/tags/Abstract-Algebra/"},{"name":"Linear regression","slug":"Linear-regression","link":"/tags/Linear-regression/"},{"name":"Time series","slug":"Time-series","link":"/tags/Time-series/"},{"name":"Spectral analysis","slug":"Spectral-analysis","link":"/tags/Spectral-analysis/"},{"name":"Engineering","slug":"Engineering","link":"/tags/Engineering/"},{"name":"software development","slug":"software-development","link":"/tags/software-development/"},{"name":"Trickster","slug":"Trickster","link":"/tags/Trickster/"},{"name":"Differentiation Programming","slug":"Differentiation-Programming","link":"/tags/Differentiation-Programming/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"Combinations","slug":"Combinations","link":"/tags/Combinations/"},{"name":"Tricks","slug":"Tricks","link":"/tags/Tricks/"}],"categories":[{"name":"Notes","slug":"Notes","link":"/categories/Notes/"},{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"},{"name":"Trickster","slug":"Trickster","link":"/categories/Trickster/"}]}