{"pages":[{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"About Author","text":"Self-introductionAs a first-year graduate student, the only thing I know is that I know nothing. I decided to start writing blogs to record interesting papers and fascinating ideas in the fields of Financial Engineering, Machine learning, Optimization, and Algorithm. Moreover, I want to share my works with you. I cannot promise all the phrases in my blogs are precise and rigorous, but I have tried my best to do so. If you find any mistakes and typos, please feel free to contact me via the following email. Thank you for reading my blogs. Peimou Email: ps3136@columbia.edu","link":"/about/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Hello World","text":"Apparently, it is my very first post. Testing, testing, 1, 2, 3… I want to list some helpful links here. How to use Hexo Hexo. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting. http://theme-next.iissnan.com/theme-settings.html (language: Chinese) https://www.jianshu.com/p/efbeddc5eb19 (language: Chinese) Math support in Hexo: Mathjax vs KatexAlthough Katexs support less mathematics symbols compared to Mathjax, Katex only takes hundreds microsecond to render math formula, which is hundred times faster than Mathjax. I appreciate Zijing Hu’s help when I struggled with the settings of Katex and hexo-renderer-kramed. You can visit Zijing’s github page for details. You can read the Katex documents to learn more about the syntax and details.","link":"/2020/01/13/Hello%20World/"},{"title":"Stochastic Approximation Methods--Notes--Part 1","text":"Generally, stochastic approximation methods are a family of iterative methods. The goal of these algorithms is to recover some properties of a function depending on random variables. The application of stochastic approximation ranges from deep learning (e.g., SGD ) to online learning methods. The earliest algorithms are Robbins-Monro(1951) and Kiefer-Wolfowitz(1952) algorithms. 1. An introduction to Stochastic Approximation MethodsIn this section, I will show you some examples of stochastic approximation methods. These are the topics I met in my daily study and research. 1.1 Example 1: ExpectationFor a given probability space $(\\Omega, \\mathcal{F}, P)$, we want to solve the expectation of $E^P(f(\\omega))$, where $f$ is a measurable function, and $\\omega \\in \\Omega$. A reasonable solution is to estimate the expectation via arithmetic average. Let $\\mathbb{E}_n$ be the estimation of $\\mathbb{E}^P(f(\\omega))$ given $n$ observation. $n \\times E_n = \\sum_{i = 1}^n f(\\omega_i)$ $ (n+1) \\times E_{n+1} = \\sum_{i = 1}^{n+1} f(\\omega_i) = n \\times E_n + f(\\omega_{n+1}) $ $ \\implies (n + 1) \\times E{n+1} = (n+1) \\times E_n + f(\\omega_{n+1}) - E_n $ $ \\implies E_{n+1} = E_n + \\frac{1}{n+1} [f(\\omega_{n+1}) - E_n] $ 1.2 Example 2: SGDIn the framework of stochastic gradient descent (SGD), or on-line gradient descent, the true gradient $\\nabla Q(\\omega)$ is approximated by a gradient at a single sample. Let $\\omega_n$ be the estimation of true gradient in the $nth$ iteration, then $\\omega_{n+1} = \\omega_n - \\lambda \\nabla Q(\\omega)$ where $\\lambda$ is the learning rate. There are several popular extensions of the SGD method, including Momentum, Nesterov Momentum, and Adam. 1.3 Example 3: Q-Learning in reinforcement learningIn the field of reinforcement learning, function $Q: S \\times A \\xrightarrow{} \\mathcal{R}$ combines the reward with states and actions. At each time $t$, the agent select an action $a_t$, get an reward $r_t$ from the environment, and enter into the next state $s_{t+1}$. The discussions of convergence of reinforcement learning cannot avoid the topics about the Q function or the value function. $$Q^{new}(s_t, a_t) = (1 - \\alpha)Q^{old}(s_t, a_t) + \\alpha (r_t + \\gamma \\max \\limits_a Q^{old}(s_{t+1}, a))$$ where $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor. These topics, including MDP and RL, are definitely will be discussed in other notes. The convergence of this Q-learning algorithm (Robbins-Monro Algorithm) will be proved in the next section. 2. Robbins-Monro AlgorithmIn 1951, Robbins and Monro developed a methodology for solving a root finding problem given a function depends (or partially depends) on a random variable. Root finding problem is the generalization of several important topics, including optimizations and extrema finding. 2.1 Root finding problem without uncertaintyLet $M(x)$ be a given function and $\\alpha$ a given constant such that the equation $M(x) = a$ has a unique root $x = \\theta$. For any iterative method for this root finding problem, we find one or more initial values $\\ x_{initial}$, and then successively obtain new values as certain functions of the previous obtained $x$. The most famous algorithm is the Newton–Raphson method. 2.2 A stochastic generalization of the root finding problemLet’s consider how to solve this problem when $M(x)$ is unknown for some reason. In this blog, I decided to rephrase the notations of the Robbins-Monro algorithm in a more specific way. We assume the randomness of the function is captured by the probability space $(\\Omega, \\mathcal{F}, P)$, and there is a random variable $Y$ on $(\\Omega, \\mathcal{F}, P)$ which takes values in a state space $(S, \\mathcal{L})$. In this blog, we assume the state space is a d-dimensional Euclidean space equipped with the $\\sigma$-field of Borel sets. In this algorithm, $M(x)$ is a deterministic function which defined as follows: $M(x) = \\int_{y \\in S }ydH(y|x)$ where $H(y|x) = \\mathbb{P}[Y(\\omega) \\le y|x]$ ($\\omega \\in \\Omega$) is the distribution function of $Y$ given an unobservable deterministic variable $x$. In other word, $M(x)$ is the expectation of $Y$ for the given $x$. Although we know nothing about the nature of function $M(x)$ and the distribution function $H(y|x)$, but we assume (which mean this assumption will work as a known condition in the proof) that the equation $M(x) = a$ has a unique solution $x = \\theta$. Bobbins-Monro algorithm introduced a method to estimate $\\theta$ by making successive observations on $Y$ at given $x_1, x_2, …$, which are determined sequentially in accordance with some definite experimental procedure. The idea of this algorithm is to construct a transition probability between two states, or (non-stationary) Markov chain of ${x_n}$. Moreover, with this sequence, the difference between $\\lim \\limits_{k \\xrightarrow{} \\infty}x_k$ and $\\theta$ can be controlled. We can conclude this idea as finding a sequence $\\{x_k\\}$, such that $\\lim \\limits_{k \\xrightarrow{} \\infty}E(x_k - \\theta)^2 = 0$ The idea also implies the convergence in probability (or in $\\mathcal{L}^2$) of $x_n$ to $\\theta$. 2.3 The proof of convergence theoremNobody can construct a sequence without any hints. The hint of this algorithm is the first example (expectation estimation) in the Introduction section. If we review the expectation estimation problem in the framework of Robbins-Monro, then in this example, $x$ denotes the expectation and function $M(x) = \\mathbb{E}^P[Y - x]$, and we want to solve the equation $M(x) = 0$. I only present brief proof to show the idea. You may read this paper for more details. We want to prove that: Condition 1: For every $x$, a distribution function in $y$, and there exist a positive constant C such that $\\mathbb{P}[|Y(\\omega)| \\le C|x] = \\int_{-c}^cdH(y|x) = 1$ Condition 2: There exist finite constants $\\alpha, \\theta$ such that $M(x) \\le \\alpha$ for $x \\lt \\theta$, and $M(x) \\ge \\alpha$ for $x \\gt \\theta$ Condition 3: Let $\\{a_n\\}$ be a fixed sequence of positive constants such that $0","link":"/2020/01/13/Stochastic%20Approximation%20Methods--Notes--Part%201/"},{"title":"Kalman Filter-Notes-Part1","text":"R.E Kalman published a paper about a recursive solution to a discrete-data linear filtering problem. The Kalman filter provided us a computational method to estimate unobservable variables through minimization the posterior error. Moreover, Kalman filter enabled us to obtain the consistent parameter estimates by maximizing the likelihood function of error innovations. 1. The basis of discrete Kalman filterGenerally, Kalman filter solved the problem of estimating the unobservable state $x \\in \\mathcal{R}^n$ of a discrete time control process. The dynamic of the unobservable state’s ($n \\times 1 $ vector) can be written as: $x_k = Ax_{k - 1} + Bu_{k - 1}＋ｗ_{k - 1}$ (A RCLL continuous time Gaussian dynamic can be discretized with the tool of stochastic calculus) The measurement variable $z_k$ follows a linear transformation of unobservable states with an linearly additive noise term $v_k$: $z_k = Hx_k＋v_k$ Note that we assume the probability measure of $w$ and $v$ follow Gaussian distribution with zero mean and covariance matrix Q , and R, which are both independent of each other and white. My favorite notation among several papers about Kalman filter is Greg Welch and Gary Bishop’s: An introduction to Kalman filter. I will use the same symbols to show you the computational origin of Kalman filter. Notations: $\\hat{x_k^{-}} \\in \\mathcal{R}^n$ is the estimation of unobservable states $x$ at time $k$ under the filtration $\\mathcal{F}_{t-1}$. $\\hat{x_k}$ is the posteriori state estimate under the information at time $k$ (with the help of $z_k$). Priori estimate error $e^{-}_{k}$: $x_k - \\hat{x_k^{-}}$. Posteriori estimate error $e_{k}$: $x_k - \\hat{x_k}$. Priori estimate error covariance matrix $P^{-}_{k} = \\mathbb{E}[e^{-}_ke^{-T}_k]$ Posteriori estimate error covariance matrix $P_{k} = \\mathbb{E}[e_ke^{T}_k]$ We assume the relationship between the posteriori estimate and the priori estimate follows: $\\hat{x_k} = \\hat{x^{-}_k} + K_k(z_k - H\\hat{x^{-}_k})$ Remark: If you know something about Gaussian distribution, you may find this is the BUE given observed states if we assume the noises is Gaussian distributed (it is trivial if you realized it is a result of joint Gaussian distribution ). I firmly believe this assumption is inspired from a probability origin. I plan to write something about Gaussian process and Gaussian process regression in the future. You may see more details there. $z_k - H\\hat{x^{-}_k}$ are related to many frightening names like measure residual or innovation. As we need to minimize the posteriori error of each entry of the state vector, we should minimize the trace of $P_k$. Note that in the classical Kalman filter, the dynamic of the system is deterministic. We only need to consider some incontrollable factors including measurement error. However, if you are familiar with stochastic calculus, you can consider the difference between this deterministic dynamic and the counterpart in stochastic analysis. Personally speaking, I believe the dynamic of stochastic process is the dynamic of measures, or the transition of functions. The Kalman filter only describe the state of observables. $\\hat{x_k} = \\hat{x^{-}_k} + K(z_k - Hx^{-}_k) \\leftrightarrow \\hat{x_k} = \\hat{x^{-}_k} + K(Hx_k + v_t - Hx^{-}_k)$ $\\leftrightarrow x_k - \\hat{x_k} = x_k - \\hat{x^{-}_k} - K(Hx_k + v_t - Hx^{-}_k)$ $\\leftrightarrow P_k = (I - K_kH_k)P_k^-(I - K_kH_k)^T+K_kR_kK_k^T$ (*) I decide to review some tricks in matrix derivation. You can see more details about layout with wiki. $\\frac{d(tr(AB))}{dA} = B^T$ $\\frac{d(tr(ACA^T))}{dA} = 2AC$ (It is trivial if you know $tr(AB) = \\sum_i\\sum_j a_{ij}b_{ji}$) We can get the optimal $K_k$ (the Kalman Gains): $K_k = P^{-}_kH^T(HP^{-}_kH^T+R)^{-1}$ With the Kalman gain and (*), we can get the posteriori estimates of state’s posteriori covariance matrix $P_k$. $P_k = (I - K_kH_k)P_k^-$ Note that if $\\lim \\limits{R \\xrightarrow{} 0}$ (there is no error in measurement), then the Kalman gain get the maximum value $H^{-1}$. If $\\lim \\limits{P_k^- \\xrightarrow{} 0}$, then the Kalman gain get the minimum value 0. You can see the adaptiveness of the methods here. If we are more confidence with the transition equation of unobservable state, we should allocate relatively more weight on the priori estimates; If we are more confident with the measurement of the observable equation, then we should make the posteriori estimate more close to the observable states. With the best estimated posteriori estimates of unobservable state, we can update the new priori estimate of unobservable variables. $P_k^- = AP_kA^T + Q$ $\\hat{x_k^-} = A\\hat{x_{k-1}} + Bu_{k-1}$ Remark: In the system of Kalman filter system, we know exactly the transition of unobservable states. In this situation, the Kalman filter found a solution to estimate the unobservable states $x_k$ based on information at time k. If we know nothing about the transition of unobservable states (unknown A for example), we can maximize the likelihood function based on the measurement function, or, in the framework of empirical loss evaluation, we can minimize the error between observable states $z_{t+1}$ at time t=1 and priori estimates of observable states $\\hat{z_{t+1}^-}$ (under $\\mathcal{F}_t$). 2. Discussions about the initial valuesUnfortunately, we need to input four tricky initial values to the Kalman filter: $x_0^-$, $P_0^-$, R, and Q. The more unfortunate fact is that KF is sensitive to the initial values. Based on this fact, we have to adjust hyperparameters for our model. The best statement of adjustment of initial values I have ever met is that the innovation ($z_k - H\\hat{x_k^-}$) should be a white noise with zero mean if KF works well (See Optimal State Estimation Kalman-H-and Nonlinear Approaches). We have to test different hyperparameters to help the model make sense. Of course if you know criteria about this topic, please feel free to share your ideas under this blog. Thank you for reading my blog! KF is a big topic and I will dig deeper in the next parts of this Note.","link":"/2020/01/16/Kalman%20Filter-Notes-Part1/"},{"title":"Kalman Filter--Projects--Part1.5--N-factors Gaussian model","text":"Before we dig deeper in the Kalman filter, I would like to share one of my projects with you, and the name of this project is presented in the title. N-factors Gaussian model is a direct application of what we have discussed in Kalman filter Part1. I think it is quite popular in the field of finance (it should be :)). I will give you a brief introduction about this method, and some tricks in this algorithm‘s implement. I strongly recommend you to read this blog if you are confused with the model calibration. 1. The Financial basis of N-factor Gaussian modelIf you have heard Barra model for stocks or other factors analysis models, you may understand this n-factors Gaussian model easily. Factor analysis is a cross-sectional methodology, which totally depend on the information at current time slot. To be more specific, it can detect the sources of correlations if we assume the correlations between different factors are caused by communalities in this framework. Of course, we can use factors analysis framework to predict with some extra assumptions (for example, the factors’ returns keep constant between time slots). At time $t$, we can know the return of $n$ assets. We assume the correlations between assets caused by $m$ factors (we also call these factors as risk factors). Matrix $X_{(m \\times n)}$ denotes the exposures of assets on risk factors, while vector $f$ denotes the ‘return’ of risk factors. If we write this model in a linear regression scheme, then $r_t = X_t^Tf + \\epsilon$ Intuitively, we decompose the observed returns based on the risk factors, and the coefficients of factors describe the average returns of each factors to some extent. In this framework, we treat the returns of factors as constants. What if we want to treat the coefficients like a distribution or something which contains the uncertainties? Let’s review the framework of risk neutral theorem in financial engineering. I will only show you how the theorem describe the uncertainties in the price generation process, rather than articulate the whole framework. We assume the market (efficient and complete) has $n$ risk factors which determine the return of observable assets. Let $\\Theta(t)$ be a n-dimensional adapted process according to $W(t)$, which is a m-dimensional Brownian motions. In this framework, the returns of the risk factors are described as a m-dimensional Brownian motion. $d\\Theta(t) = \\mu(\\Theta)dt + \\sigma(\\Theta)dW(t)$ $sigma(\\Theta)$ works the same as the risk factors exposures in this framework. From this perspective, multi-factors model in stocks is a special case of the popular financial engineering basics. 2. The settings of N-factors Gaussian modelWe assume the spot prices of assets ($\\mathbf{S}_t$ is a $N \\times 1$ vector)in a market can be described with $n$ risk factors as: $log\\mathbf{S_t} = \\mathbf{L}_t’ \\mathbf{x_t} + \\mu t $ $d\\mathbf{x}_t = -\\mathbf{K}\\mathbf{x}_tdt + \\mathbf{\\Sigma} d\\mathbf{W}_t$ where $\\mathbf{K} = \\begin{Bmatrix} 0 & 0 & … & 0 \\\\ 0 & k_2 & … & 0 \\\\ .. & .. & .. & .. \\\\ 0 & 0 &…&k_n \\end{Bmatrix}$, and $\\mathbf{\\Sigma} = \\begin{Bmatrix} \\sigma_1 & 0 & … & 0 \\\\ 0 & \\sigma_2 & … & 0 \\\\ .. & .. & .. & .. \\\\ 0 & 0 &…&\\sigma_n \\end{Bmatrix}$ $\\mathbf{L}_t$ is a $n \\times N$ matrix. The settings of multi-factors analysis include the independence of communalities, and so they are here. However, of course, we can set the correlations between the risk factors. For example, we assume the risk factors are linear combinations of m independent factors (m < n), then the observed returns of the markets is a affined transformation of m-factor Gaussian model we showed above (See Affined N-factors Gaussian Model). 3. Parameters estimationThis is a direct application of Kalman filter. I will discuss some details in the estimation here. (See here to know more about Kalman filter). As we have discussed in the Part I of KF Notes, we can maximize the likelihood function based on innovation ($z_t - \\hat{z_t}$) to estimate the transition matrix of unobservable states. I will show you the details of the aforementioned methodology. $\\mathbf{x}_t = A \\mathbf{x}_{t-1} + \\mathbf{c}_t + \\mathbf{\\epsilon}_t$, where $\\mathbf{\\epsilon}_t \\backsim N(0, Q_t)$ $\\mathbf{z}_t = H \\mathbf{x}_{t} + \\mathbf{d}_t + \\mathbf{v}_t$, where $\\mathbf{v}_t \\backsim N(0, R_t)$ $\\hat{x_t^-} = A\\hat{x_{t-1}} + \\mathbf{c}_t$ $\\hat{P_t^-} = A \\hat{P_{t-1}}A^T + Q_t$ $\\hat{z_t^-} = H \\hat{\\mathbf{x}_t^-} + \\mathbf{d}_t$ Let $\\hat{F_t^-}$ be the priori covariance matrix of observable states. $\\hat{F_t^-} = H\\hat{P_t^-}H^T + R_t$ We need to parameterize the covariance matrix $R_t$. Some papers assume the error of measurement equation is a homoscedastic diagonal matrix. However, of course we can try some more complex assumption, which is another typical bias and variance trade off. We need to maximize the log-likelihood function of $z_t$: $\\mathcal{L}(\\Theta) = -\\frac{1}{2} \\sum_t log|F_t| - \\frac{1}{2} \\sum_t [z_t - \\hat{z_t^-(\\Theta)}]^T F_t^{-1}[z_t - \\hat{z_t^-(\\Theta)}]$ If $H_t$ also needs to be parameterized, we can use EM algorithm to estimate the parameters. EM is another big topic, and I will start a new series of notes to articulate its history and applications. Remark: We used the priori estimation of the covariance matrix of $\\hat{z_t^-}$ in the likelihood function. You may ask why not the posterior covariance matrix? The answer is quite trivial if you notice that the posterior estimation includes a weighted average of priori estimation and innovation (measure residual), which means we have to know the innovation before the posteriori estimation. In the likelihood function methodology, we use Fisher information matrix to estimate the variance of parameters.","link":"/2020/01/18/Kalman%20Filter-Projects-Part1.5/"}],"tags":[{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Katex","slug":"Katex","link":"/tags/Katex/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Kalman Filter","slug":"Kalman-Filter","link":"/tags/Kalman-Filter/"},{"name":"Optimal Control","slug":"Optimal-Control","link":"/tags/Optimal-Control/"},{"name":"N-factors Gaussian Model in Finance","slug":"N-factors-Gaussian-Model-in-Finance","link":"/tags/N-factors-Gaussian-Model-in-Finance/"}],"categories":[{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"},{"name":"Notes","slug":"Notes","link":"/categories/Notes/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"}]}