{"pages":[{"title":"About Author","text":"Self-introductionAs a first-year graduate student, the only thing I know is that I know nothing. I decided to start writing blogs to record interesting papers and fascinating ideas in the fields of Financial Engineering, Machine learning, Optimization, and Algorithm. Moreover, I want to share my works with you. I cannot promise all the phrases in my blogs are precise and rigorous, but I have tried my best to do so. If you find any mistakes and typos, please feel free to contact me via the following email. Thank you for reading my blogs. Peimou Email: ps3136@columbia.edu","link":"/about/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"},{"title":"Categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"Hello World","text":"Apparently, it is my very first post. Testing, testing, 1, 2, 3… I want to list some helpful links here. How to use Hexo Hexo. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting. http://theme-next.iissnan.com/theme-settings.html (language: Chinese) https://www.jianshu.com/p/efbeddc5eb19 (language: Chinese) Math support in Hexo: Mathjax vs KatexAlthough Katexs support less mathematics symbols compared to Mathjax, Katex only takes hundreds microsecond to render math formula, which is hundred times faster than Mathjax. I appreciate Zijing Hu’s help when I struggled with the settings of Katex and hexo-renderer-kramed. You can visit Zijing’s github page for details.","link":"/2020/01/13/Hello%20World/"},{"title":"Note for Stochastic Approximation Methods--Part 1","text":"1. An introduction for Stochastic Approximation MethodsGenerally, stochastic approximation methods are a family of iterative methods. The goal of these algorithms is to recover some properties of a function depending on a random variables. The application of stochastic approximation ranges from deep learning (eg. SGD ) to online learning methods. The earliest algorithms are Robbins-Monro(1951) and Kiefer-Wolfowitz(1952) algorithms. 1.1 Example 1: ExpectationFor a given probability space $(\\Omega, \\mathcal{F}, P)$, we want to solve the expectation of $E^P(f(\\omega))$, where $f$ is a measurable function, and $\\omega \\in \\Omega$. A reasonable solution is to estimate the expectation via arithmetic average. Let $\\mathbb{E}_n$ be the estimation of $\\mathbb{E}^P(f(\\omega))$ given $n$ observation. $$n \\times E_n = \\sum_{i = 1}^n f(\\omega_i)$$ $$(n+1) \\times E_{n+1} = \\sum_{i = 1}^{n+1} f(\\omega_i) = n \\times E_n + f(\\omega_{n+1})$$ $$\\implies (n + 1) \\times E{n+1} = (n+1) \\times E_n + f(\\omega_{n+1}) - E_n$$ $$\\implies E_{n+1} = E_n + \\frac{1}{n+1} [f(\\omega_{n+1}) - E_n]$$ 1.2 Example 2: SGDStochastic gradient descent (SGD), or on-line gradient descent, the true gradient $\\nabla Q(\\omega)$ is approximated by a gradient at a single sample. Let $\\omega_n$ be the estimation of true gradient in the $nth$ iteration, then $$\\omega_{n+1} = \\omega_n - \\lambda \\nabla Q(\\omega)$$ where $\\lambda$ is the learning rate. There are several famous extensions of SGD method, including Momentum, Nesterov Momentum, and Adam. 1.3 Example 3: Q-Learning in reinforcement learningIn the field of reinforcement learning, function $Q: S \\times A \\xrightarrow{} \\mathcal{R}$ combines the reward with states and actions. At each time $t$, the agent select an action $a_t$, get an reward $r_t$ from the environment, and enter into the next state $s_{t+1}$. The discussions of convergence of reinforcement learning cannot avoid the topics about the Q function or the value function. $$Q^{new}(s_t, a_t) = (1 - \\alpha)Q^{old}(s_t, a_t) + \\alpha (r_t + \\gamma \\max \\limits_a Q^{old}(s_{t+1}, a))$$ where $\\alpha$ is the learning rate and $\\gamma$ is the discount factor. These topics, including MDP and RL, are definitely will be discussed in other notes. The convergence of this Q-learning algorithm (Robbins-Monro Algorithm) will be proved in the next section. 2. Robbins-Monro AlgorithmIn 1951, Robbins and Monro developed a methodology for solving a root finding problem given a function depends (or partially depends) on a random variable. Root finding problem is the generalization of several important topics, including optimizations and extrema finding. 2.1 Root finding problem without uncertaintyLet $M(x)$ be a given function and $\\alpha$ a given constant such that the equation $$M(x) = a$$ has a unique root $x = \\theta$. For any iterative method for this root finding problem, we find one or more initial values $\\ x_{initial}$, and then successively obtain new values as certain functions of the previous obtained $x$. The most famous algorithm is Newton–Raphson method. 2.2 A stochastic generalization of the root finding problemLet’s consider how to solve this problem when $M(x)$ is unknown for some reason. In this blog, I decide to rephrase the notations of Robbins-Monro algorithm in a more specific way. We assume the randomness of the function is captured by the probability space $(\\Omega, \\mathcal{F}, P)$, and there is a random variable $Y$ on $(\\Omega, \\mathcal{F}, P)$ which take values in a state space $(S, \\mathcal{L})$. In this blog, we assume the state space is a d-dimensional Euclidean space equipped with the $\\sigma$-field of Borel sets. In this algorithm, $M(x)$ is a deterministic function which defined as follows: $$M(x) = \\int_{y \\in S }ydH(y|x)$$ where $H(y|x) = \\mathbb{P}[Y(\\omega) \\le y|x]$ ($\\omega \\in \\Omega$) is the distribution function of $Y$ given an unobservable deterministic variable $x$. In another word, $M(x)$ is the expectation of $Y$ for the given $x$. Although we know nothing about the nature of function $M(x)$ and the distribution function $H(y|x)$, but we assume (which mean this assumption will work as a known condition in the proof) that the equation $M(x) = a$ has a unique solution $x = \\theta$. Bobbins-Monro algorithm introduced a method to estimate $\\theta$ by making successive observations on $Y$ at given $x_1, x_2, …$, which are determined sequentially in accordance with some definite experimental procedure. The idea of this algorithm is to construct a transition probability between two states, or (non-stationary) Markov chain of $\\{ x_n \\}$. Moreover, with this sequence, the difference between $\\lim \\limits_{k \\xrightarrow{} \\infty}x_k$ and $\\theta$ can be controlled. We can conclude this idea as finding a sequence $\\{x_k\\}$, such that $$\\lim \\limits_{k \\xrightarrow{} \\infty}E(x_k - \\theta)^2 = 0$$ The idea also implies the convergence in probability (or in $\\mathcal{L}^2$) of $x_n$ to $\\theta$. 2.3 The proof of convergence theoremApparently, nobody can construct a sequence without any hints. The hint of this algorithm is the first example (expectation estimation) in the Introduction section. If we review the expectation estimation problem in the framework of Robbins-Monro, then in this example, $x$ denotes the expectation and function $M(x) = \\mathbb{E}^P[Y - x]$, and we want to solve the equation $M(x) = 0$. I only present the brief proof to show the idea. You may read this paper for more details. We want to prove that: Condition 1: For every $x$, a distribution function in $y$, and there exist a positive constant C such that $\\mathbb{P}[|Y(\\omega)| \\le C|x] = \\int_{-c}^cdH(y|x) = 1$ Condition 2: There exist finite constants $\\alpha, \\theta$ such that $M(x) \\le \\alpha$ for $x \\lt \\theta$, and $M(x) \\ge \\alpha$ for $x \\gt \\theta$ Condition 3: Let $\\{a_n\\}$ be a fixed sequence of positive constants such that $0","link":"/2020/01/13/Note%20for%20Stochastic%20Approximation%20Method--Part%201/"}],"tags":[{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Katex","slug":"Katex","link":"/tags/Katex/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"}],"categories":[{"name":"How to write a blog","slug":"How-to-write-a-blog","link":"/categories/How-to-write-a-blog/"},{"name":"Optimization","slug":"Optimization","link":"/categories/Optimization/"}]}