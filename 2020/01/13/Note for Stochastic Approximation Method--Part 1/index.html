<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>Note for Stochastic Approximation Methods--Part 1 - Peimou&#39;s Blog</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">




<meta name="description" content="">





    <meta name="description" content="1. An introduction to Stochastic Approximation MethodsGenerally, stochastic approximation methods are a family of iterative methods. The goal of these algorithms is to recover some properties of a fun">
<meta property="og:type" content="article">
<meta property="og:title" content="Note for Stochastic Approximation Methods--Part 1">
<meta property="og:url" content="http://yoursite.com/2020/01/13/Note%20for%20Stochastic%20Approximation%20Method--Part%201/index.html">
<meta property="og:site_name" content="Peimou&#39;s Blog">
<meta property="og:description" content="1. An introduction to Stochastic Approximation MethodsGenerally, stochastic approximation methods are a family of iterative methods. The goal of these algorithms is to recover some properties of a fun">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-01-14T02:25:37.000Z">
<meta property="article:modified_time" content="2020-01-14T23:16:57.136Z">
<meta property="article:author" content="Peimou Sun">
<meta property="article:tag" content="Optimization">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">





<link rel="icon" href="/favicon.ico">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    renderMathInElement(document.body,
   {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "$", right: "$", display: false}
              ]
          }
  );
</script>
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                    
                    Peimou&#39;s Blog
                    
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/categories">Categories</a>
            
            <a class="navbar-item "
               href="/tags">Tags</a>
            
            <a class="navbar-item "
               href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            <div class="navbar-item is-hoverable has-dropdown is-hidden-mobile is-hidden-tablet-only toc">
                <a class="navbar-item" title="Table of Contents">
                    <i class="fa fa-list"></i>
                </a>
                <div class="navbar-dropdown is-right">
                    
                    
                    
                    
                    <a class="navbar-item" href="#1-An-introduction-to-Stochastic-Approximation-Methods">1&nbsp;&nbsp;<b>1. An introduction to Stochastic Approximation Methods</b></a>
                    
                    
                    
                    <a class="navbar-item" href="#1-1-Example-1-Expectation">1.1&nbsp;&nbsp;1.1 Example 1: Expectation</a>
                    
                    
                    
                    <a class="navbar-item" href="#1-2-Example-2-SGD">1.2&nbsp;&nbsp;1.2 Example 2: SGD</a>
                    
                    
                    
                    <a class="navbar-item" href="#1-3-Example-3-Q-Learning-in-reinforcement-learning">1.3&nbsp;&nbsp;1.3 Example 3: Q-Learning in reinforcement learning</a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#2-Robbins-Monro-Algorithm">2&nbsp;&nbsp;<b>2. Robbins-Monro Algorithm</b></a>
                    
                    
                    
                    <a class="navbar-item" href="#2-1-Root-finding-problem-without-uncertainty">2.1&nbsp;&nbsp;2.1 Root finding problem without uncertainty</a>
                    
                    
                    
                    <a class="navbar-item" href="#2-2-A-stochastic-generalization-of-the-root-finding-problem">2.2&nbsp;&nbsp;2.2 A stochastic generalization of the root finding problem</a>
                    
                    
                    
                    <a class="navbar-item" href="#2-3-The-proof-of-convergence-theorem">2.3&nbsp;&nbsp;2.3 The proof  of  convergence theorem</a>
                    
                </div>
            </div>
            
            
            <a class="navbar-item" title="GitHub" href="https://github.com/Peimou/Peimou.github.io.git" target="_blank" rel="noopener">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope itemprop="blogPost">
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            Note for Stochastic Approximation Methods--Part 1
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            <time datetime="2020-01-14T02:25:37.000Z" itemprop="datePublished">Jan 13 2020</time>
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Optimization/">Optimization</a>
        </span>
        
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><h1 id="1-An-introduction-to-Stochastic-Approximation-Methods"><a href="#1-An-introduction-to-Stochastic-Approximation-Methods" class="headerlink" title="1. An introduction to Stochastic Approximation Methods"></a>1. An introduction to Stochastic Approximation Methods</h1><p>Generally, stochastic approximation methods are a family of iterative methods. The goal of these algorithms is to recover some properties of a function depending on random variables. The application of stochastic approximation ranges from deep learning (e.g., SGD ) to online learning methods. The earliest algorithms are Robbins-Monro(1951) and Kiefer-Wolfowitz(1952) algorithms. </p>
<a id="more"></a>
<h2 id="1-1-Example-1-Expectation"><a href="#1-1-Example-1-Expectation" class="headerlink" title="1.1 Example 1: Expectation"></a>1.1 Example 1: Expectation</h2><p>For a given probability space $(\Omega, \mathcal{F}, P)$, we want to solve the expectation of $E^P(f(\omega))$, where $f$ is a measurable function, and $\omega \in \Omega$. A reasonable solution is to estimate the expectation via arithmetic average. </p>
<p>Let $\mathbb{E}_n$ be the estimation of $\mathbb{E}^P(f(\omega))$ given $n$ observation. </p>
<script type="math/tex; mode=display">n \times E_n = \sum_{i = 1}^n f(\omega_i)</script><script type="math/tex; mode=display">(n+1) \times E_{n+1} = \sum_{i = 1}^{n+1} f(\omega_i)  = n \times  E_n + f(\omega_{n+1})</script><script type="math/tex; mode=display">\implies (n + 1) \times E{n+1} = (n+1) \times E_n + f(\omega_{n+1}) - E_n</script><script type="math/tex; mode=display">\implies E_{n+1} =  E_n + \frac{1}{n+1} [f(\omega_{n+1}) - E_n]</script><h2 id="1-2-Example-2-SGD"><a href="#1-2-Example-2-SGD" class="headerlink" title="1.2 Example 2: SGD"></a>1.2 Example 2: SGD</h2><p>In the framework of stochastic gradient descent (SGD), or on-line gradient descent, the true gradient $\nabla Q(\omega)$ is approximated by a gradient at a single sample. Let $\omega_n$ be the estimation of true gradient in the $nth$ iteration, then</p>
<script type="math/tex; mode=display">\omega_{n+1} = \omega_n - \lambda \nabla Q(\omega)</script><p>where $\lambda$ is the learning rate. </p>
<p>There are several popular extensions of the SGD method, including Momentum, Nesterov Momentum, and Adam. </p>
<h2 id="1-3-Example-3-Q-Learning-in-reinforcement-learning"><a href="#1-3-Example-3-Q-Learning-in-reinforcement-learning" class="headerlink" title="1.3 Example 3: Q-Learning in reinforcement learning"></a>1.3 Example 3: Q-Learning in reinforcement learning</h2><p>In the field of reinforcement learning, function $Q: S \times A \xrightarrow{} \mathcal{R}$ combines the reward with states and actions. At each time $t$, the agent select an action $a_t$, get an reward $r_t$ from the environment, and enter into the next state $s_{t+1}$. The discussions of convergence of reinforcement learning cannot avoid the topics about the Q function or the value function. </p>
<script type="math/tex; mode=display">Q^{new}(s_t, a_t) = (1 - \alpha)Q^{old}(s_t, a_t) + \alpha (r_t + \gamma \max \limits_a Q^{old}(s_{t+1}, a))</script><p>where $\alpha$ is the learning rate, and $\gamma$ is the discount factor. These topics, including MDP and RL, are definitely will be discussed in other notes. The convergence of this Q-learning algorithm (Robbins-Monro Algorithm) will be proved in the next section. </p>
<h1 id="2-Robbins-Monro-Algorithm"><a href="#2-Robbins-Monro-Algorithm" class="headerlink" title="2. Robbins-Monro Algorithm"></a>2. Robbins-Monro Algorithm</h1><p>In 1951, <a href="https://www.jstor.org/stable/2236626?seq=1" target="_blank" rel="noopener">Robbins and Monro</a> developed a methodology for solving a root finding problem given a function depends (or partially depends) on a random variable. Root finding problem is the generalization of several important topics, including optimizations and extrema finding. </p>
<h2 id="2-1-Root-finding-problem-without-uncertainty"><a href="#2-1-Root-finding-problem-without-uncertainty" class="headerlink" title="2.1 Root finding problem without uncertainty"></a>2.1 Root finding problem without uncertainty</h2><p>Let $M(x)$ be a given function and $\alpha$ a given constant such that the equation</p>
<script type="math/tex; mode=display">M(x) = a</script><p>has a unique root $x = \theta$. For any iterative method for this root finding problem, we find one or more initial values $ x_{initial}$, and then successively obtain new values as certain functions of the previous obtained $x$. The most famous algorithm is the <strong>Newton–Raphson method</strong>. </p>
<h2 id="2-2-A-stochastic-generalization-of-the-root-finding-problem"><a href="#2-2-A-stochastic-generalization-of-the-root-finding-problem" class="headerlink" title="2.2 A stochastic generalization of the root finding problem"></a>2.2 A stochastic generalization of the root finding problem</h2><p>Let’s consider how to solve this problem when $M(x)$ is unknown for some reason. In this blog, I decided to rephrase the notations of the Robbins-Monro algorithm in a more specific way. We assume the randomness of the function is captured by the probability space $(\Omega, \mathcal{F}, P)$, and there is a random variable $Y$ on $(\Omega, \mathcal{F}, P)$ which takes values in a state space $(S, \mathcal{L})$. In this blog, we assume the state space is a d-dimensional Euclidean space equipped with the $\sigma$-field of Borel sets. In this algorithm, $M(x)$ is a deterministic function which defined as follows:</p>
<script type="math/tex; mode=display">M(x) = \int_{y \in S }ydH(y|x)</script><p>where $H(y|x) = \mathbb{P}[Y(\omega) \le y|x]$ ($\omega \in \Omega$) is the distribution function of $Y$ given an unobservable deterministic variable $x$. </p>
<p>In other word, $M(x)$ is the expectation of $Y$ for the given $x$. Although we know nothing about the nature of function $M(x)$ and the distribution function $H(y|x)$, but we <strong>assume</strong> (which mean this assumption will work as a known condition in the proof) that the equation $M(x) = a$ has a unique solution $x = \theta$. Bobbins-Monro algorithm introduced a method to estimate $\theta$ by making successive observations on $Y$ at given $x_1, x_2, …$, which are determined sequentially in accordance with some definite experimental procedure. </p>
<p>The idea of this algorithm is to construct a transition probability between two states, or (non-stationary) Markov chain of $\\\{ x_n \\\}$. Moreover, with this sequence, the difference between $\lim \limits_{k \xrightarrow{} \infty}x_k$ and $\theta$ can be controlled. We can conclude this idea as finding a sequence $\\{x_k\\}$, such that</p>
<script type="math/tex; mode=display">\lim \limits_{k \xrightarrow{} \infty}E(x_k - \theta)^2 = 0</script><p>The idea also implies the convergence in probability (or in $\mathcal{L}^2$) of $x_n$ to $\theta$.</p>
<h2 id="2-3-The-proof-of-convergence-theorem"><a href="#2-3-The-proof-of-convergence-theorem" class="headerlink" title="2.3 The proof  of  convergence theorem"></a>2.3 The proof  of  convergence theorem</h2><p>Nobody can construct a sequence without any hints. The hint of this algorithm is the first example (expectation estimation) in the Introduction section. If we review the expectation estimation problem in the framework of Robbins-Monro, then in this example, $x$ denotes the expectation and function $M(x) = \mathbb{E}^P[Y - x]$, and we want to solve the equation  $M(x) = 0$. I only present brief proof to show the idea. You may read this <a href="https://www.jstor.org/stable/2236626?seq=1" target="_blank" rel="noopener">paper</a> for more details. </p>
<p><strong>We want to prove that:</strong></p>
<p><em>Condition 1</em>: For every $x$, a distribution function in $y$, and there exist a positive constant C such that $\mathbb{P}[|Y(\omega)| \le C|x] = \int_{-c}^cdH(y|x) = 1$</p>
<p><em>Condition 2</em>: There exist finite constants $\alpha, \theta$ such that $M(x) \le \alpha$ for $x \lt \theta$, and $M(x) \ge \alpha$ for $x \gt \theta$</p>
<p><em>Condition 3</em>: Let $\\{a_n\\}$ be a fixed sequence of positive constants such that $0<\sum_{1}^{\infty} a_n^2 = A < + \infty$, and $\sum_{1}^{\infty} a_n = + \infty$</p>
<p><strong>For a Markov chain $\\{x_n\\}$ starts with an arbitrary constant $x_1$:</strong></p>
<script type="math/tex; mode=display">x_{n+1} = x_n + a_n(\alpha - y_n)</script><p>where $y_n$ is the value of random variable $Y$ given $x_n$ as we have defined in  section 2.2.</p>
<p><strong>If <em>condition 1</em>, <em>condition 2</em>, and <em>condition 3</em> hold, let $b_n = \mathbb{E}(x_n - \theta)^2$, then $\lim \limits_{n \xrightarrow{} \infty} b = 0$</strong></p>
<p><strong>Proof:</strong></p>
<script type="math/tex; mode=display">b_{n+1} = \mathbb{E}(x_{n+1} - \theta)^2 = \mathbb{E}[\mathbb{E}(x_{n+1} - \theta)^2|x_n]</script><script type="math/tex; mode=display">b_{n+1} = b_{n} + a_n^2\mathbb{E}[\int_{-\infty}^{\infty}\\{(x_n - \theta) - a_n(y - a)\\}^2dH(y|x_n)]</script><p>Setting $d_n = \mathbb{E}[(x_n - \theta)(M(x_n) - \alpha)] $, and $c_n = \mathbb{E}[\int_{-\infty}^{\infty} (y - \alpha)^2 dH(y|x_n)]$, then</p>
<script type="math/tex; mode=display">b_{n+1} - b_n = a_n^2e_n - 2a_nd_n</script><p>Accumulate from $n = 1$, $b_{n+1}$ can be rewritten to</p>
<script type="math/tex; mode=display">b_n = b_1 + \sum_{i = 1}^n a_i^2e_i - 2\sum_{i = 1}^n a_id_i</script><p>Note that for every n, $d_n \ge 0$ and $0 \le e_n \le (C+ |\alpha|)^2$</p>
<p>Since we know the function has unique solution $\theta$, $\sum_{i = 1}^n a_id_i$ must converge. </p>
<p>Due to the definition of Markov chain ${x_n}$,</p>
<script type="math/tex; mode=display">x_n = x_1 + \sum_{i = 1}^{n - 1} a_n (\alpha - y_n) \le x_1 +  \sum_{i = 1}^{n - 1} a_n (C + |\alpha|)</script><script type="math/tex; mode=display">x_n -\theta \le x_1 - \theta +  \sum_{i = 1}^{n - 1} a_n (C + |\alpha|)</script><script type="math/tex; mode=display">|x_n -\theta| \le |x_1 - \theta +  \sum_{i = 1}^{n - 1} a_n (C + |\alpha|)| \le |x_1 - \theta| +  \sum_{i = 1}^{n - 1} a_n (C + |\alpha|)</script><p>The last inequality holds since $\sum_{i = 1}^{n - 1} a_n (C + |\alpha|)  \gt 0$.</p>
<p>Let $A_n = |x_1 - \theta| +  \sum_{i = 1}^{n - 1} a_n (C + |\alpha|)$, note that $\mathbb{P}\\{|x_n - \theta| \le An\\} = 1$.</p>
<p>Now, we want to find a sequence $\\{k_n\\}$ of nonnegative constants such that $d_n \ge k_nb_n$, $\sum_{i = 1}^{\infty}a_nd_n = \infty$. The goal of this step is to prove $\lim \limits_{n \xrightarrow{} {\infty}}b_n = 0$, since we have known $\sum_{i = 1}^n a_id_i$ converge._</p>
<p>In another word, if we find such sequence $\\{k_n\\}$ with the aforementioned two condition (1.$d_n \ge k_nb_n$, 2. $\sum_{i = 1}^{\infty}a_nd_n = \infty$), we can prove $\lim \limits_{n \xrightarrow{} {\infty}}b_n = 0$. </p>
<p>Set $\bar{k_n} = \inf [\frac{M(x) - \alpha}{x - \theta}]$. </p>
<p>On one hand, $d_n \ge \int_{|x_n - \theta| \le A_n}\bar{k_n}|x - \theta|^2dP_n(x) = \bar{k_n}b_n$, which means $d_n \ge k_nb_n$ holds. </p>
<p>On the other hand, we can assume without generality that $\bar{k_n} \ge \frac{K}{A_n} $ for some given $K \gt 0$here. Moreover, <em>condition 3</em> implies that </p>
<script type="math/tex; mode=display">\sum_{n = 2}^{\infty} \frac{a_n}{a_2 +...+a_{n-1}} = \infty</script><p>, and for sufficiently large n:</p>
<script type="math/tex; mode=display">A_n \le 2\sum_{i = 1}^{n - 1} a_n (C + |\alpha|)</script><p>These two conclusions indicate that </p>
<script type="math/tex; mode=display">a_n\bar{k_n} \ge \frac{a_nK}{A_n} \ge \frac{a_nK}{2\sum_{i = 1}^{n - 1} a_n (C + |\alpha|)}</script><p>, which implies $\sum_{i = 1}^{\infty}a_nd_n = \infty$ holds. </p>
<p>$\therefore \lim \limits_{n \xrightarrow{} {\infty}}b_n = 0$ holds. The convergence is proved. </p>
</body></html>
    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Optimization/">#Optimization</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Machine-Learning/">#Machine Learning</a></span>
    
    </div>
    
    
    <div class="columns is-mobile is-multiline article-nav">
        <span class="column is-12-mobile is-half-desktop is-hidden-mobile article-nav-prev">
            
        </span>
        <span class="column is-12-mobile is-half-desktop  article-nav-next">
            
            <a href="/2020/01/13/Hello%20World/">Hello World</a>
            
        </span>
    </div>
    
</article>


<div class="sharebox">
    
<div class="addthis_inline_share_toolbox"></div>
<script type="text/javascript" src="true"></script>

</div>



<div class="comments">
    <h3 class="title is-4">Comments</h3>
    
<script>
    var disqus_config = function () {
        this.page.url = 'http://yoursite.com/2020/01/13/Note%20for%20Stochastic%20Approximation%20Method--Part%201/';
        this.page.identifier = '2020/01/13/Note for Stochastic Approximation Method--Part 1/';
        
        this.language = 'en';
        
    };
    (function() {
        var d = document, s = d.createElement('script');  
        s.src = '//' + 'peimousun' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>

<div id="disqus_thread">
    
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
</div>
</div>

    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2020 Peimou Sun&nbsp;
                All text on this site are released under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0 License</a> unless otherwise stated. 
                <br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            
        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/" target="_blank" rel="noopener">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
<script>
    renderMathInElement(document.body,
   {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "$", right: "$", display: false}
              ]
          }
  );
</script>
    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: 'Home Page',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>